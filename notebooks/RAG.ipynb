{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Document (ml.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "\n",
    "pdf_loader = PDFMinerLoader(\"./data/ml.pdf\")\n",
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2460"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "len(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(chunked_documents))]\n",
    "vector_store.add_documents(documents=chunked_documents, ids=uuids)\n",
    "vector_store.save_local(\"faiss_index1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Document (ml2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "\n",
    "pdf_loader2 = PDFMinerLoader(\"./data/xai.pdf\")\n",
    "documents2 = pdf_loader2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "chunked_documents2 = text_splitter2.split_documents(documents2)\n",
    "\n",
    "len(chunked_documents2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "embeddings2 = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "\n",
    "index2 = faiss.IndexFlatL2(len(embeddings2.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store2 = FAISS(\n",
    "    embedding_function=embeddings2,\n",
    "    index=index2,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "uuids2 = [str(uuid4()) for _ in range(len(chunked_documents2))]\n",
    "vector_store2.add_documents(documents=chunked_documents2, ids=uuids2)\n",
    "vector_store2.save_local(\"faiss_index2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index1\", embeddings, allow_dangerous_deserialization=True)\n",
    "vector_store2 = FAISS.load_local(\"faiss_index2\", embeddings2, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to add explainability to convolution and attention layers?\"\n",
    "results = vector_store.similarity_search(\n",
    "    question,\n",
    "    k=400\n",
    ")\n",
    "\n",
    "results2 = vector_store2.similarity_search(\n",
    "    question,\n",
    "    k=400\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\\n\".join(map(lambda i: i.page_content, results))\n",
    "results2 = \"\\n\".join(map(lambda i: i.page_content, results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model_id, max_new_tokens=1000000)\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTâ€Œ4o Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index1\", embeddings, allow_dangerous_deserialization=True)\n",
    "vector_store2 = FAISS.load_local(\"faiss_index2\", embeddings2, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to add explainability to convolution and attention layers?\"\n",
    "results = vector_store.similarity_search(\n",
    "    question,\n",
    "    k=40\n",
    ")\n",
    "\n",
    "results2 = vector_store2.similarity_search(\n",
    "    question,\n",
    "    k=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\\n\".join(map(lambda i: i.page_content, results2))\n",
    "results2 = \"\\n\".join(map(lambda i: i.page_content, results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "template = f\"\"\"\n",
    "Answer the request using only the context below.\n",
    "Context:\n",
    "{results}\n",
    "\n",
    "{results2}\n",
    "\n",
    "\n",
    "Request: {question}\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    store=True,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Combine only Context section provided and answer based on it only, summerizer it and dont use of your knowlegde in answer.\"},\n",
    "        {\"role\": \"user\", \"content\": template}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain specific information about adding explainability to convolution and attention layers in machine learning models. It focuses on general principles of interpretability, the significance of contrastive explanations, the importance of social context in explanations, and methods for creating understandable explanations in machine learning. For insights on specific techniques or methods for explaining convolution and attention layers, further specialized sources would be necessary.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation phase of new architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./preface.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index1\", embeddings, allow_dangerous_deserialization=True)\n",
    "vector_store2 = FAISS.load_local(\"faiss_index2\", embeddings2, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to add explainability to convolution and attention layers?\"\n",
    "results = vector_store.similarity_search(\n",
    "    question,\n",
    "    k=40\n",
    ")\n",
    "\n",
    "results2 = vector_store2.similarity_search(\n",
    "    question,\n",
    "    k=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def query(question, results, client):\n",
    "    template = f\"\"\"\n",
    "    Answer the request using only the context below.\n",
    "    Context:\n",
    "    \n",
    "    {\"              \".join(results)}\n",
    "\n",
    "\n",
    "    Request: {question}\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Combine only Context section provided and answer based on it only, summerizer it and dont use of your knowlegde in answer.\"},\n",
    "            {\"role\": \"user\", \"content\": template}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Explainability can be added to convolution and attention layers through the use of attention mechanisms. These mechanisms help to clarify what aspects of the input data are influencing the model's predictions, which is crucial for understanding model behavior and improving its interpretability.\\n\\nIn the context of convolutional neural networks (CNNs), attention mechanisms can assist in revealing which parts of an image contribute significantly to a particular classification. For example, if a CNN misclassifies an image, examining the attention map can highlight which features attracted the model's focus, allowing for better debugging and retraining with more relevant data.\\n\\nSimilarly, in attention-based architectures like Transformers, explainability can be achieved as attention models prioritize different parts of the input at each time step in sequence data processing. By visualizing these attention weights, stakeholders can understand the model's reasoning behind specific outputs, thereby offering insights into its decision-making process.\\n\\nOverall, integrating attention-based approaches enhances the explainability of both convolutional and attention layers, enabling users to systematically check and improve models based on their focus areas during predictions.\", \"To add explainability to convolutional and attention layers, one can employ interpretable models or techniques that generate explanations for individual predictions. Here are some suggested approaches based on the context provided:\\n\\n1. **Local Surrogate Models (LIME)**: Utilize local interpretable model-agnostic explanations (LIME) to create surrogate models that approximate the predictions of the convolutional or attention layers. This allows for an interpretation of individual predictions by focusing on the local decision-making process of the model.\\n\\n2. **Visual Attention**: For attention layers, explaining the attention weights can provide insight into how the model focuses on specific parts of the input data. This visualization helps in understanding which features contribute to the final prediction.\\n\\n3. **Counterfactual Explanations**: Implement counterfactual explanations to illustrate how changes in specific features influence predictions. This method provides a causal interpretation of the model's decision process.\\n\\n4. **Highlighting Important Features**: Employ techniques that highlight or visualize the most important features or regions of the input data that the convolutional layers focus on, helping elucidate the decision-making pathway.\\n\\nThese methods can enhance the interpretability of convolutional and attention layers, enabling better understanding and trust in the model's predictions.\", 'The context does not provide specific methods or techniques on how to add explainability to convolution and attention layers. However, it discusses the importance of interpretability in machine learning, suggesting that model-agnostic methods may offer a flexible approach to interpret any machine learning model. The context emphasizes the need for understanding the \"why\" behind predictions, which can be important for certain applications. For specific techniques related to convolutional and attention layers, further details would be needed that are not available in the provided context.', \"The context provided outlines several components related to machine learning models, particularly focusing on Convolutional Neural Networks (CNNs) and attention mechanisms. To add explainability to convolution and attention layers, one could use model-agnostic methods such as example-based explanations, counterfactual explanations, and influence functions, which help to elucidate model predictions.\\n\\nFor convolution layers, interpretability techniques could include visualizing the activation maps of different layers to highlight which features contribute to the model's predictions. This may involve techniques like saliency maps or Grad-CAM, which show areas of the input that have the most influence on the output.\\n\\nIn the case of attention layers, one could examine the attention weights to understand how the model focuses on different parts of the input when making predictions. Visualization of these weights can provide insights into which elements were most influential in the decision-making process.\\n\\nCombining these techniques can facilitate a deeper understanding of how convolutional and attention layers contribute to model outcomes, thereby enhancing explainability.\"]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def group_elements(input_list, group_size):\n",
    "    return [\"\\n\".join(map(lambda l: l.page_content, input_list[i:i + group_size])) for i in range(0, len(input_list), group_size)]\n",
    "\n",
    "\n",
    "input1 = group_elements(results, 10)\n",
    "input2 = group_elements(results2, 10)\n",
    "output = []\n",
    "for res in zip(input1, input2):\n",
    "    output.append(query(question, res, client))\n",
    "    time.sleep(30)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Explainability in machine learning, particularly with convolutional and attention layers, can be enhanced by analyzing how these layers contribute to the model's predictions. For convolutional layers, you can check the feature maps produced during processing to understand what patterns the model is focusing on, such as specific shapes or textures in images. In contrast, attention mechanisms allow for a more explicit analysis of the model's focus. When the model generates an output, you can inspect the attention weights to see which parts of the input it considered most relevant. This approach helps clarify why the model made a particular decision, which can be crucial for debugging or meeting legal requirements in applications like loan approvals. This means both convolutional and attention layers can be utilized to gain insights into model behavior, facilitating improvements and ensuring compliance.\", 'The context does not provide specific methods or techniques for adding explainability to convolution and attention layers. It discusses various aspects of Convolutional Neural Networks (CNNs) and their architectures, including issues related to semantic segmentation, pooling layers, and different CNN models like ResNet and Inception. However, it does not address the concept of explainability or how to implement it in these contexts.', \"The provided context does not directly address how to add explainability to convolution and attention layers. However, it emphasizes the importance of short, contrastive explanations in interpretable machine learning. To apply these insights to convolution and attention layers, one might consider using techniques like local interpretable model-agnostic explanations (LIME) to generate concise explanations for predictions made by these layers. Additionally, focusing on the specific feature values that contribute to the model's predictions could lead to clearer explanations that are easier for users to understand. The social context of explanations and the audience's understanding should also be taken into account when designing such explanations.\", 'The context provided does not directly address how to add explainability to convolution and attention layers. However, it does discuss various model-agnostic interpretability methods, such as local surrogate models, Shapley values, and example-based explanations. These methods can potentially apply to deep learning models, including those with convolution and attention layers, by generating explanations independent of the underlying model. It emphasizes the importance of interpretability and understanding predictions to improve trust, debugging, and learning. To implement these methods, one might need to evaluate the specific characteristics and requirements of the convolution and attention layers in relation to the chosen explanation method.']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def group_elements(input_list, group_size):\n",
    "    return [\"\\n\".join(map(lambda l: l.page_content, input_list[i:i + group_size])) for i in range(0, len(input_list), group_size)]\n",
    "\n",
    "\n",
    "input1 = group_elements(results, 10)\n",
    "input2 = group_elements(results2, 10)\n",
    "input1.extend(input2)\n",
    "output = []\n",
    "for i in range(0, len(input1), 2):\n",
    "    output.append(query(question, input1[i: i + 2], client))\n",
    "    time.sleep(10)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To add explainability to convolution and attention layers, one can analyze how these layers contribute to the model's predictions. For convolutional layers, examining the feature maps can reveal the patterns the model is focusing on, such as shapes or textures in images. For attention mechanisms, inspecting the attention weights can identify which parts of the input are deemed most relevant during output generation. Utilizing interpretability methods like local interpretable model-agnostic explanations (LIME) or model-agnostic methods like local surrogate models and Shapley values may also help generate concise explanations for predictions. It is essential to consider the audience's understanding and the social context of the explanations to ensure they are clear and effective.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(question, output, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To enhance explainability in convolutional and attention layers, attention '\n",
      " 'mechanisms can be applied. These mechanisms allow for an understanding of '\n",
      " 'model outputs by identifying which parts of the input (such as images or '\n",
      " 'text) the model concentrated on during predictions. For instance, if an '\n",
      " \"image is misclassified, attention mechanisms can help analyze the model's \"\n",
      " 'focus, clarifying its reasoning.\\n'\n",
      " '\\n'\n",
      " 'In practice, visual attention can be incorporated, where a convolutional '\n",
      " 'neural network processes an image and delivers feature maps to a decoder. '\n",
      " 'The decoder, using an attention mechanism, generates outputs (like captions) '\n",
      " 'sequentially while emphasizing relevant aspects of the input image.\\n'\n",
      " '\\n'\n",
      " 'Additionally, the Transformer architecture exemplifies the effectiveness of '\n",
      " 'solely using attention mechanisms for understanding and explaining model '\n",
      " 'predictions. By including attention layers, models become more '\n",
      " 'interpretable, offering insights into their decision-making processes.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(\"To enhance explainability in convolutional and attention layers, attention mechanisms can be applied. These mechanisms allow for an understanding of model outputs by identifying which parts of the input (such as images or text) the model concentrated on during predictions. For instance, if an image is misclassified, attention mechanisms can help analyze the model's focus, clarifying its reasoning.\\n\\nIn practice, visual attention can be incorporated, where a convolutional neural network processes an image and delivers feature maps to a decoder. The decoder, using an attention mechanism, generates outputs (like captions) sequentially while emphasizing relevant aspects of the input image.\\n\\nAdditionally, the Transformer architecture exemplifies the effectiveness of solely using attention mechanisms for understanding and explaining model predictions. By including attention layers, models become more interpretable, offering insights into their decision-making processes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Document (ml.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "\n",
    "pdf_loader = PDFMinerLoader(\"./data/ml.pdf\")\n",
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2460"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "len(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(chunked_documents))]\n",
    "vector_store.add_documents(documents=chunked_documents, ids=uuids)\n",
    "vector_store.save_local(\"faiss_index1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Document (ml2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "\n",
    "pdf_loader2 = PDFMinerLoader(\"./data/xai.pdf\")\n",
    "documents2 = pdf_loader2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "chunked_documents2 = text_splitter2.split_documents(documents2)\n",
    "\n",
    "len(chunked_documents2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "embeddings2 = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "\n",
    "index2 = faiss.IndexFlatL2(len(embeddings2.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store2 = FAISS(\n",
    "    embedding_function=embeddings2,\n",
    "    index=index2,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "uuids2 = [str(uuid4()) for _ in range(len(chunked_documents2))]\n",
    "vector_store2.add_documents(documents=chunked_documents2, ids=uuids2)\n",
    "vector_store2.save_local(\"faiss_index2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index1\", embeddings, allow_dangerous_deserialization=True)\n",
    "vector_store2 = FAISS.load_local(\"faiss_index2\", embeddings2, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* In  some  applications,  explainability  is  not  just  a  tool  to  debug  a  model;  it  can  be  a\n",
      "legal  requirement  (think  of  a  system  deciding  whether  or  not  it  should  grant  you  a\n",
      "loan).\n",
      "\n",
      "18 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.\n",
      "\n",
      "19 Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” Proceed‐\n",
      "ings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\n",
      "1135–1144.\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "553\n",
      "\n",
      "\fAttention  mechanisms  are  so  powerful  that  you  can  actually  build  state-of-the-art\n",
      "models using only attention mechanisms. [{'source': './data/ml.pdf', 'start_index': 1201072}]\n",
      "* Explainability\n",
      "One extra benefit of attention mechanisms is that they make it easier to understand\n",
      "what led the model to produce its output. This is called explainability. It can be espe‐\n",
      "cially useful when the model makes a mistake: for example, if an image of a dog walk‐\n",
      "ing in the snow is labeled as “a wolf walking in the snow,” then you can go back and\n",
      "check what the model focused on when it output the word “wolf.” You may find that it\n",
      "was paying attention not only to the dog, but also to the snow, hinting at a possible\n",
      "explanation: perhaps the way the model learned to distinguish dogs from wolves is by\n",
      "checking whether or not there’s a lot of snow around. You can then fix this by training\n",
      "the model with more images of wolves without snow, and dogs with snow. This exam‐\n",
      "ple comes from a great 2016 paper19 by Marco Tulio Ribeiro et al. that uses a different\n",
      "approach  to  explainability:  learning  an  interpretable  model  locally  around  a  classi‐\n",
      "fier’s prediction. [{'source': './data/ml.pdf', 'start_index': 1200084}]\n",
      "* As you can see, the field of Deep Computer Vision is vast and moving fast, with all\n",
      "sorts of architectures popping out every year, all based on convolutional neural net‐\n",
      "works. The progress made in just a few years has been astounding, and researchers\n",
      "are now focusing on harder and harder problems, such as adversarial learning (which\n",
      "attempts to make the network more resistant to images designed to fool it), explaina‐\n",
      "bility (understanding why the network makes a specific classification), realistic image\n",
      "generation (which we will come back to in Chapter 17), and single-shot learning (a sys‐\n",
      "tem  that  can  recognize  an  object  after  it  has  seen  it  just  once).  Some  even  explore\n",
      "completely  novel  architectures,  such  as  Geoffrey  Hinton’s  capsule  networks35  (I  pre‐\n",
      "sented them in a couple of videos, with the corresponding code in a notebook). Now\n",
      "on to the next chapter, where we will look at how to process sequential data such as [{'source': './data/ml.pdf', 'start_index': 1068947}]\n",
      "* Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "553\n",
      "\n",
      "\fAttention  mechanisms  are  so  powerful  that  you  can  actually  build  state-of-the-art\n",
      "models using only attention mechanisms.\n",
      "\n",
      "Attention Is All You Need: The Transformer Architecture\n",
      "In  a  groundbreaking  2017  paper,20  a  team  of  Google  researchers  suggested  that\n",
      "“Attention Is All You Need.” They managed to create an architecture called the Trans‐\n",
      "former, which significantly improved the state of the art in NMT without using any\n",
      "recurrent or convolutional layers,21 just attention mechanisms (plus embedding lay‐\n",
      "ers, dense layers, normalization layers, and a few other bits and pieces). As an extra\n",
      "bonus, this architecture was also much faster to train and easier to parallelize, so they\n",
      "managed to train it at a fraction of the time and cost of the previous state-of-the-art\n",
      "models.\n",
      "\n",
      "The Transformer architecture is represented in Figure 16-8. [{'source': './data/ml.pdf', 'start_index': 1201614}]\n",
      "* We simply wrap the decoder cell in an AttentionWrapper, and we provide the desired\n",
      "attention mechanism (Luong attention in this example).\n",
      "\n",
      "Visual Attention\n",
      "Attention mechanisms are now used for a variety of purposes. One of their first appli‐\n",
      "cations  beyond  NMT  was  in  generating  image  captions  using  visual  attention:17  a\n",
      "convolutional  neural  network  first  processes  the  image  and  outputs  some  feature\n",
      "maps, then a decoder RNN equipped with an attention mechanism generates the cap‐\n",
      "tion, one word at a time. At each decoder time step (each word), the decoder uses the\n",
      "attention  model  to  focus  on  just  the  right  part  of  the  image.  For  example,  in\n",
      "Figure  16-7,  the  model  generated  the  caption  “A  woman  is  throwing  a  frisbee  in  a\n",
      "park,” and you can see what part of the input image the decoder focused its attention\n",
      "on when it was about to output the word “frisbee”: clearly, most of its attention was\n",
      "focused on the frisbee. [{'source': './data/ml.pdf', 'start_index': 1198718}]\n",
      "* 16 Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation,” Proceed‐\n",
      "\n",
      "ings of the 2015 Conference on Empirical Methods in Natural Language Processing (2015): 1412–1421.\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "551\n",
      "\n",
      "\fEquation 16-1. Attention mechanisms\n",
      "\n",
      " t = ∑\n",
      "i\n",
      "\n",
      "α t, i\n",
      "\n",
      " i\n",
      "\n",
      "with α t, i =\n",
      "\n",
      "and e t, i =\n",
      "\n",
      "exp e t, i\n",
      "∑i′ exp e t, i′\n",
      "⊺  i\n",
      "⊺   i\n",
      "tanh   t ;  i\n",
      "\n",
      " t\n",
      " t\n",
      "⊺\n",
      "\n",
      "dot\n",
      "\n",
      "general\n",
      "\n",
      "concat\n",
      "\n",
      "Here is how you can add Luong attention to an Encoder–Decoder model using Ten‐\n",
      "sorFlow Addons:\n",
      "\n",
      "attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
      "    units, encoder_state, memory_sequence_length=encoder_sequence_length)\n",
      "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
      "    decoder_cell, attention_mechanism, attention_layer_size=n_units)\n",
      "\n",
      "We simply wrap the decoder cell in an AttentionWrapper, and we provide the desired\n",
      "attention mechanism (Luong attention in this example). [{'source': './data/ml.pdf', 'start_index': 1197924}]\n",
      "* Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "397\n",
      "\n",
      "\fLet’s go through this code:\n",
      "\n",
      "• The  constructor  creates  the  DNN  with  five  dense  hidden  layers  and  one  dense\n",
      "\n",
      "output layer.\n",
      "\n",
      "• The  build()  method  creates  an  extra  dense  layer  which  will  be  used  to  recon‐\n",
      "struct the inputs of the model. It must be created here because its number of units\n",
      "must be equal to the number of inputs, and this number is unknown before the\n",
      "build() method is called.\n",
      "\n",
      "• The  call()  method  processes  the  inputs  through  all  five  hidden  layers,  then\n",
      "passes  the  result  through  the  reconstruction  layer,  which  produces  the  recon‐\n",
      "struction. [{'source': './data/ml.pdf', 'start_index': 857618}]\n",
      "* Convolutional neural networks (CNNs) emerged from the study of the brain’s visual\n",
      "cortex, and they have been used in image recognition since the 1980s. In the last few\n",
      "years, thanks to the increase in computational power, the amount of available training\n",
      "data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐\n",
      "aged to achieve superhuman performance on some complex visual tasks. They power\n",
      "image  search  services,  self-driving  cars,  automatic  video  classification  systems,  and\n",
      "more. Moreover, CNNs are not restricted to visual perception: they are also successful\n",
      "at many other tasks, such as voice recognition and natural language processing. How‐\n",
      "ever, we will focus on visual applications for now. [{'source': './data/ml.pdf', 'start_index': 965580}]\n",
      "* In the second part of this chapter, we will look at attention mechanisms. As their name\n",
      "suggests,  these  are  neural  network  components  that  learn  to  select  the  part  of  the\n",
      "inputs that the rest of the model should focus on at each time step. First we will see\n",
      "how to boost the performance of an RNN-based Encoder–Decoder architecture using\n",
      "attention, then we will drop RNNs altogether and look at a very successful attention-\n",
      "only  architecture  called  the  Transformer.  Finally,  we  will  take  a  look  at  some  of  the\n",
      "most  important  advances  in  NLP  in  2018  and  2019,  including  incredibly  powerful\n",
      "language models such as GPT-2 and BERT, both based on Transformers.\n",
      "\n",
      "Let’s start with a simple and fun model that can write like Shakespeare (well, sort of). [{'source': './data/ml.pdf', 'start_index': 1134784}]\n",
      "* Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "449\n",
      "\n",
      "\fFigure 14-4. Reducing dimensionality using a stride of 2\n",
      "\n",
      "Filters\n",
      "A neuron’s weights can be represented as a small image the size of the receptive field.\n",
      "For example, Figure 14-5 shows two possible sets of weights, called filters (or convolu‐\n",
      "tion kernels). The first one is represented as a black square with a vertical white line in\n",
      "the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\n",
      "1s); neurons using these weights will ignore everything in their receptive field except\n",
      "for  the  central  vertical  line  (since  all  inputs  will  get  multiplied  by  0,  except  for  the\n",
      "ones  located  in  the  central  vertical  line).  The  second  filter  is  a  black  square  with  a\n",
      "horizontal  white  line  in  the  middle.  Once  again,  neurons  using  these  weights  will\n",
      "ignore everything in their receptive field except for the central horizontal line. [{'source': './data/ml.pdf', 'start_index': 973811}]\n",
      "* an inception network or every residual unit in a ResNet to recalibrate the relative\n",
      "importance  of  feature  maps.  Finally,  Xception’s  main  innovation  was  the  use  of\n",
      "depthwise  separable  convolutional  layers,  which  look  at  spatial  patterns  and\n",
      "depthwise patterns separately. [{'source': './data/ml.pdf', 'start_index': 1620115}]\n",
      "* CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "475\n",
      "\n",
      "\fSeparable convolutional layers use fewer parameters, less memory,\n",
      "and  fewer  computations  than  regular  convolutional  layers,  and  in\n",
      "general  they  even  perform  better,  so  you  should  consider  using\n",
      "them by default (except after layers with few channels).\n",
      "\n",
      "The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\n",
      "versity of Hong Kong. They used an ensemble of many different techniques, includ‐\n",
      "ing  a  sophisticated  object-detection  system  called  GBD-Net,21  to  achieve  a  top-five\n",
      "error rate below 3%. Although this result is unquestionably impressive, the complex‐\n",
      "ity of the solution contrasted with the simplicity of ResNets. Moreover, one year later\n",
      "another fairly simple architecture performed even better, as we will see now. [{'source': './data/ml.pdf', 'start_index': 1022942}]\n",
      "* 280-295\n",
      "\n",
      "Hopfield networks, 773\n",
      "implementing MLPs with Keras, 295-320\n",
      "overview of, 279\n",
      "restricted Boltzmann machines (RBMs), 776\n",
      "self-organizing maps (SOMs), 780\n",
      "\n",
      "artificial neurons, 283\n",
      "association rule learning, 12\n",
      "associative memory networks, 773\n",
      "Asynchronous Advantage Actor-Critic (A3C),\n",
      "\n",
      "662\n",
      "\n",
      "asynchronous updates, 707\n",
      "Atari preprocessing, 645\n",
      "attention mechanisms\n",
      "\n",
      "defined, 526\n",
      "explainability and, 553\n",
      "overview of, 549\n",
      "Transformer architecture, 554\n",
      "visual attention, 552\n",
      "\n",
      "802 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "attributes, 8\n",
      "autoencoders\n",
      "\n",
      "convolutional, 579\n",
      "denoising, 581\n",
      "efficient data representations, 569\n",
      "generative, 586\n",
      "versus Generative Adversarial Networks\n",
      "\n",
      "(GANs), 568\n",
      "overview of, 567\n",
      "parts of, 569\n",
      "PCA with undercomplete linear autoencod‐\n",
      "\n",
      "ers, 570\n",
      "\n",
      "probabilistic, 586\n",
      "recurrent, 580\n",
      "sparse, 582\n",
      "stacked, 572-575\n",
      "undercomplete, 570\n",
      "unsupervised pretraining using stacked,\n",
      "\n",
      "576-579\n",
      "\n",
      "variational, 586-591\n",
      "\n",
      "AutoGraphs, 407\n",
      "automatic differentiation (autodiff), 290, 399, [{'source': './data/ml.pdf', 'start_index': 1729952}]\n",
      "* VGGNet, 470\n",
      "\n",
      "\fvirtual GPU devices, 695\n",
      "visible units, 775\n",
      "visual attention, 552\n",
      "visualization algorithms, 11\n",
      "vocabulary, 432\n",
      "voice recognition, 445\n",
      "\n",
      "W\n",
      "wall time, 341\n",
      "warmup phase, 708\n",
      "WaveNet, 498, 521\n",
      "weak learners, 190\n",
      "weighted moving average model, 506\n",
      "white box models, 178\n",
      "Wide & Deep neural networks, 308\n",
      "wisdom of the crowd, 189\n",
      "word embeddings, 434\n",
      "\n",
      "word tokenization, 536\n",
      "WordTrees, 490\n",
      "workspace creation, 42\n",
      "\n",
      "X\n",
      "Xavier initialization, 333\n",
      "Xception (Extreme Inception), 474\n",
      "XGBoost, 208\n",
      "\n",
      "Y\n",
      "You Only Look Once (YOLO), 489\n",
      "\n",
      "Z\n",
      "zero padding, 449\n",
      "zero-shot learning (ZSL), 564\n",
      "ZF Net, 466\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "819\n",
      "\n",
      "\fAbout the Author [{'source': './data/ml.pdf', 'start_index': 1770749}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "743\n",
      "\n",
      "\fsuffers less from unstable gradients. One or more 1D convolutional layers can be\n",
      "useful in an RNN to efficiently preprocess the inputs, for example to reduce their\n",
      "temporal  resolution  (downsampling)  and  thereby  help  the  RNN  layers  detect\n",
      "long-term  patterns.  In  fact,  it  is  possible  to  use  only  convolutional  layers,  for\n",
      "example by building a WaveNet architecture. [{'source': './data/ml.pdf', 'start_index': 1627271}]\n",
      "* 5 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,\n",
      "\n",
      "no. 11 (1998): 2278–2324.\n",
      "\n",
      "The Architecture of the Visual Cortex \n",
      "\n",
      "| \n",
      "\n",
      "447\n",
      "\n",
      "\fConvolutional Layers\n",
      "The most important building block of a CNN is the convolutional layer:6 neurons in\n",
      "the first convolutional layer are not connected to every single pixel in the input image\n",
      "(like they were in the layers discussed in previous chapters), but only to pixels in their\n",
      "receptive  fields  (see  Figure  14-2).  In  turn,  each  neuron  in  the  second  convolutional\n",
      "layer is connected only to neurons located within a small rectangle in the first layer.\n",
      "This architecture allows the network to concentrate on small low-level features in the\n",
      "first  hidden  layer,  then  assemble  them  into  larger  higher-level  features  in  the  next\n",
      "hidden layer, and so on. This hierarchical structure is common in real-world images,\n",
      "which is one of the reasons why CNNs work so well for image recognition. [{'source': './data/ml.pdf', 'start_index': 970630}]\n",
      "* Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "vii\n",
      "\n",
      "\fSaving and Restoring a Model                                                                                314\n",
      "Using Callbacks                                                                                                         315\n",
      "Using TensorBoard for Visualization                                                                    317\n",
      "Fine-Tuning Neural Network Hyperparameters                                                     320\n",
      "Number of Hidden Layers                                                                                       323\n",
      "Number of Neurons per Hidden Layer                                                                 324\n",
      "Learning Rate, Batch Size, and Other Hyperparameters                                    325\n",
      "Exercises                                                                                                                        327 [{'source': './data/ml.pdf', 'start_index': 24201}]\n",
      "* 17 Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,” Proceedings\n",
      "\n",
      "of the 32nd International Conference on Machine Learning (2015): 2048–2057.\n",
      "\n",
      "552 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fFigure 16-7. Visual attention: an input image (left) and the model’s focus before produc‐\n",
      "ing the word “frisbee” (right)18 [{'source': './data/ml.pdf', 'start_index': 1199693}]\n",
      "* Figure 14-3. Connections between layers and zero padding\n",
      "\n",
      "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
      "the  receptive  fields,  as  shown  in  Figure  14-4.  This  dramatically  reduces  the  model’s\n",
      "computational complexity. The shift from one receptive field to the next is called the\n",
      "stride. In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4\n",
      "layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride is the\n",
      "same  in  both  directions,  but  it  does  not  have  to  be  so).  A  neuron  located  in  row  i,\n",
      "column j in the upper layer is connected to the outputs of the neurons in the previous\n",
      "layer located in rows i × sh to i × sh + fh – 1, columns j × sw to j × sw + fw – 1, where sh\n",
      "and sw are the vertical and horizontal strides.\n",
      "\n",
      "Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "449\n",
      "\n",
      "\fFigure 14-4. Reducing dimensionality using a stride of 2 [{'source': './data/ml.pdf', 'start_index': 972972}]\n",
      "* Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "603\n",
      "\n",
      "\favoid division by zero). This technique avoids explosions in the activations due\n",
      "to excessive competition between the generator and the discriminator.\n",
      "\n",
      "The  combination  of  all  these  techniques  allowed  the  authors  to  generate  extremely\n",
      "convincing high-definition images of faces. But what exactly do we call “convincing”?\n",
      "Evaluation is one of the big challenges when working with GANs: although it is possi‐\n",
      "ble to automatically evaluate the diversity of the generated images, judging their qual‐\n",
      "ity is a much trickier and subjective task. One technique is to use human raters, but\n",
      "this is costly and time-consuming. So the authors proposed to measure the similarity\n",
      "between  the  local  image  structure  of  the  generated  images  and  the  training  images,\n",
      "considering  every  scale.  This  idea  led  them  to  another  groundbreaking  innovation:\n",
      "StyleGANs. [{'source': './data/ml.pdf', 'start_index': 1306178}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "741\n",
      "\n",
      "\fsible to convert a trained CNN this way by appropriately reshaping the dense lay‐\n",
      "ers’ weight matrices.\n",
      "\n",
      "8. The main technical difficulty of semantic segmentation is the fact that a lot of the\n",
      "spatial  information  gets  lost  in  a  CNN  as  the  signal  flows  through  each  layer,\n",
      "especially  in  pooling  layers  and  layers  with  a  stride  greater  than  1.  This  spatial\n",
      "information needs to be restored somehow to accurately predict the class of each\n",
      "pixel.\n",
      "\n",
      "For  the  solutions  to  exercises  9  to  12,  please  see  the  Jupyter  notebooks  available  at\n",
      "https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "1. Here are a few RNN applications: [{'source': './data/ml.pdf', 'start_index': 1621431}]\n",
      "* 460 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\ffully  connected  layers  (+ReLUs),  and  the  final  layer  outputs  the  prediction  (e.g.,  a\n",
      "softmax layer that outputs estimated class probabilities).\n",
      "\n",
      "Figure 14-11. Typical CNN architecture\n",
      "\n",
      "A common mistake is to use convolution kernels that are too large.\n",
      "For  example,  instead  of  using  a  convolutional  layer  with  a  5  ×  5\n",
      "kernel, stack two layers with 3 × 3 kernels: it will use fewer parame‐\n",
      "ters  and  require  fewer  computations,  and  it  will  usually  perform\n",
      "better. One exception is for the first convolutional layer: it can typi‐\n",
      "cally have a large kernel (e.g., 5 × 5), usually with a stride of 2 or\n",
      "more: this will reduce the spatial dimension of the image without\n",
      "losing  too  much  information,  and  since  the  input  image  only  has\n",
      "three channels in general, it will not be too costly. [{'source': './data/ml.pdf', 'start_index': 995338}]\n",
      "* Now let’s look at the second common building block of CNNs: the pooling layer.\n",
      "\n",
      "Pooling Layers\n",
      "Once  you  understand  how  convolutional  layers  work,  the  pooling  layers  are  quite\n",
      "easy  to  grasp.  Their  goal  is  to  subsample  (i.e.,  shrink)  the  input  image  in  order  to\n",
      "\n",
      "7 A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\n",
      "\n",
      "× 1002 × 3 = 675 million parameters!\n",
      "\n",
      "8 In the international system of units (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\n",
      "\n",
      "456 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\freduce  the  computational  load,  the  memory  usage,  and  the  number  of  parameters\n",
      "(thereby limiting the risk of overfitting). [{'source': './data/ml.pdf', 'start_index': 986538}]\n",
      "* 472 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-17. ResNet architecture\n",
      "\n",
      "Note that the number of feature maps is doubled every few residual units, at the same\n",
      "time as their height and width are halved (using a convolutional layer with stride 2).\n",
      "When this happens, the inputs cannot be added directly to the outputs of the residual\n",
      "unit  because  they  don’t  have  the  same  shape  (for  example,  this  problem  affects  the\n",
      "skip connection represented by the dashed arrow in Figure 14-17). To solve this prob‐\n",
      "lem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the\n",
      "right number of output feature maps (see Figure 14-18).\n",
      "\n",
      "Figure 14-18. Skip connection when changing feature map size and depth\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "473 [{'source': './data/ml.pdf', 'start_index': 1017733}]\n",
      "* 740 \n",
      "\n",
      "|  Appendix A: Exercise Solutions\n",
      "\n",
      "\f3. If your GPU runs out of memory while training a CNN, here are five things you\n",
      "could try to solve the problem (other than purchasing a GPU with more RAM):\n",
      "\n",
      "• Reduce the mini-batch size.\n",
      "\n",
      "• Reduce dimensionality using a larger stride in one or more layers.\n",
      "\n",
      "• Remove one or more layers.\n",
      "\n",
      "• Use 16-bit floats instead of 32-bit floats.\n",
      "\n",
      "• Distribute the CNN across multiple devices.\n",
      "\n",
      "4. A max pooling layer has no parameters at all, whereas a convolutional layer has\n",
      "\n",
      "quite a few (see the previous questions).\n",
      "\n",
      "5. A local response normalization layer makes the neurons that most strongly acti‐\n",
      "vate inhibit neurons at the same location but in neighboring feature maps, which\n",
      "encourages  different  feature  maps  to  specialize  and  pushes  them  apart,  forcing\n",
      "them to explore a wider range of features. It is typically used in the lower layers to\n",
      "have a larger pool of low-level features that the upper layers can build upon. [{'source': './data/ml.pdf', 'start_index': 1618393}]\n",
      "* (2017).\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "323\n",
      "\n",
      "\famount  of  time:  you  would  have  to  draw  each  tree  individually,  branch  by  branch,\n",
      "leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch,\n",
      "then copy and paste that branch to create a tree, and finally copy and paste this tree to\n",
      "make a forest, you would be finished in no time. Real-world data is often structured\n",
      "in such a hierarchical way, and deep neural networks automatically take advantage of\n",
      "this fact: lower hidden layers model low-level structures (e.g., line segments of vari‐\n",
      "ous  shapes  and  orientations),  intermediate  hidden  layers  combine  these  low-level\n",
      "structures to model intermediate-level structures (e.g., squares, circles), and the high‐\n",
      "est  hidden  layers  and  the  output  layer  combine  these  intermediate  structures  to\n",
      "model high-level structures (e.g., faces). [{'source': './data/ml.pdf', 'start_index': 685755}]\n",
      "* For example, suppose we’d already trained a CNN for flower classification and locali‐\n",
      "zation. It was trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4\n",
      "are sent through the softmax activation function, and this gives the class probabilities\n",
      "(one per class); output 5 is sent through the logistic activation function, and this gives\n",
      "the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐\n",
      "resent the bounding box’s center coordinates, as well as its height and width. We can\n",
      "now  convert  its  dense  layers  to  convolutional  layers.  In  fact,  we  don’t  even  need  to\n",
      "retrain it; we can just copy the weights from the dense layers to the convolutional lay‐\n",
      "ers! Alternatively, we could have converted the CNN into an FCN before training. [{'source': './data/ml.pdf', 'start_index': 1051139}]\n",
      "* hence the name of these modules.\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "467\n",
      "\n",
      "\ftional  cost  and  the  number  of  parameters,  speeding  up  training  and  improving\n",
      "generalization.\n",
      "\n",
      "• Each  pair  of  convolutional  layers  ([1  ×  1,  3  ×  3]  and  [1  ×  1,  5  ×  5])  acts  like  a\n",
      "single powerful convolutional layer, capable of capturing more complex patterns.\n",
      "Indeed, instead of sweeping a simple linear classifier across the image (as a single\n",
      "convolutional  layer  does),  this  pair  of  convolutional  layers  sweeps  a  two-layer\n",
      "neural network across the image.\n",
      "\n",
      "In  short,  you  can  think  of  the  whole  inception  module  as  a  convolutional  layer  on\n",
      "steroids, able to output feature maps that capture complex patterns at various scales.\n",
      "\n",
      "The number of convolutional kernels for each convolutional layer\n",
      "is  a  hyperparameter.  Unfortunately,  this  means  that  you  have  six\n",
      "more hyperparameters to tweak for every inception layer you add. [{'source': './data/ml.pdf', 'start_index': 1009843}]\n",
      "* | \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-5. Applying two different filters to get two feature maps\n",
      "\n",
      "Stacking Multiple Feature Maps\n",
      "Up to now, for simplicity, I have represented the output of each convolutional layer as\n",
      "a  2D  layer,  but  in  reality  a  convolutional  layer  has  multiple  filters  (you  decide  how\n",
      "many) and outputs one feature map per filter, so it is more accurately represented in\n",
      "3D (see Figure 14-6). It has one neuron per pixel in each feature map, and all neurons\n",
      "within a given feature map share the same parameters (i.e., the same weights and bias\n",
      "term). Neurons in different feature maps use different parameters. A neuron’s recep‐\n",
      "tive field is the same as described earlier, but it extends across all the previous layers’\n",
      "feature maps. In short, a convolutional layer simultaneously applies multiple trainable\n",
      "filters  to  its  inputs,  making  it  capable  of  detecting  multiple  features  anywhere  in  its\n",
      "inputs. [{'source': './data/ml.pdf', 'start_index': 975665}]\n",
      "* CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "471\n",
      "\n",
      "\fFigure 14-16. Regular deep neural network (left) and deep residual network (right)\n",
      "\n",
      "Now let’s look at ResNet’s architecture (see Figure 14-17). It is surprisingly simple. It\n",
      "starts  and  ends  exactly  like  GoogLeNet  (except  without  a  dropout  layer),  and  in\n",
      "between is just a very deep stack of simple residual units. Each residual unit is com‐\n",
      "posed of two convolutional layers (and no pooling layer!), with Batch Normalization\n",
      "(BN)  and  ReLU  activation,  using  3  ×  3  kernels  and  preserving  spatial  dimensions\n",
      "(stride 1, \"same\" padding).\n",
      "\n",
      "472 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-17. ResNet architecture [{'source': './data/ml.pdf', 'start_index': 1017145}]\n",
      "* Customizing Models and Training Algorithms                                                       384\n",
      "Custom Loss Functions                                                                                           384\n",
      "Saving and Loading Models That Contain Custom Components                     385\n",
      "Custom Activation Functions, Initializers, Regularizers, and Constraints      387\n",
      "Custom Metrics                                                                                                         388\n",
      "Custom Layers                                                                                                           391\n",
      "Custom Models                                                                                                         394\n",
      "Losses and Metrics Based on Model Internals                                                      397\n",
      "Computing Gradients Using Autodiff                                                                   399 [{'source': './data/ml.pdf', 'start_index': 28898}]\n",
      "* (CUDA), 690\n",
      "\n",
      "concatenative attention, 550\n",
      "concrete functions, 791\n",
      "conditional probability, 547\n",
      "confusion matrix, 90\n",
      "connectionism, 280\n",
      "constrained optimization, 166\n",
      "Contrastive Divergence, 777\n",
      "convergence, 118\n",
      "convex function, 120\n",
      "convolution kernels, 450\n",
      "convolutional autoencoders, 579\n",
      "convolutional layer\n",
      "filters, 450\n",
      "memory requirements, 456\n",
      "overview of, 448\n",
      "stacking multiple feature maps, 451\n",
      "TensorFlow implementation, 453\n",
      "Convolutional Neural Networks (CNNs)\n",
      "architecture of visual cortex, 446\n",
      "classification and localization, 483\n",
      "CNN architectures, 460-478\n",
      "convolutional layer, 448-456\n",
      "object detection, 485-492\n",
      "overview of, 445\n",
      "pooling layer, 456\n",
      "pretrained models for transfer learning, 481\n",
      "pretrained models from Keras, 479\n",
      "ResNet-34 using Keras, 478\n",
      "semantic segmentation, 492\n",
      "\n",
      "core instances, 255\n",
      "corpus development, 24\n",
      "correlation coefficient, 58\n",
      "cost functions [{'source': './data/ml.pdf', 'start_index': 1734204}]\n",
      "* Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +\n",
      "1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and \"same\" padding. The input\n",
      "signal is first copied and fed to four different layers. All convolutional layers use the\n",
      "ReLU activation function. Note that the second set of convolutional layers uses differ‐\n",
      "ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different\n",
      "scales. Also note that every single layer uses a stride of 1 and \"same\" padding (even\n",
      "the max pooling layer), so their outputs all have the same height and width as their\n",
      "inputs. This makes it possible to concatenate all the outputs along the depth dimen‐\n",
      "sion  in  the  final  depth  concatenation  layer  (i.e.,  stack  the  feature  maps  from  all  four\n",
      "top  convolutional  layers).  This  concatenation  layer  can  be  implemented  in  Tensor‐\n",
      "Flow using the tf.concat() operation, with axis=3 (the axis is the depth).\n",
      "\n",
      "Figure 14-13. Inception module [{'source': './data/ml.pdf', 'start_index': 1008208}]\n",
      "* 15 Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recog‐\n",
      "\n",
      "nition,” arXiv preprint arXiv:1409.1556 (2014).\n",
      "\n",
      "470 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fResNet\n",
      "Kaiming  He  et  al.  won  the  ILSVRC  2015  challenge  using  a  Residual  Network  (or\n",
      "ResNet),16 that delivered an astounding top-five error rate under 3.6%. The winning\n",
      "variant used an extremely deep CNN composed of 152 layers (other variants had 34,\n",
      "50,  and  101  layers).  It  confirmed  the  general  trend:  models  are  getting  deeper  and\n",
      "deeper, with fewer and fewer parameters. The key to being able to train such a deep\n",
      "network is to use skip connections (also called shortcut connections): the signal feeding\n",
      "into a layer is also added to the output of a layer located a bit higher up the stack. Let’s\n",
      "see why this is useful. [{'source': './data/ml.pdf', 'start_index': 1014993}]\n",
      "* In  this  chapter  we  will  explore  where  CNNs  came  from,  what  their  building  blocks\n",
      "look like, and how to implement them using TensorFlow and Keras. Then we will dis‐\n",
      "cuss  some  of  the  best  CNN  architectures,  as  well  as  other  visual  tasks,  including\n",
      "\n",
      "445\n",
      "\n",
      "\fobject detection (classifying multiple objects in an image and placing bounding boxes\n",
      "around  them)  and  semantic  segmentation  (classifying  each  pixel  according  to  the\n",
      "class of the object it belongs to). [{'source': './data/ml.pdf', 'start_index': 966319}]\n",
      "* 560 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fIf  we  ignore  the  skip  connections,  the  layer  normalization  layers,  the  Feed  Forward\n",
      "blocks, and the fact that this is Scaled Dot-Product Attention, not exactly Multi-Head\n",
      "Attention, then the rest of the Transformer model can be implemented like this:\n",
      "\n",
      "Z = encoder_in\n",
      "for N in range(6):\n",
      "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
      "\n",
      "encoder_outputs = Z\n",
      "Z = decoder_in\n",
      "for N in range(6):\n",
      "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
      "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
      "\n",
      "outputs = keras.layers.TimeDistributed(\n",
      "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z) [{'source': './data/ml.pdf', 'start_index': 1216623}]\n",
      "* weights, just like in Bahdanau attention. Another simplification they proposed was to\n",
      "use the decoder’s hidden state at the current time step rather than at the previous time\n",
      "step  (i.e.,  h(t))  rather  than  h(t–1)),  then  to  use  the  output  of  the  attention  mechanism\n",
      "(noted   t )  directly  to  compute  the  decoder’s  predictions  (rather  than  using  it  to\n",
      "compute the decoder’s current hidden state). They also proposed a variant of the dot\n",
      "product mechanism where the encoder outputs first go through a linear transforma‐\n",
      "tion (i.e., a time-distributed Dense layer without a bias term) before the dot products\n",
      "are computed. This is called the “general” dot product approach. They compared both\n",
      "dot product approaches to the concatenative attention mechanism (adding a rescaling\n",
      "parameter vector v), and they observed that the dot product variants performed bet‐\n",
      "ter than concatenative attention. For this reason, concatenative attention is much less [{'source': './data/ml.pdf', 'start_index': 1196850}]\n",
      "* Fully Convolutional Networks\n",
      "The idea of FCNs was first introduced in a 2015 paper25 by Jonathan Long et al., for\n",
      "semantic  segmentation  (the  task  of  classifying  every  pixel  in  an  image  according  to\n",
      "the class of the object it belongs to). The authors pointed out that you could replace\n",
      "the dense layers at the top of a CNN by convolutional layers. To understand this, let’s\n",
      "look at an example: suppose a dense layer with 200 neurons sits on top of a convolu‐\n",
      "tional layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map\n",
      "size, not the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7\n",
      "activations  from  the  convolutional  layer  (plus  a  bias  term).  Now  let’s  see  what  hap‐\n",
      "pens if we replace the dense layer with a convolutional layer using 200 filters, each of\n",
      "size 7 × 7, and with \"valid\" padding. This layer will output 200 feature maps, each 1 [{'source': './data/ml.pdf', 'start_index': 1048414}]\n",
      "* 468 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-14. GoogLeNet architecture\n",
      "\n",
      "Let’s go through this network:\n",
      "\n",
      "• The first two layers divide the image’s height and width by 4 (so its area is divided\n",
      "by 16), to reduce the computational load. The first layer uses a large kernel size so\n",
      "that much of the information is preserved.\n",
      "\n",
      "• Then the local response normalization layer ensures that the previous layers learn\n",
      "\n",
      "a wide variety of features (as discussed earlier).\n",
      "\n",
      "• Two  convolutional  layers  follow,  where  the  first  acts  like  a  bottleneck  layer.  As\n",
      "explained  earlier,  you  can  think  of  this  pair  as  a  single  smarter  convolutional\n",
      "layer.\n",
      "\n",
      "• Again, a local response normalization layer ensures that the previous layers cap‐\n",
      "\n",
      "ture a wide variety of patterns.\n",
      "\n",
      "• Next,  a  max  pooling  layer  reduces  the  image  height  and  width  by  2,  again  to\n",
      "\n",
      "speed up computations. [{'source': './data/ml.pdf', 'start_index': 1011432}]\n",
      "* Then we can create the first layers of the Transformer:\n",
      "\n",
      "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
      "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
      "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
      "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
      "encoder_embeddings = embeddings(encoder_inputs)\n",
      "decoder_embeddings = embeddings(decoder_inputs)\n",
      "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
      "encoder_in = positional_encoding(encoder_embeddings)\n",
      "decoder_in = positional_encoding(decoder_embeddings)\n",
      "\n",
      "558 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fNow let’s look deeper into the heart of the Transformer model: the Multi-Head Atten‐\n",
      "tion layer.\n",
      "\n",
      "Multi-Head Attention [{'source': './data/ml.pdf', 'start_index': 1210825}]\n",
      "* This was the core idea in a groundbreaking 2014 paper13 by Dzmitry Bahdanau et al.\n",
      "They  introduced  a  technique  that  allowed  the  decoder  to  focus  on  the  appropriate\n",
      "words  (as  encoded  by  the  encoder)  at  each  time  step.  For  example,  at  the  time  step\n",
      "where  the  decoder  needs  to  output  the  word  “lait,”  it  will  focus  its  attention  on  the\n",
      "word “milk.” This means that the path from an input word to its translation is now\n",
      "much shorter, so the short-term memory limitations of RNNs have much less impact.\n",
      "Attention mechanisms revolutionized neural machine translation (and NLP in gen‐\n",
      "eral),  allowing  a  significant  improvement  in  the  state  of  the  art,  especially  for  long\n",
      "sentences (over 30 words).14 [{'source': './data/ml.pdf', 'start_index': 1191715}]\n",
      "* on Neural Information Processing Systems (2017): 972–981.\n",
      "\n",
      "The Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "337\n",
      "\n",
      "\f• The  paper  only  guarantees  self-normalization  if  all  layers  are  dense,  but  some\n",
      "researchers  have  noted  that  the  SELU  activation  function  can  improve  perfor‐\n",
      "mance in convolutional neural nets as well (see Chapter 14). [{'source': './data/ml.pdf', 'start_index': 718584}]\n",
      "* Figure 14-13. Inception module\n",
      "\n",
      "You  may  wonder  why  inception  modules  have  convolutional  layers  with  1  ×  1  ker‐\n",
      "nels.  Surely  these  layers  cannot  capture  any  features  because  they  look  at  only  one\n",
      "pixel at a time? In fact, the layers serve three purposes:\n",
      "\n",
      "• Although  they  cannot  capture  spatial  patterns,  they  can  capture  patterns  along\n",
      "\n",
      "the depth dimension.\n",
      "\n",
      "• They are configured to output fewer feature maps than their inputs, so they serve\n",
      "as bottleneck layers, meaning they reduce dimensionality. This cuts the computa‐\n",
      "\n",
      "14 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams;\n",
      "\n",
      "hence the name of these modules.\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "467\n",
      "\n",
      "\ftional  cost  and  the  number  of  parameters,  speeding  up  training  and  improving\n",
      "generalization. [{'source': './data/ml.pdf', 'start_index': 1009173}]\n",
      "* the 10th International Workshop on Artificial Intelligence and Statistics (2005): 59–66.\n",
      "\n",
      "Other Popular ANN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "777\n",
      "\n",
      "\fYee-Whye  Teh,  one  of  Geoffrey  Hinton’s  students,  observed  that  it  was  possible  to\n",
      "train DBNs one layer at a time using Contrastive Divergence, starting with the lower\n",
      "layers and then gradually moving up to the top layers. This led to the groundbreaking\n",
      "article that kickstarted the Deep Learning tsunami in 2006.2\n",
      "\n",
      "Just like RBMs, DBNs learn to reproduce the probability distribution of their inputs,\n",
      "without any supervision. However, they are much better at it, for the same reason that\n",
      "deep neural networks are more powerful than shallow ones: real-world data is often\n",
      "organized in hierarchical patterns, and DBNs take advantage of that. Their lower lay‐\n",
      "ers  learn  low-level  features  in  the  input  data,  while  higher  layers  learn  high-level\n",
      "features. [{'source': './data/ml.pdf', 'start_index': 1689465}]\n",
      "* Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\n",
      "parov back in 1996, it wasn’t until fairly recently that computers were able to reliably\n",
      "perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\n",
      "spoken words. Why are these tasks so effortless to us humans? The answer lies in the\n",
      "fact that perception largely takes place outside the realm of our consciousness, within\n",
      "specialized  visual,  auditory,  and  other  sensory  modules  in  our  brains.  By  the  time\n",
      "sensory information reaches our consciousness, it is already adorned with high-level\n",
      "features; for example, when you look at a picture of a cute puppy, you cannot choose\n",
      "not to see the puppy, not to notice its cuteness. Nor can you explain how you recog‐\n",
      "nize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective expe‐\n",
      "rience: perception is not trivial at all, and to understand it we must look at how the\n",
      "sensory modules work. [{'source': './data/ml.pdf', 'start_index': 964604}]\n",
      "* These  studies  of  the  visual  cortex  inspired  the  neocognitron,4  introduced  in  1980,\n",
      "which  gradually  evolved  into  what  we  now  call  convolutional  neural  networks.  An\n",
      "important  milestone  was  a  1998  paper5  by  Yann  LeCun  et  al.  that  introduced  the\n",
      "famous  LeNet-5  architecture,  widely  used  by  banks  to  recognize  handwritten  check\n",
      "numbers. This architecture has some building blocks that you already know, such as\n",
      "fully  connected  layers  and  sigmoid  activation  functions,  but  it  also  introduces  two\n",
      "new building blocks: convolutional layers and pooling layers. Let’s look at them now. [{'source': './data/ml.pdf', 'start_index': 969159}]\n",
      "* We will first look at the classical LeNet-5 architecture (1998), then three of the win‐\n",
      "ners  of  the  ILSVRC  challenge:  AlexNet  (2012),  GoogLeNet  (2014),  and  ResNet\n",
      "(2015).\n",
      "\n",
      "462 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fLeNet-5\n",
      "The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\n",
      "mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used\n",
      "for  handwritten  digit  recognition  (MNIST).  It  is  composed  of  the  layers  shown  in\n",
      "Table 14-1.\n",
      "\n",
      "Table 14-1. LeNet-5 architecture\n",
      "\n",
      "Kernel size\n",
      "–\n",
      "\n",
      "Stride Activation\n",
      "RBF\n",
      "–\n",
      "\n",
      "Layer\n",
      "Out\n",
      "\n",
      "Type\n",
      "Fully connected –\n",
      "\n",
      "Maps\n",
      "\n",
      "F6\n",
      "\n",
      "C5\n",
      "\n",
      "S4\n",
      "\n",
      "C3\n",
      "\n",
      "S2\n",
      "\n",
      "C1\n",
      "\n",
      "In\n",
      "\n",
      "Fully connected –\n",
      "\n",
      "Convolution\n",
      "\n",
      "Avg pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Avg pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Input\n",
      "\n",
      "120\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "Size\n",
      "10\n",
      "\n",
      "84\n",
      "\n",
      "1 × 1\n",
      "\n",
      "5 × 5\n",
      "\n",
      "–\n",
      "\n",
      "5 × 5\n",
      "\n",
      "2 × 2\n",
      "\n",
      "10 × 10\n",
      "\n",
      "5 × 5\n",
      "\n",
      "14 × 14\n",
      "\n",
      "2 × 2\n",
      "\n",
      "28 × 28\n",
      "\n",
      "5 × 5\n",
      "\n",
      "32 × 32 –\n",
      "\n",
      "–\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "–\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "– [{'source': './data/ml.pdf', 'start_index': 999684}]\n",
      "* ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\n",
      "ble,  making  them  ideal  to  tackle  large  and  highly  complex  Machine  Learning  tasks\n",
      "such as classifying billions of images (e.g., Google Images), powering speech recogni‐\n",
      "tion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\n",
      "of millions of users every day (e.g., YouTube), or learning to beat the world champion\n",
      "at the game of Go (DeepMind’s AlphaGo).\n",
      "\n",
      "The  first  part  of  this  chapter  introduces  artificial  neural  networks,  starting  with  a\n",
      "quick  tour  of  the  very  first  ANN  architectures  and  leading  up  to  Multilayer  Percep‐\n",
      "trons  (MLPs),  which  are  heavily  used  today  (other  architectures  will  be  explored  in\n",
      "the next chapters). In the second part, we will look at how to implement neural net‐\n",
      "works using the popular Keras API. This is a beautifully designed and simple high- [{'source': './data/ml.pdf', 'start_index': 586248}]\n",
      "* The Machine Learning Tsunami\n",
      "In 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\n",
      "network  capable  of  recognizing  handwritten  digits  with  state-of-the-art  precision\n",
      "(>98%).  They  branded  this  technique  “Deep  Learning.”  A  deep  neural  network  is  a\n",
      "(very) simplified model of our cerebral cortex, composed of a stack of layers of artifi‐\n",
      "cial  neurons.  Training  a  deep  neural  net  was  widely  considered  impossible  at  the\n",
      "time,2  and  most  researchers  had  abandoned  the  idea  in  the  late  1990s.  This  paper\n",
      "revived  the  interest  of  the  scientific  community,  and  before  long  many  new  papers\n",
      "demonstrated  that  Deep  Learning  was  not  only  possible,  but  capable  of  mind-\n",
      "blowing achievements that no other Machine Learning (ML) technique could hope to\n",
      "match  (with  the  help  of  tremendous  computing  power  and  great  amounts  of  data).\n",
      "This enthusiasm soon extended to many other areas of Machine Learning. [{'source': './data/ml.pdf', 'start_index': 49790}]\n",
      "* Masking\n",
      "As it stands, the model will need to learn that the padding tokens should be ignored.\n",
      "But we already know that! Why don’t we tell the model to ignore the padding tokens,\n",
      "so that it can focus on the data that actually matters? It’s actually trivial: simply add\n",
      "\n",
      "538 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fmask_zero=True when creating the Embedding layer. This means that padding tokens\n",
      "(whose ID is 0)8 will be ignored by all downstream layers. That’s all! [{'source': './data/ml.pdf', 'start_index': 1167271}]\n",
      "* 324 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\ftures  can  coalesce  into  far  fewer  high-level  features.  A  typical  neural  network  for\n",
      "MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200,\n",
      "and the third with 100. However, this practice has been largely abandoned because it\n",
      "seems  that  using  the  same  number  of  neurons  in  all  hidden  layers  performs  just  as\n",
      "well  in  most  cases,  or  even  better;  plus,  there  is  only  one  hyperparameter  to  tune,\n",
      "instead of one per layer. That said, depending on the dataset, it can sometimes help to\n",
      "make the first hidden layer bigger than the others. [{'source': './data/ml.pdf', 'start_index': 688955}]\n",
      "* 576-579\n",
      "\n",
      "variational, 586-591\n",
      "\n",
      "AutoGraphs, 407\n",
      "automatic differentiation (autodiff), 290, 399,\n",
      "\n",
      "765-772\n",
      "AutoML, 323\n",
      "autonomous driving systems, 497\n",
      "autoregressive integrated moving average\n",
      "\n",
      "(ARIMA) models, 506\n",
      "average absolute deviation, 41\n",
      "average pooling layer, 459\n",
      "Average Precision (AP), 491\n",
      "\n",
      "B\n",
      "backpropagation, 289-292\n",
      "backpropagation through time (BPTT), 502\n",
      "bag of words, 438\n",
      "bagging and pasting\n",
      "\n",
      "out-of-bag evaluation, 195\n",
      "overview of, 192\n",
      "in Scikit-Learn, 194\n",
      "Bahdanau attention, 550\n",
      "bandwidth saturation, 708\n",
      "basic cells, 500\n",
      "Batch Gradient Descent, 121\n",
      "batch learning, 15\n",
      "Batch Normalization (BN), 339\n",
      "batch size, 325\n",
      "batched action step, 657\n",
      "batched time step, 657\n",
      "batched trajectory, 657 [{'source': './data/ml.pdf', 'start_index': 1730820}]\n",
      "* Exercises\n",
      "\n",
      "1. The  TensorFlow  Playground  is  a  handy  neural  network  simulator  built  by  the\n",
      "TensorFlow team. In this exercise, you will train several binary classifiers in just a\n",
      "few  clicks,  and  tweak  the  model’s  architecture  and  its  hyperparameters  to  gain\n",
      "some  intuition  on  how  neural  networks  work  and  what  their  hyperparameters\n",
      "do. Take some time to explore the following:\n",
      "\n",
      "a. The patterns learned by a neural net. Try training the default neural network\n",
      "by clicking the Run button (top left). Notice how it quickly finds a good solu‐\n",
      "tion  for  the  classification  task.  The  neurons  in  the  first  hidden  layer  have\n",
      "learned  simple  patterns,  while  the  neurons  in  the  second  hidden  layer  have\n",
      "learned  to  combine  the  simple  patterns  of  the  first  hidden  layer  into  more\n",
      "complex patterns. In general, the more layers there are, the more complex the\n",
      "patterns can be. [{'source': './data/ml.pdf', 'start_index': 696073}]\n",
      "* • When a CNN has learned a kernel that can detect a particular feature, it can\n",
      "detect that feature anywhere in the image. In contrast, when a DNN learns a\n",
      "feature  in  one  location,  it  can  detect  it  only  in  that  particular  location.  Since\n",
      "images  typically  have  very  repetitive  features,  CNNs  are  able  to  generalize\n",
      "much better than DNNs for image processing tasks such as classification, using\n",
      "fewer training examples.\n",
      "\n",
      "• Finally, a DNN has no prior knowledge of how pixels are organized; it does not\n",
      "know  that  nearby  pixels  are  close.  A  CNN’s  architecture  embeds  this  prior\n",
      "knowledge. Lower layers typically identify features in small areas of the images,\n",
      "while higher layers combine the lower-level features into larger features. This\n",
      "works well with most natural images, giving CNNs a decisive head start com‐\n",
      "pared to DNNs. [{'source': './data/ml.pdf', 'start_index': 1613864}]\n",
      "* Reusing Pretrained Embeddings                                                                            540\n",
      "An Encoder–Decoder Network for Neural Machine Translation                         542\n",
      "Bidirectional RNNs                                                                                                  546\n",
      "Beam Search                                                                                                              547\n",
      "Attention Mechanisms                                                                                                549\n",
      "Visual Attention                                                                                                        552\n",
      "Attention Is All You Need: The Transformer Architecture                                554\n",
      "Recent Innovations in Language Models                                                                  563 [{'source': './data/ml.pdf', 'start_index': 38861}]\n",
      "* Figure 14-2. CNN layers with rectangular local receptive fields\n",
      "\n",
      "All the multilayer neural networks we’ve looked at so far had layers\n",
      "composed  of  a  long  line  of  neurons,  and  we  had  to  flatten  input\n",
      "images to 1D before feeding them to the neural network. In a CNN\n",
      "each  layer  is  represented  in  2D,  which  makes  it  easier  to  match\n",
      "neurons with their corresponding inputs.\n",
      "\n",
      "6 A convolution is a mathematical operation that slides one function over another and measures the integral of\n",
      "their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform\n",
      "and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\n",
      "similar to convolutions (see https://homl.info/76 for more details).\n",
      "\n",
      "448 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks [{'source': './data/ml.pdf', 'start_index': 971628}]\n",
      "* Now  let’s  look  at  the  architecture  of  the  GoogLeNet  CNN  (see  Figure  14-14).  The\n",
      "number of feature maps output by each convolutional layer and each pooling layer is\n",
      "shown before the kernel size. The architecture is so deep that it has to be represented\n",
      "in three columns, but GoogLeNet is actually one tall stack, including nine inception\n",
      "modules (the boxes with the spinning tops). The six numbers in the inception mod‐\n",
      "ules represent the number of feature maps output by each convolutional layer in the\n",
      "module (in the same order as in Figure 14-13). Note that all the convolutional layers\n",
      "use the ReLU activation function.\n",
      "\n",
      "468 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-14. GoogLeNet architecture\n",
      "\n",
      "Let’s go through this network: [{'source': './data/ml.pdf', 'start_index': 1010795}]\n",
      "* Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "319\n",
      "\n",
      "\fThis is actually a useful visualization tool to have, even beyond TensorFlow or Deep\n",
      "Learning.\n",
      "\n",
      "Let’s summarize what you’ve learned so far in this chapter: we saw where neural nets\n",
      "came from, what an MLP is and how you can use it for classification and regression,\n",
      "how  to  use  tf.keras’s  Sequential  API  to  build  MLPs,  and  how  to  use  the  Functional\n",
      "API or the Subclassing API to build more complex model architectures. You learned\n",
      "how  to  save  and  restore  a  model  and  how  to  use  callbacks  for  checkpointing,  early\n",
      "stopping,  and  more.  Finally,  you  learned  how  to  use  TensorBoard  for  visualization.\n",
      "You can already go ahead and use neural networks to tackle many problems! How‐\n",
      "ever,  you  may  wonder  how  to  choose  the  number  of  hidden  layers,  the  number  of\n",
      "neurons in the network, and all the other hyperparameters. Let’s look at this now. [{'source': './data/ml.pdf', 'start_index': 675162}]\n",
      "* — The  encoder’s  Multi-Head  Attention  layer  encodes  each  word’s  relationship\n",
      "with  every  other  word  in  the  same  sentence,  paying  more  attention  to  the\n",
      "most relevant ones. For example, the output of this layer for the word “Queen”\n",
      "in  the  sentence  “They  welcomed  the  Queen  of  the  United  Kingdom”  will\n",
      "depend on all the words in the sentence, but it will probably pay more atten‐\n",
      "tion to the words “United” and “Kingdom” than to the words “They” or “wel‐\n",
      "comed.”  This  attention  mechanism  is  called  self-attention  (the  sentence  is\n",
      "paying  attention  to  itself).  We  will  discuss  exactly  how  it  works  shortly.  The\n",
      "decoder’s  Masked  Multi-Head  Attention  layer  does  the  same  thing,  but  each\n",
      "word is only allowed to attend to words located before it. Finally, the decoder’s\n",
      "upper Multi-Head Attention layer is where the decoder pays attention to the\n",
      "words in the input sentence. For example, the decoder will probably pay close [{'source': './data/ml.pdf', 'start_index': 1205229}]\n",
      "* Figure 6-2. Decision Tree decision boundaries\n",
      "\n",
      "Model Interpretation: White Box Versus Black Box\n",
      "Decision Trees are intuitive, and their decisions are easy to interpret. Such models are\n",
      "often called white box models. In contrast, as we will see, Random Forests or neural\n",
      "networks  are  generally  considered  black  box  models.  They  make  great  predictions,\n",
      "and you can easily check the calculations that they performed to make these predic‐\n",
      "tions; nevertheless, it is usually hard to explain in simple terms why the predictions\n",
      "were made. For example, if a neural network says that a particular person appears on\n",
      "a picture, it is hard to know what contributed to this prediction: did the model recog‐\n",
      "nize  that  person’s  eyes?  Their  mouth?  Their  nose?  Their  shoes?  Or  even  the  couch\n",
      "that they were sitting on? Conversely, Decision Trees provide nice, simple classifica‐\n",
      "tion rules that can even be applied manually if need be (e.g., for flower classification). [{'source': './data/ml.pdf', 'start_index': 402752}]\n",
      "* CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "461\n",
      "\n",
      "\fLet’s go through this model:\n",
      "\n",
      "• The first layer uses 64 fairly large filters (7 × 7) but no stride because the input\n",
      "images  are  not  very  large.  It  also  sets  input_shape=[28,  28,  1],  because  the\n",
      "images are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
      "\n",
      "• Next we have a max pooling layer which uses a pool size of 2, so it divides each\n",
      "\n",
      "spatial dimension by a factor of 2.\n",
      "\n",
      "• Then we repeat the same structure twice: two convolutional layers followed by a\n",
      "max pooling layer. For larger images, we could repeat this structure several more\n",
      "times (the number of repetitions is a hyperparameter you can tune). [{'source': './data/ml.pdf', 'start_index': 997114}]\n",
      "* Next, the encoding network will optionally apply a list of convolutions sequentially,\n",
      "provided  you  specify  their  parameters  via  the  conv_layer_params  argument.  This\n",
      "must  be  a  list  composed  of  3-tuples  (one  per  convolutional  layer)  indicating  the\n",
      "\n",
      "The TF-Agents Library \n",
      "\n",
      "| \n",
      "\n",
      "651\n",
      "\n",
      "\fnumber of filters, the kernel size, and the stride. After these convolutional layers, the\n",
      "encoding  network  will  optionally  apply  a  sequence  of  dense  layers,  if  you  set  the\n",
      "fc_layer_params argument: it must be a list containing the number of neurons for\n",
      "each dense layer. Optionally, you can also pass a list of dropout rates (one per dense\n",
      "layer)  via  the  dropout_layer_params  argument  if  you  want  to  apply  dropout  after\n",
      "each dense layer. The QNetwork takes the output of this encoding network and passes\n",
      "it to the dense output layer (with one unit per action).\n",
      "\n",
      "Figure 18-14. Architecture of an encoding network [{'source': './data/ml.pdf', 'start_index': 1411177}]\n",
      "* While writing this second edition, I was fortunate enough to get plenty of help from\n",
      "members  of  the  TensorFlow  team—in  particular  Martin  Wicke,  who  tirelessly\n",
      "answered dozens of my questions and dispatched the rest to the right people, includ‐\n",
      "ing  Karmel  Allison,  Paige  Bailey,  Eugene  Brevdo,  William  Chargin,  Daniel  “Wolff ”\n",
      "Dobson,  Nick  Felt,  Bruce  Fontaine,  Goldie  Gadde,  Sandeep  Gupta,  Priya  Gupta,\n",
      "Kevin  Haas,  Konstantinos  Katsiapis  ,Viacheslav  Kovalevskyi,  Allen  Lavoie,  Clemens\n",
      "Mewald, Dan Moldovan, Sean Morgan, Tom O’Malley, Alexandre Passos, André Sus‐\n",
      "ano  Pinto,  Anthony  Platanios,  Oscar  Ramirez,  Anna  Revinskaya,  Saurabh  Saxena,\n",
      "Ryan Sepassi, Jiri Simsa, Xiaodan Song, Christina Sorokin, Dustin Tran, Todd Wang,\n",
      "Pete  Warden  (who  also  reviewed  the  first  edition)  Edd  Wilder-James,  and  Yuefeng\n",
      "Zhou, all of whom were tremendously helpful. Huge thanks to all of you, and to all [{'source': './data/ml.pdf', 'start_index': 69153}]\n",
      "* A decade or so later, Machine Learning has conquered the industry: it is at the heart\n",
      "of much of the magic in today’s high-tech products, ranking your web search results,\n",
      "powering your smartphone’s speech recognition, recommending videos, and beating\n",
      "the world champion at the game of Go. Before you know it, it will be driving your car.\n",
      "\n",
      "Machine Learning in Your Projects\n",
      "So,  naturally  you  are  excited  about  Machine  Learning  and  would  love  to  join  the\n",
      "party!\n",
      "\n",
      "1 Geoffrey E. Hinton et al., “A Fast Learning Algorithm for Deep Belief Nets,” Neural Computation 18 (2006):\n",
      "\n",
      "1527–1554.\n",
      "\n",
      "2 Despite the fact that Yann LeCun’s deep convolutional neural networks had worked well for image recognition\n",
      "\n",
      "since the 1990s, although they were not as general-purpose.\n",
      "\n",
      "xv\n",
      "\n",
      "\fPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\n",
      "ognize faces? Or learn to walk around? [{'source': './data/ml.pdf', 'start_index': 50788}]\n",
      "* 6. The  most  important  layer  in  the  Transformer  architecture  is  the  Multi-Head\n",
      "Attention  layer  (the  original  Transformer  architecture  contains  18  of  them,\n",
      "including  6  Masked  Multi-Head  Attention  layers).  It  is  at  the  core  of  language\n",
      "\n",
      "Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "745\n",
      "\n",
      "\fmodels  such  as  BERT  and  GPT-2.  Its  purpose  is  to  allow  the  model  to  identify\n",
      "which  words  are  most  aligned  with  each  other,  and  then  improve  each  word’s\n",
      "representation using these contextual clues. [{'source': './data/ml.pdf', 'start_index': 1633325}]\n",
      "* ference on Neural Information Processing Systems (2018): 698–707.\n",
      "\n",
      "Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "597\n",
      "\n",
      "\fDeep Convolutional GANs\n",
      "The  original  GAN  paper  in  2014  experimented  with  convolutional  layers,  but  only\n",
      "tried  to  generate  small  images.  Soon  after,  many  researchers  tried  to  build  GANs\n",
      "based  on  deeper  convolutional  nets  for  larger  images.  This  proved  to  be  tricky,  as\n",
      "training was very unstable, but Alec Radford et al. finally succeeded in late 2015, after\n",
      "experimenting  with  many  different  architectures  and  hyperparameters.  They  called\n",
      "their  architecture  deep  convolutional  GANs  (DCGANs).13  Here  are  the  main  guide‐\n",
      "lines they proposed for building stable convolutional GANs:\n",
      "\n",
      "• Replace any pooling layers with strided convolutions (in the discriminator) and\n",
      "\n",
      "transposed convolutions (in the generator).\n",
      "\n",
      "• Use Batch Normalization in both the generator and the discriminator, except in [{'source': './data/ml.pdf', 'start_index': 1293737}]\n",
      "* max pooling layers to reduce dimensionality and speed up the net.\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "469\n",
      "\n",
      "\f• Next, the global average pooling layer outputs the mean of each feature map: this\n",
      "drops  any  remaining  spatial  information,  which  is  fine  because  there  was  not\n",
      "much spatial information left at that point. Indeed, GoogLeNet input images are\n",
      "typically  expected  to  be  224  ×  224  pixels,  so  after  5  max  pooling  layers,  each\n",
      "dividing the height and width by 2, the feature maps are down to 7 × 7. More‐\n",
      "over,  it  is  a  classification  task,  not  localization,  so  it  does  not  matter  where  the\n",
      "object is. Thanks to the dimensionality reduction brought by this layer, there is\n",
      "no  need  to  have  several  fully  connected  layers  at  the  top  of  the  CNN  (like  in\n",
      "AlexNet),  and  this  considerably  reduces  the  number  of  parameters  in  the  net‐\n",
      "work and limits the risk of overfitting. [{'source': './data/ml.pdf', 'start_index': 1012465}]\n",
      "* Another approach, which often yields better results, is to measure the actual sparsity\n",
      "of the coding layer at each training iteration, and penalize the model when the meas‐\n",
      "ured sparsity differs from a target sparsity. We do so by computing the average activa‐\n",
      "tion of each neuron in the coding layer, over the whole training batch. The batch size\n",
      "must not be too small, or else the mean will not be accurate. [{'source': './data/ml.pdf', 'start_index': 1263892}]\n",
      "* Now if all neurons in a layer use the same vertical line filter (and the same bias term),\n",
      "and you feed the network the input image shown in Figure 14-5 (the bottom image),\n",
      "the  layer  will  output  the  top-left  image.  Notice  that  the  vertical  white  lines  get\n",
      "enhanced while the rest gets blurred. Similarly, the upper-right image is what you get\n",
      "if all neurons use the same horizontal line filter; notice that the horizontal white lines\n",
      "get  enhanced  while  the  rest  is  blurred  out.  Thus,  a  layer  full  of  neurons  using  the\n",
      "same filter outputs a feature map, which highlights the areas in an image that activate\n",
      "the filter the most. Of course, you do not have to define the filters manually: instead,\n",
      "during training the convolutional layer will automatically learn the most useful filters\n",
      "for  its  task,  and  the  layers  above  will  learn  to  combine  them  into  more  complex\n",
      "patterns.\n",
      "\n",
      "450 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks [{'source': './data/ml.pdf', 'start_index': 974743}]\n",
      "* There are a few more innovations you might be interested in, such as the use of skip\n",
      "connections to recover some of the spatial resolution that is lost in the CNN (we will\n",
      "discuss this shortly, when we look at semantic segmentation). In the 2016 paper, the\n",
      "authors  introduce  the  YOLO9000  model  that  uses  hierarchical  classification:  the\n",
      "model predicts a probability for each node in a visual hierarchy called WordTree. This\n",
      "makes it possible for the network to predict with high confidence that an image rep‐\n",
      "resents, say, a dog, even though it is unsure what specific type of dog. I encourage you\n",
      "\n",
      "490 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fto go ahead and read all three papers: they are quite pleasant to read, and they pro‐\n",
      "vide  excellent  examples  of  how  Deep  Learning  systems  can  be  incrementally\n",
      "improved. [{'source': './data/ml.pdf', 'start_index': 1057479}]\n",
      "* regularizers,  weight  constraints,  and  more.  You  may  even  need  to  fully  control  the\n",
      "training loop itself, for example to apply special transformations or constraints to the\n",
      "gradients (beyond just clipping them) or to use multiple optimizers for different parts\n",
      "of the network. We will cover all these cases in this chapter, and we will also look at\n",
      "how you can boost your custom models and training algorithms using TensorFlow’s\n",
      "automatic graph generation feature. But first, let’s take a quick tour of TensorFlow. [{'source': './data/ml.pdf', 'start_index': 806706}]\n",
      "* The fact that all neurons in a feature map share the same parame‐\n",
      "ters dramatically reduces the number of parameters in the model.\n",
      "Once the CNN has learned to recognize a pattern in one location, it\n",
      "can  recognize  it  in  any  other  location.  In  contrast,  once  a  regular\n",
      "DNN has learned to recognize a pattern in one location, it can rec‐\n",
      "ognize it only in that particular location.\n",
      "\n",
      "Input  images  are  also  composed  of  multiple  sublayers:  one  per  color  channel.  There\n",
      "are  typically  three:  red,  green,  and  blue  (RGB).  Grayscale  images  have  just  one\n",
      "\n",
      "Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "451\n",
      "\n",
      "\fchannel,  but  some  images  may  have  much  more—for  example,  satellite  images  that\n",
      "capture extra light frequencies (such as infrared).\n",
      "\n",
      "Figure 14-6. Convolutional layers with multiple feature maps, and images with three\n",
      "color channels [{'source': './data/ml.pdf', 'start_index': 976662}]\n",
      "* 340 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fApplied to a state-of-the-art image classification model, Batch Normalization achieves\n",
      "the same accuracy with 14 times fewer training steps, and beats the original model by a\n",
      "significant margin. […] Using an ensemble of batch-normalized networks, we improve\n",
      "upon the best published result on ImageNet classification: reaching 4.9% top-5 valida‐\n",
      "tion error (and 4.8% test error), exceeding the accuracy of human raters.\n",
      "\n",
      "Finally,  like  a  gift  that  keeps  on  giving,  Batch  Normalization  acts  like  a  regularizer,\n",
      "reducing  the  need  for  other  regularization  techniques  (such  as  dropout,  described\n",
      "later in this chapter). [{'source': './data/ml.pdf', 'start_index': 726218}]\n",
      "* With  four  parameters  I  can  fit  an  elephant  and  with  five  I  can  make  him  wiggle  his\n",
      "trunk.\n",
      "\n",
      "—John von Neumann, cited by Enrico Fermi in Nature 427\n",
      "\n",
      "With thousands of parameters, you can fit the whole zoo. Deep neural networks typi‐\n",
      "cally have tens of thousands of parameters, sometimes even millions. This gives them\n",
      "an incredible amount of freedom and means they can fit a huge variety of complex\n",
      "datasets.  But  this  great  flexibility  also  makes  the  network  prone  to  overfitting  the\n",
      "training set. We need regularization.\n",
      "\n",
      "We  already  implemented  one  of  the  best  regularization  techniques  in  Chapter  10:\n",
      "early  stopping.  Moreover,  even  though  Batch  Normalization  was  designed  to  solve\n",
      "the unstable gradients problems, it also acts like a pretty good regularizer. In this sec‐\n",
      "tion we will examine other popular regularization techniques for neural networks: ℓ1\n",
      "and ℓ2 regularization, dropout, and max-norm regularization. [{'source': './data/ml.pdf', 'start_index': 781720}]\n",
      "* • Looking more closely, you can see that you are already familiar with most com‐\n",
      "ponents: there are two embedding layers, 5 × N skip connections, each of them\n",
      "followed by a layer normalization layer, 2 × N “Feed Forward” modules that are\n",
      "composed of two dense layers each (the first one using the ReLU activation func‐\n",
      "tion,  the  second  with  no  activation  function),  and  finally  the  output  layer  is  a\n",
      "dense  layer  using  the  softmax  activation  function.  All  of  these  layers  are  time-\n",
      "distributed, so each word is treated independently of all the others. But how can\n",
      "we translate a sentence by only looking at one word at a time? Well, that’s where\n",
      "the new components come in: [{'source': './data/ml.pdf', 'start_index': 1204530}]\n",
      "* 3. If your GPU runs out of memory while training a CNN, what are five things you\n",
      "\n",
      "could try to solve the problem?\n",
      "\n",
      "4. Why  would  you  want  to  add  a  max  pooling  layer  rather  than  a  convolutional\n",
      "\n",
      "layer with the same stride?\n",
      "\n",
      "5. When would you want to add a local response normalization layer?\n",
      "\n",
      "6. Can  you  name  the  main  innovations  in  AlexNet,  compared  to  LeNet-5?  What\n",
      "\n",
      "about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
      "\n",
      "7. What is a fully convolutional network? How can you convert a dense layer into a\n",
      "\n",
      "convolutional layer?\n",
      "\n",
      "8. What is the main technical difficulty of semantic segmentation?\n",
      "\n",
      "9. Build your own CNN from scratch and try to achieve the highest possible accu‐\n",
      "\n",
      "racy on MNIST.\n",
      "\n",
      "10. Use transfer learning for large image classification, going through these steps: [{'source': './data/ml.pdf', 'start_index': 1070852}]\n",
      "* For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\n",
      "maps of size 150 × 100, with stride 1 and \"same\" padding. If the input is a 150 × 100\n",
      "RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\n",
      "= 15,200 (the + 1 corresponds to the bias terms), which is fairly small compared to a\n",
      "fully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\n",
      "rons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\n",
      "75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\n",
      "nected layer, but still quite computationally intensive. Moreover, if the feature maps\n",
      "are represented using 32-bit floats, then the convolutional layer’s output will occupy\n",
      "200  ×  150  ×  100  ×  32  =  96  million  bits  (12  MB)  of  RAM.8  And  that’s  just  for  one\n",
      "instance—if a training batch contains 100 instances, then this layer will use up 1.2 GB\n",
      "of RAM! [{'source': './data/ml.pdf', 'start_index': 984818}]\n",
      "* 448 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fA neuron located in row i, column j of a given layer is connected to the outputs of the\n",
      "neurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\n",
      "where  fh  and  fw  are  the  height  and  width  of  the  receptive  field  (see  Figure  14-3).  In\n",
      "order for a layer to have the same height and width as the previous layer, it is com‐\n",
      "mon  to  add  zeros  around  the  inputs,  as  shown  in  the  diagram.  This  is  called  zero\n",
      "padding.\n",
      "\n",
      "Figure 14-3. Connections between layers and zero padding [{'source': './data/ml.pdf', 'start_index': 972421}]\n",
      "* 454 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\f— If set to \"VALID\", the convolutional layer does not use zero padding and may\n",
      "ignore  some  rows  and  columns  at  the  bottom  and  right  of  the  input  image,\n",
      "depending on the stride, as shown in Figure 14-7 (for simplicity, only the hor‐\n",
      "izontal dimension is shown here, but of course the same logic applies to the\n",
      "vertical dimension). This means that every neuron’s receptive field lies strictly\n",
      "within valid positions inside the input (it does not go out of bounds), hence\n",
      "the name valid.\n",
      "\n",
      "Figure 14-7. Padding=\"SAME” or “VALID” (with input width 13, filter width 6, stride\n",
      "5)\n",
      "\n",
      "In  this  example  we  manually  defined  the  filters,  but  in  a  real  CNN  you  would  nor‐\n",
      "mally  define  filters  as  trainable  variables  so  the  neural  net  can  learn  which  filters\n",
      "work  best,  as  explained  earlier.  Instead  of  manually  creating  the  variables,  use  the\n",
      "keras.layers.Conv2D layer: [{'source': './data/ml.pdf', 'start_index': 982763}]\n",
      "* SENet\n",
      "The  winning  architecture  in  the  ILSVRC  2017  challenge  was  the  Squeeze-and-\n",
      "Excitation Network (SENet).22 This architecture extends existing architectures such as\n",
      "inception networks and ResNets, and boosts their performance. This allowed SENet\n",
      "to win the competition with an astonishing 2.25% top-five error rate! The extended\n",
      "versions  of  inception  networks  and  ResNets  are  called  SE-Inception  and  SE-ResNet,\n",
      "respectively. The boost comes from the fact that a SENet adds a small neural network,\n",
      "called  an  SE  block,  to  every  unit  in  the  original  architecture  (i.e.,  every  inception\n",
      "module or every residual unit), as shown in Figure 14-20.\n",
      "\n",
      "Figure 14-20. SE-Inception module (left) and SE-ResNet unit (right)\n",
      "\n",
      "21 Xingyu Zeng et al., “Crafting GBD-Net for Object Detection,” IEEE Transactions on Pattern Analysis and\n",
      "\n",
      "Machine Intelligence 40, no. 9 (2018): 2109–2123. [{'source': './data/ml.pdf', 'start_index': 1023745}]\n",
      "* MOT), 359\n",
      "\n",
      "TensorFlow Playground, 295\n",
      "TensorFlow, basics of\n",
      "architecture, 377\n",
      "benefits, xvi, 376\n",
      "community support, 379\n",
      "features, 376\n",
      "getting help, 379\n",
      "installing, 296\n",
      "library ecosystem, 378\n",
      "operating system compatibility, 378\n",
      "PyTorch library and, 296\n",
      "versions covered, 375\n",
      "\n",
      "TensorFlow, CNNs\n",
      "\n",
      "convolution operations, 494\n",
      "convolutional layers, 453\n",
      "\n",
      "\fpooling layer, 458\n",
      "\n",
      "TensorFlow, custom models and training\n",
      "\n",
      "about, 375\n",
      "activation functions, initializers, regulariz‐\n",
      "\n",
      "testing and validation\n",
      "data mismatch, 32\n",
      "hyperparameter tuning, 31\n",
      "model selection, 31\n",
      "\n",
      "ers, and constraints, 387\n",
      "\n",
      "text generation\n",
      "\n",
      "computing gradients using Autodiff, 399,\n",
      "\n",
      "765-772\n",
      "\n",
      "implementing learning rate scheduling, 363\n",
      "layers, 391\n",
      "loss functions, 384\n",
      "losses and metrics, 397\n",
      "metrics, 388\n",
      "models, 394\n",
      "saving and loading, 385\n",
      "special data structures, 783-789\n",
      "training loops, 402\n",
      "\n",
      "TensorFlow, data loading and preprocessing [{'source': './data/ml.pdf', 'start_index': 1765707}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "739\n",
      "\n",
      "\ffeature  map.  Since  this  first  convolutional  layer  has  100  feature  maps,  it  has  a\n",
      "total of 2,800 parameters. The second convolutional layer has 3 × 3 kernels and\n",
      "its input is the set of 100 feature maps of the previous layer, so each feature map\n",
      "has 3 × 3 × 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this\n",
      "layer  has  901  ×  200  =  180,200  parameters.  Finally,  the  third  and  last  convolu‐\n",
      "tional layer also has 3 × 3 kernels, and its input is the set of 200 feature maps of\n",
      "the previous layers, so each feature map has 3 × 3 × 200 = 1,800 weights, plus a\n",
      "bias  term.  Since  it  has  400  feature  maps,  this  layer  has  a  total  of  1,801  ×  400  =\n",
      "720,400 parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 = 903,400\n",
      "parameters. [{'source': './data/ml.pdf', 'start_index': 1614982}]\n",
      "* Building Complex Models Using the Functional API\n",
      "One  example  of  a  nonsequential  neural  network  is  a  Wide  &  Deep  neural  network.\n",
      "This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng\n",
      "et  al.16  It  connects  all  or  part  of  the  inputs  directly  to  the  output  layer,  as  shown  in\n",
      "Figure 10-14. This architecture makes it possible for the neural network to learn both\n",
      "deep  patterns  (using  the  deep  path)  and  simple  rules  (through  the  short  path).17  In\n",
      "contrast,  a  regular  MLP  forces  all  the  data  to  flow  through  the  full  stack  of  layers;\n",
      "\n",
      "16 Heng-Tze Cheng et al., “Wide & Deep Learning for Recommender Systems,” Proceedings of the First Workshop\n",
      "\n",
      "on Deep Learning for Recommender Systems (2016): 7–10.\n",
      "\n",
      "17 The short path can also be used to provide manually engineered features to the neural network.\n",
      "\n",
      "308 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras [{'source': './data/ml.pdf', 'start_index': 649573}]\n",
      "* Fine-Tuning Neural Network Hyperparameters\n",
      "The flexibility of neural networks is also one of their main drawbacks: there are many\n",
      "hyperparameters  to  tweak.  Not  only  can  you  use  any  imaginable  network  architec‐\n",
      "ture, but even in a simple MLP you can change the number of layers, the number of\n",
      "neurons per layer, the type of activation function to use in each layer, the weight initi‐\n",
      "alization logic, and much more. How do you know what combination of hyperpara‐\n",
      "meters is the best for your task? [{'source': './data/ml.pdf', 'start_index': 676091}]\n",
      "* feature maps) and the new output layer. The weight of the new outputs is α, while the\n",
      "weight of the original outputs is 1 – α, and α is slowly increased from 0 to 1. In other\n",
      "words, the new convolutional layers (represented with dashed lines in Figure 17-19)\n",
      "are gradually faded in, while the original output layer is gradually faded out. A similar\n",
      "fade-in/fade-out  technique  is  used  when  a  new  convolutional  layer  is  added  to  the\n",
      "discriminator (followed by an average pooling layer for downsampling). [{'source': './data/ml.pdf', 'start_index': 1302541}]\n",
      "* 580 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fDenoising Autoencoders\n",
      "Another  way  to  force  the  autoencoder  to  learn  useful  features  is  to  add  noise  to  its\n",
      "inputs, training it to recover the original, noise-free inputs. This idea has been around\n",
      "since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008\n",
      "paper,5 Pascal Vincent et al. showed that autoencoders could also be used for feature\n",
      "extraction. In a 2010 paper,6 Vincent et al. introduced stacked denoising autoencoders.\n",
      "\n",
      "The  noise  can  be  pure  Gaussian  noise  added  to  the  inputs,  or  it  can  be  randomly\n",
      "switched-off  inputs,  just  like  in  dropout  (introduced  in  Chapter  11).  Figure  17-8\n",
      "shows both options.\n",
      "\n",
      "Figure 17-8. Denoising autoencoders, with Gaussian noise (left) or dropout (right) [{'source': './data/ml.pdf', 'start_index': 1258722}]\n",
      "* 11 Alex Krizhevsky et al., “ImageNet Classification with Deep Convolutional Neural Networks,” _Proceedings of\n",
      "\n",
      "the 25th International Conference on Neural Information Processing Systems 1 (2012): 1097–1105.\n",
      "\n",
      "464 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fData Augmentation\n",
      "Data  augmentation  artificially  increases  the  size  of  the  training  set  by  generating\n",
      "many realistic variants of each training instance. This reduces overfitting, making this\n",
      "a regularization technique. The generated instances should be as realistic as possible:\n",
      "ideally, given an image from the augmented training set, a human should not be able\n",
      "to tell whether it was augmented or not. Simply adding white noise will not help; the\n",
      "modifications should be learnable (white noise is not). [{'source': './data/ml.pdf', 'start_index': 1003764}]\n",
      "* Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fPART II\n",
      "Neural Networks and Deep Learning\n",
      "\n",
      "\f\fCHAPTER 10\n",
      "Introduction to Artificial Neural Networks\n",
      "with Keras\n",
      "\n",
      "Birds  inspired  us  to  fly,  burdock  plants  inspired  Velcro,  and  nature  has  inspired\n",
      "countless more inventions. It seems only logical, then, to look at the brain’s architec‐\n",
      "ture  for  inspiration  on  how  to  build  an  intelligent  machine.  This  is  the  logic  that\n",
      "sparked  artificial  neural  networks  (ANNs):  an  ANN  is  a  Machine  Learning  model\n",
      "inspired  by  the  networks  of  biological  neurons  found  in  our  brains.  However,\n",
      "although planes were inspired by birds, they don’t have to flap their wings. Similarly,\n",
      "ANNs  have  gradually  become  quite  different  from  their  biological  cousins.  Some\n",
      "researchers even argue that we should drop the biological analogy altogether (e.g., by\n",
      "saying  “units”  rather  than  “neurons”),  lest  we  restrict  our  creativity  to  biologically\n",
      "plausible systems.1 [{'source': './data/ml.pdf', 'start_index': 585250}]\n",
      "* This  was  quite  a  technical  chapter,  and  you  may  feel  that  it  is  a  bit  far  from  the\n",
      "abstract beauty of neural networks, but the fact is Deep Learning often involves large\n",
      "amounts  of  data,  and  knowing  how  to  load,  parse,  and  preprocess  it  efficiently  is  a\n",
      "crucial  skill  to  have.  In  the  next  chapter,  we  will  look  at  convolutional  neural  net‐\n",
      "works, which are among the most successful neural net architectures for image pro‐\n",
      "cessing and many other applications.\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. Why would you want to use the Data API?\n",
      "\n",
      "2. What are the benefits of splitting a large dataset into multiple files?\n",
      "\n",
      "3. During  training,  how  can  you  tell  that  your  input  pipeline  is  the  bottleneck?\n",
      "\n",
      "What can you do to fix it?\n",
      "\n",
      "4. Can  you  save  any  binary  data  to  a  TFRecord  file,  or  only  serialized  protocol\n",
      "\n",
      "buffers?\n",
      "\n",
      "5. Why would you go through the hassle of converting all your data to the Example [{'source': './data/ml.pdf', 'start_index': 960274}]\n",
      "* channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\n",
      "makes the strong assumption that spatial patterns and cross-channel patterns can be\n",
      "modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part\n",
      "applies  a  single  spatial  filter  for  each  input  feature  map,  then  the  second  part  looks [{'source': './data/ml.pdf', 'start_index': 1020490}]\n",
      "* 11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   331\n",
      "The Vanishing/Exploding Gradients Problems                                                       332\n",
      "Glorot and He Initialization                                                                                    333\n",
      "Nonsaturating Activation Functions                                                                     335\n",
      "Batch Normalization                                                                                                338\n",
      "Gradient Clipping                                                                                                     345\n",
      "Reusing Pretrained Layers                                                                                          345\n",
      "Transfer Learning with Keras                                                                                  347 [{'source': './data/ml.pdf', 'start_index': 25116}]\n",
      "* Xception\n",
      "Another  variant  of  the  GoogLeNet  architecture  is  worth  noting:  Xception19  (which\n",
      "stands for Extreme Inception) was proposed in 2016 by François Chollet (the author\n",
      "of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350\n",
      "million images and 17,000 classes). Just like Inception-v4, it merges the ideas of Goo‐\n",
      "gLeNet and ResNet, but it replaces the inception modules with a special type of layer\n",
      "called  a  depthwise  separable  convolution  layer  (or  separable  convolution  layer  for\n",
      "short20). These layers had been used before in some CNN architectures, but they were\n",
      "not  as  central  as  in  the  Xception  architecture.  While  a  regular  convolutional  layer\n",
      "uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\n",
      "channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\n",
      "makes the strong assumption that spatial patterns and cross-channel patterns can be [{'source': './data/ml.pdf', 'start_index': 1019682}]\n",
      "* Now suppose the last convolutional layer before the output layer (also called the bot‐\n",
      "tleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image\n",
      "(see the left side of Figure 14-25). If we feed the FCN a 448 × 448 image (see the right\n",
      "side  of  Figure  14-25),  the  bottleneck  layer  will  now  output  14  ×  14  feature  maps.27\n",
      "Since the dense output layer was replaced by a convolutional layer using 10 filters of\n",
      "size 7 × 7, with \"valid\" padding and stride 1, the output will be composed of 10 fea‐\n",
      "tures  maps,  each  of  size  8  ×  8  (since  14  –  7  +  1  =  8).  In  other  words,  the  FCN  will\n",
      "process the whole image only once, and it will output an 8 × 8 grid where each cell\n",
      "contains  10  numbers  (5  class  probabilities,  1  objectness  score,  and  4  bounding  box\n",
      "coordinates). It’s exactly like taking the original CNN and sliding it across the image\n",
      "using 8 steps per row and 8 steps per column. To visualize this, imagine chopping the [{'source': './data/ml.pdf', 'start_index': 1051934}]\n",
      "* Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "395\n",
      "\n",
      "\fvariables. The rest of this class is self-explanatory. Next, let’s use the Subclassing API\n",
      "to define the model itself:\n",
      "\n",
      "class ResidualRegressor(keras.Model):\n",
      "    def __init__(self, output_dim, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
      "                                          kernel_initializer=\"he_normal\")\n",
      "        self.block1 = ResidualBlock(2, 30)\n",
      "        self.block2 = ResidualBlock(2, 30)\n",
      "        self.out = keras.layers.Dense(output_dim)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = self.hidden1(inputs)\n",
      "        for _ in range(1 + 3):\n",
      "            Z = self.block1(Z)\n",
      "        Z = self.block2(Z)\n",
      "        return self.out(Z) [{'source': './data/ml.pdf', 'start_index': 852711}]\n",
      "* def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.main_layers:\n",
      "            Z = layer(Z)\n",
      "        skip_Z = inputs\n",
      "        for layer in self.skip_layers:\n",
      "            skip_Z = layer(skip_Z)\n",
      "        return self.activation(Z + skip_Z)\n",
      "\n",
      "478 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fAs you can see, this code matches Figure 14-18 pretty closely. In the constructor, we\n",
      "create all the layers we will need: the main layers are the ones on the right side of the\n",
      "diagram,  and  the  skip  layers  are  the  ones  on  the  left  (only  needed  if  the  stride  is\n",
      "greater than 1). Then in the call() method, we make the inputs go through the main\n",
      "layers and the skip layers (if any), then we add both outputs and apply the activation\n",
      "function. [{'source': './data/ml.pdf', 'start_index': 1028543}]\n",
      "* Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "387\n",
      "\n",
      "\fThe activation function will be applied to the output of this Dense layer, and its result\n",
      "will  be  passed  on  to  the  next  layer.  The  layer’s  weights  will  be  initialized  using  the\n",
      "value returned by the initializer. At each training step the weights will be passed to the\n",
      "regularization function to compute the regularization loss, which will be added to the\n",
      "main loss to get the final loss used for training. Finally, the constraint function will be\n",
      "called  after  each  training  step,  and  the  layer’s  weights  will  be  replaced  by  the  con‐\n",
      "strained weights. [{'source': './data/ml.pdf', 'start_index': 833665}]\n",
      "* 2. Cover  additional  libraries  and  APIs  (Keras,  the  Data  API,  TF-Agents  for  Rein‐\n",
      "forcement  Learning)  and  training  and  deploying  TF  models  at  scale  using  the\n",
      "Distribution  Strategies  API,  TF-Serving,  and  Google  Cloud  AI  Platform.  Also\n",
      "briefly introduce TF Transform, TFLite, TF Addons/Seq2Seq, and TensorFlow.js.\n",
      "\n",
      "3. Discuss some of the latest important results from Deep Learning research.\n",
      "\n",
      "4. Migrate  all  TensorFlow  chapters  to  TensorFlow  2,  and  use  TensorFlow’s  imple‐\n",
      "\n",
      "mentation of the Keras API (tf.keras) whenever possible.\n",
      "\n",
      "5. Update the code examples to use the latest versions of Scikit-Learn, NumPy, pan‐\n",
      "\n",
      "das, Matplotlib, and other libraries.\n",
      "\n",
      "6. Clarify  some  sections  and  fix  some  errors,  thanks  to  plenty  of  great  feedback\n",
      "\n",
      "from readers.\n",
      "\n",
      "Some  chapters  were  added,  others  were  rewritten,  and  a  few  were  reordered.  See\n",
      "https://homl.info/changes2 for more details on what changed in the second edition. [{'source': './data/ml.pdf', 'start_index': 58683}]\n",
      "* Max pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "256\n",
      "\n",
      "256\n",
      "\n",
      "384\n",
      "\n",
      "384\n",
      "\n",
      "256\n",
      "\n",
      "256\n",
      "\n",
      "96\n",
      "\n",
      "96\n",
      "\n",
      "F9\n",
      "\n",
      "S8\n",
      "\n",
      "C7\n",
      "\n",
      "C6\n",
      "\n",
      "C5\n",
      "\n",
      "S4\n",
      "\n",
      "C3\n",
      "\n",
      "S2\n",
      "\n",
      "C1\n",
      "\n",
      "In\n",
      "\n",
      "Size\n",
      "1,000\n",
      "\n",
      "4,096\n",
      "\n",
      "4,096\n",
      "\n",
      "6 × 6\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "27 × 27\n",
      "\n",
      "27 × 27\n",
      "\n",
      "55 × 55\n",
      "\n",
      "Kernel size\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "5 × 5\n",
      "\n",
      "3 × 3\n",
      "\n",
      "11 × 11\n",
      "\n",
      "Stride Padding Activation\n",
      "–\n",
      "\n",
      "Softmax\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "valid\n",
      "\n",
      "–\n",
      "\n",
      "same\n",
      "\n",
      "same\n",
      "\n",
      "same\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "valid\n",
      "\n",
      "–\n",
      "\n",
      "same\n",
      "\n",
      "ReLU\n",
      "\n",
      "valid\n",
      "\n",
      "–\n",
      "\n",
      "valid\n",
      "\n",
      "ReLU\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "Input\n",
      "\n",
      "3 (RGB)\n",
      "\n",
      "227 × 227 –\n",
      "\n",
      "To  reduce  overfitting,  the  authors  used  two  regularization  techniques.  First,  they\n",
      "applied dropout (introduced in Chapter 11) with a 50% dropout rate during training\n",
      "to  the  outputs  of  layers  F9  and  F10.  Second,  they  performed  data  augmentation  by\n",
      "randomly shifting the training images by various offsets, flipping them horizontally,\n",
      "and changing the lighting conditions. [{'source': './data/ml.pdf', 'start_index': 1002872}]\n",
      "* 5. An attention mechanism is a technique initially used in Encoder–Decoder mod‐\n",
      "els  to  give  the  decoder  more  direct  access  to  the  input  sequence,  allowing  it  to\n",
      "deal with longer input sequences. At each decoder time step, the current decod‐\n",
      "er’s state and the full output of the encoder are processed by an alignment model\n",
      "that  outputs  an  alignment  score  for  each  input  time  step.  This  score  indicates\n",
      "which  part  of  the  input  is  most  relevant  to  the  current  decoder  time  step.  The\n",
      "weighted sum of the encoder output (weighted by their alignment score) is then\n",
      "fed to the decoder, which produces the next decoder state and the output for this\n",
      "time step. The main benefit of using an attention mechanism is the fact that the\n",
      "Encoder–Decoder  model  can  successfully  process  longer  input  sequences.\n",
      "Another benefit is that the alignment scores makes the model easier to debug and [{'source': './data/ml.pdf', 'start_index': 1632097}]\n",
      "* I  am  also  incredibly  thankful  to  all  the  amazing  people  who  took  time  out  of  their\n",
      "busy lives to review my book with such care. In particular, I would like to thank Fran‐\n",
      "çois Chollet for reviewing all the chapters based on Keras and TensorFlow and giving\n",
      "me some great in-depth feedback. Since Keras is one of the main additions to this sec‐\n",
      "ond edition, having its author review the book was invaluable. I highly recommend\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xxiii [{'source': './data/ml.pdf', 'start_index': 67720}]\n",
      "* Another common attention mechanism was proposed shortly after, in a 2015 paper16\n",
      "by Minh-Thang Luong et al. Because the goal of the attention mechanism is to meas‐\n",
      "ure  the  similarity  between  one  of  the  encoder’s  outputs  and  the  decoder’s  previous\n",
      "hidden state, the authors proposed to simply compute the dot product (see Chapter 4)\n",
      "of  these  two  vectors,  as  this  is  often  a  fairly  good  similarity  measure,  and  modern\n",
      "hardware can compute it much faster. For this to be possible, both vectors must have\n",
      "the same dimensionality. This is called Luong attention (again, after the paper’s first\n",
      "author), or sometimes multiplicative attention. The dot product gives a score, and all\n",
      "the scores (at a given decoder time step) go through a softmax layer to give the final\n",
      "weights, just like in Bahdanau attention. Another simplification they proposed was to\n",
      "use the decoder’s hidden state at the current time step rather than at the previous time [{'source': './data/ml.pdf', 'start_index': 1196061}]\n",
      "* model = keras.models.Sequential()\n",
      "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
      "for rate in (1, 2, 4, 8) * 2:\n",
      "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
      "                                  activation=\"relu\", dilation_rate=rate))\n",
      "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
      "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
      "history = model.fit(X_train, Y_train, epochs=20,\n",
      "                    validation_data=(X_valid, Y_valid))\n",
      "\n",
      "This Sequential model starts with an explicit input layer (this is simpler than trying\n",
      "to  set  input_shape  only  on  the  first  layer),  then  continues  with  a  1D  convolutional\n",
      "layer using \"causal\" padding: this ensures that the convolutional layer does not peek\n",
      "into the future when making predictions (it is equivalent to padding the inputs with\n",
      "the  right  amount  of  zeros  on  the  left  and  using  \"valid\"  padding).  We  then  add [{'source': './data/ml.pdf', 'start_index': 1127506}]\n",
      "* 14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .   445\n",
      "The Architecture of the Visual Cortex                                                                      446\n",
      "Convolutional Layers                                                                                                   448\n",
      "Filters                                                                                                                          450\n",
      "Stacking Multiple Feature Maps                                                                             451\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "ix [{'source': './data/ml.pdf', 'start_index': 32843}]\n",
      "* outputs = keras.layers.TimeDistributed(\n",
      "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\n",
      "\n",
      "The  use_scale=True  argument  creates  an  additional  parameter  that  lets  the  layer\n",
      "learn how to properly downscale the similarity scores. This is a bit different from the\n",
      "Transformer model, which always downscales the similarity scores by the same factor\n",
      "( dkeys).  The  causal=True  argument  when  creating  the  second  attention  layer\n",
      "ensures  that  each  output  token  only  attends  to  previous  output  tokens,  not  future\n",
      "ones.\n",
      "\n",
      "Now it’s time to look at the final piece of the puzzle: what is a Multi-Head Attention\n",
      "layer? Its architecture is shown in Figure 16-10.\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "561\n",
      "\n",
      "\fFigure 16-10. Multi-Head Attention layer architecture23 [{'source': './data/ml.pdf', 'start_index': 1217245}]\n",
      "* Nonsaturating Activation Functions\n",
      "One  of  the  insights  in  the  2010  paper  by  Glorot  and  Bengio  was  that  the  problems\n",
      "with unstable gradients were in part due to a poor choice of activation function. Until\n",
      "then most people had assumed that if Mother Nature had chosen to use roughly sig‐\n",
      "moid activation functions in biological neurons, they must be an excellent choice. But\n",
      "it  turns  out  that  other  activation  functions  behave  much  better  in  deep  neural  net‐\n",
      "works—in particular, the ReLU activation function, mostly because it does not satu‐\n",
      "rate for positive values (and because it is fast to compute). [{'source': './data/ml.pdf', 'start_index': 711878}]\n",
      "* Encoder–Decoder  model  can  successfully  process  longer  input  sequences.\n",
      "Another benefit is that the alignment scores makes the model easier to debug and\n",
      "interpret: for example, if the model makes a mistake, you can look at which part\n",
      "of the input it was paying attention to, and this can help diagnose the issue. An\n",
      "attention  mechanism  is  also  at  the  core  of  the  Transformer  architecture,  in  the\n",
      "Multi-Head Attention layers. See the next answer. [{'source': './data/ml.pdf', 'start_index': 1632860}]\n",
      "* Transformer-like  architecture.  The  authors  pretrained  a  large  but  fairly  simple\n",
      "architecture composed of a stack of 12 Transformer modules (using only Masked\n",
      "Multi-Head  Attention  layers)  on  a  large  dataset,  once  again  trained  using  self-\n",
      "supervised  learning.  Then  they  fine-tuned  it  on  various  language  tasks,  using\n",
      "only minor adaptations for each task. The tasks were quite diverse: they included\n",
      "text classification, entailment (whether sentence A entails sentence B),27 similarity\n",
      "(e.g., “Nice weather today” is very similar to “It is sunny”), and question answer‐\n",
      "ing (given a few paragraphs of text giving some context, the model must answer\n",
      "some multiple-choice questions). Just a few months later, in February 2019, Alec\n",
      "Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper,28\n",
      "which proposed a very similar architecture, but larger still (with over 1.5 billion\n",
      "parameters!) and they showed that it could achieve good performance on many [{'source': './data/ml.pdf', 'start_index': 1222675}]\n",
      "* In the next chapter we will discuss how to learn deep representations in an unsuper‐\n",
      "vised  way  using  autoencoders,  and  we  will  use  generative  adversarial  networks\n",
      "(GANs) to produce images and more!\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
      "\n",
      "2. Why  do  people  use  Encoder–Decoder  RNNs  rather  than  plain  sequence-to-\n",
      "\n",
      "sequence RNNs for automatic translation?\n",
      "\n",
      "3. How  can  you  deal  with  variable-length  input  sequences?  What  about  variable-\n",
      "\n",
      "length output sequences?\n",
      "\n",
      "4. What is beam search and why would you use it? What tool can you use to imple‐\n",
      "\n",
      "ment it?\n",
      "\n",
      "5. What is an attention mechanism? How does it help?\n",
      "\n",
      "30 Maha Elbayad et al., “Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Pre‐\n",
      "\n",
      "diction,” arXiv preprint arXiv:1808.03867 (2018).\n",
      "\n",
      "31 Shuai Li et al., “Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN,” [{'source': './data/ml.pdf', 'start_index': 1227292}]\n",
      "* model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
      "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
      "                    validation_data=(X_valid, Y_valid[:, 3::2]))\n",
      "\n",
      "If you train and evaluate this model, you will find that it is the best model so far. The\n",
      "convolutional layer really helps. In fact, it is actually possible to use only 1D convolu‐\n",
      "tional layers and drop the recurrent layers entirely!\n",
      "\n",
      "WaveNet [{'source': './data/ml.pdf', 'start_index': 1125269}]\n",
      "* In  the  next  chapter  we  will  move  to  an  entirely  different  branch  of  Deep  Learning:\n",
      "Deep Reinforcement Learning.\n",
      "\n",
      "606 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fExercises\n",
      "\n",
      "1. What are the main tasks that autoencoders are used for?\n",
      "\n",
      "2. Suppose you want to train a classifier, and you have plenty of unlabeled training\n",
      "data  but  only  a  few  thousand  labeled  instances.  How  can  autoencoders  help?\n",
      "How would you proceed?\n",
      "\n",
      "3. If  an  autoencoder  perfectly  reconstructs  the  inputs,  is  it  necessarily  a  good\n",
      "\n",
      "autoencoder? How can you evaluate the performance of an autoencoder?\n",
      "\n",
      "4. What are undercomplete and overcomplete autoencoders? What is the main risk\n",
      "of  an  excessively  undercomplete  autoencoder?  What  about  the  main  risk  of  an\n",
      "overcomplete autoencoder?\n",
      "\n",
      "5. How do you tie weights in a stacked autoencoder? What is the point of doing so? [{'source': './data/ml.pdf', 'start_index': 1312698}]\n",
      "* class ReconstructingRegressor(keras.Model):\n",
      "    def __init__(self, output_dim, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
      "                                          kernel_initializer=\"lecun_normal\")\n",
      "                       for _ in range(5)]\n",
      "        self.out = keras.layers.Dense(output_dim)\n",
      "\n",
      "    def build(self, batch_input_shape):\n",
      "        n_inputs = batch_input_shape[-1]\n",
      "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
      "        super().build(batch_input_shape)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        reconstruction = self.reconstruct(Z)\n",
      "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
      "        self.add_loss(0.05 * recon_loss)\n",
      "        return self.out(Z)\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "397\n",
      "\n",
      "\fLet’s go through this code: [{'source': './data/ml.pdf', 'start_index': 856787}]\n",
      "* sented them in a couple of videos, with the corresponding code in a notebook). Now\n",
      "on to the next chapter, where we will look at how to process sequential data such as\n",
      "time series using recurrent neural networks and convolutional neural networks. [{'source': './data/ml.pdf', 'start_index': 1069739}]\n",
      "* As you can see, it is just a bunch of Scaled Dot-Product Attention layers, each pre‐\n",
      "ceded  by  a  linear  transformation  of  the  values,  keys,  and  queries  (i.e.,  a  time-\n",
      "distributed  Dense  layer  with  no  activation  function).  All  the  outputs  are  simply\n",
      "concatenated,  and  they  go  through  a  final  linear  transformation  (again,  time-\n",
      "distributed). But why? What is the intuition behind this architecture? Well, consider\n",
      "the  word  “played”  we  discussed  earlier  (in  the  sentence  “They  played  chess”).  The\n",
      "encoder was smart enough to encode the fact that it is a verb. But the word represen‐\n",
      "tation also includes its position in the text, thanks to the positional encodings, and it\n",
      "probably includes many other features that are useful for its translation, such as the\n",
      "fact that it is in the past tense. In short, the word representation encodes many differ‐\n",
      "ent characteristics of the word. If we just used a single Scaled Dot-Product Attention [{'source': './data/ml.pdf', 'start_index': 1218023}]\n",
      "* Pattern Recognition (2018): 7132–7141.\n",
      "\n",
      "476 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fAn SE block analyzes the output of the unit it is attached to, focusing exclusively on\n",
      "the depth dimension (it does not look for any spatial pattern), and it learns which fea‐\n",
      "tures are usually most active together. It then uses this information to recalibrate the\n",
      "feature  maps,  as  shown  in  Figure  14-21.  For  example,  an  SE  block  may  learn  that\n",
      "mouths, noses, and eyes usually appear together in pictures: if you see a mouth and a\n",
      "nose, you should expect to see eyes as well. So if the block sees a strong activation in\n",
      "the mouth and nose feature maps, but only mild activation in the eye feature map, it\n",
      "will  boost  the  eye  feature  map  (more  accurately,  it  will  reduce  irrelevant  feature\n",
      "maps).  If  the  eyes  were  somewhat  confused  with  something  else,  this  feature  map\n",
      "recalibration will help resolve the ambiguity. [{'source': './data/ml.pdf', 'start_index': 1024762}]\n",
      "* 332-345\n",
      "\n",
      "Deep Neuroevolution, 323\n",
      "Deep Q-Learning\n",
      "\n",
      "Double DQN, 640\n",
      "Dueling DQN, 641\n",
      "fixed Q-Value targets, 639\n",
      "implementing, 634\n",
      "overview of, 633\n",
      "prioritized experience replay, 640\n",
      "variants of, 639\n",
      "\n",
      "deep Q-networks (DQNs), 633, 650, 650\n",
      "denoising autoencoders, 581\n",
      "dense layer, 285\n",
      "dense vectors, 556\n",
      "density estimation, 236, 264\n",
      "depth concat layer, 467\n",
      "depth radius, 466\n",
      "depthwise separable convolution, 474\n",
      "deques, 635\n",
      "development sets (dev sets), 31\n",
      "differencing, 506\n",
      "dimensionality reduction\n",
      "\n",
      "additional techniques, 232\n",
      "approaches for, 215-218\n",
      "using clustering, 237\n",
      "curse of dimensionality, 214\n",
      "goal of, 12\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "805\n",
      "\n",
      "\fLLE (Locally Linear Embedding), 230\n",
      "overview of, 213\n",
      "PCA (Principal Component Analysis), [{'source': './data/ml.pdf', 'start_index': 1738366}]\n",
      "* The keras.layers.Attention layer implements Scaled Dot-Product Attention, effi‐\n",
      "ciently applying Equation 16-3 to multiple sentences in a batch. Its inputs are just like\n",
      "Q, K, and V, except with an extra batch dimension (the first dimension).\n",
      "\n",
      "In TensorFlow, if  A and  B are tensors with more than two dimen‐\n",
      "sions—say,  of  shape  [2,  3,  4,  5]  and  [2,  3,  5,  6]  respectively—then\n",
      "tf.matmul(A, B) will treat these tensors as 2 × 3 arrays where each\n",
      "cell contains a matrix, and it will multiply the corresponding matri‐\n",
      "ces: the matrix at the ith row and jth column in A will be multiplied\n",
      "by the matrix at the ith row and jth column in B. Since the product of\n",
      "a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6 matrix, tf.matmul(A,\n",
      "B) will return an array of shape [2, 3, 4, 6].\n",
      "\n",
      "560 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention [{'source': './data/ml.pdf', 'start_index': 1215839}]\n",
      "* backpropagation and, 289-292\n",
      "classification MLPs, 294\n",
      "regression MLPs, 292\n",
      "multinomial classifiers, 100\n",
      "Multinomial Logistic Regression, 148\n",
      "multioutput classification, 107\n",
      "multiple outputs, 311\n",
      "multiple regression problems, 39\n",
      "multiplicative attention, 551\n",
      "multitask classification, 311\n",
      "multivariate regression problems, 39\n",
      "multivariate time series, 503\n",
      "\n",
      "N\n",
      "naive forecasting, 505\n",
      "Nash equilibrium, 596\n",
      "natural language processing (NLP)\n",
      "attention mechanisms, 549-563\n",
      "CNNs for, 445\n",
      "Encoder–Decoder network for, 542-548\n",
      "\n",
      "generating text using character RNNs,\n",
      "\n",
      "526-534\n",
      "overview of, 525\n",
      "recent innovations in, 563\n",
      "RNNS for, 497\n",
      "sentiment analysis, 534-542\n",
      "uses for, 351\n",
      "nested datasets, 529\n",
      "Nesterov Accelerated Gradient (NAG), 353\n",
      "Nesterov momentum optimization, 353\n",
      "neural machine translation (NMT), 542-563\n",
      "(see also natural language processing\n",
      "(NLP))\n",
      "\n",
      "neurons [{'source': './data/ml.pdf', 'start_index': 1751871}]\n",
      "* pens if we replace the dense layer with a convolutional layer using 200 filters, each of\n",
      "size 7 × 7, and with \"valid\" padding. This layer will output 200 feature maps, each 1\n",
      "×  1  (since  the  kernel  is  exactly  the  size  of  the  input  feature  maps  and  we  are  using\n",
      "\"valid\"  padding).  In  other  words,  it  will  output  200  numbers,  just  like  the  dense\n",
      "layer did; and if you look closely at the computations performed by a convolutional\n",
      "layer, you will notice that these numbers will be precisely the same as those the dense\n",
      "layer  produced.  The  only  difference  is  that  the  dense  layer’s  output  was  a  tensor  of\n",
      "shape  [batch  size,  200],  while  the  convolutional  layer  will  output  a  tensor  of  shape\n",
      "[batch size, 1, 1, 200]. [{'source': './data/ml.pdf', 'start_index': 1049157}]\n",
      "* Pixelwise normalization layer\n",
      "\n",
      "Added after each convolutional layer in the generator. It normalizes each activa‐\n",
      "tion based on all the activations in the same image and at the same location, but\n",
      "across all channels (dividing by the square root of the mean squared activation).\n",
      "In TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),\n",
      "axis=-1,  keepdims=True)  +  1e-8)  (the  smoothing  term  1e-8  is  needed  to\n",
      "\n",
      "17 The dynamic range of a variable is the ratio between the highest and the lowest value it may take.\n",
      "\n",
      "Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "603\n",
      "\n",
      "\favoid division by zero). This technique avoids explosions in the activations due\n",
      "to excessive competition between the generator and the discriminator. [{'source': './data/ml.pdf', 'start_index': 1305643}]\n",
      "* 7. Fully convolutional networks are neural networks composed exclusively of con‐\n",
      "volutional and pooling layers. FCNs can efficiently process images of any width\n",
      "and  height  (at  least  above  the  minimum  size).  They  are  most  useful  for  object\n",
      "detection and semantic segmentation because they only need to look at the image\n",
      "once  (instead  of  having  to  run  a  CNN  multiple  times  on  different  parts  of  the\n",
      "image). If you have a CNN with some dense layers on top, you can convert these\n",
      "dense  layers  to  convolutional  layers  to  create  an  FCN:  just  replace  the  lowest\n",
      "dense layer with a convolutional layer with a kernel size equal to the layer’s input\n",
      "size,  with  one  filter  per  neuron  in  the  dense  layer,  and  using  \"valid\"  padding.\n",
      "Generally the stride should be 1, but you can set it to a higher value if you want.\n",
      "The activation function should be the same as the dense layer’s. The other dense [{'source': './data/ml.pdf', 'start_index': 1620407}]\n",
      "* Vision and Pattern Recognition (2015): 1–9.\n",
      "\n",
      "466 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fpossible  by  subnetworks  called  inception  modules,14  which  allow  GoogLeNet  to  use\n",
      "parameters  much  more  efficiently  than  previous  architectures:  GoogLeNet  actually\n",
      "has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million). [{'source': './data/ml.pdf', 'start_index': 1007815}]\n",
      "* Visualizing the Fashion MNIST Dataset\n",
      "Now that we have trained a stacked autoencoder, we can use it to reduce the dataset’s\n",
      "dimensionality. For visualization, this does not give great results compared to other\n",
      "dimensionality reduction algorithms (such as those we discussed in Chapter 8), but\n",
      "one big advantage of autoencoders is that they can handle large datasets, with many\n",
      "instances and many features. So one strategy is to use an autoencoder to reduce the\n",
      "dimensionality  down  to  a  reasonable  level,  then  use  another  dimensionality\n",
      "\n",
      "574 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\freduction  algorithm  for  visualization.  Let’s  use  this  strategy  to  visualize  Fashion\n",
      "MNIST. First, we use the encoder from our stacked autoencoder to reduce the dimen‐\n",
      "sionality  down  to  30,  then  we  use  Scikit-Learn’s  implementation  of  the  t-SNE  algo‐\n",
      "rithm to reduce the dimensionality down to 2 for visualization: [{'source': './data/ml.pdf', 'start_index': 1245564}]\n",
      "* 17 It is a common practice when describing a neural network to count only layers with parameters.\n",
      "\n",
      "18 Christian Szegedy et al., “Inception–v4, Inception-ResNet and the Impact of Residual Connections on Learn‐\n",
      "\n",
      "ing,” arXiv preprint arXiv:1602.07261 (2016).\n",
      "\n",
      "19 François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,” arXiv preprint arXiv:\n",
      "\n",
      "1610.02357 (2016).\n",
      "\n",
      "20 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable\n",
      "\n",
      "convolutions” as well.\n",
      "\n",
      "474 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fexclusively for cross-channel patterns—it is just a regular convolutional layer with 1 ×\n",
      "1 filters.\n",
      "\n",
      "Figure 14-19. Depthwise separable convolutional layer [{'source': './data/ml.pdf', 'start_index': 1020850}]\n",
      "* ReLU (Rectified Linear Unit function), 292-293\n",
      "replay buffers, 635, 649, 654\n",
      "replay memory, 635\n",
      "representation learning, 68, 434 (see also\n",
      "\n",
      "autoencoders)\n",
      "residual blocks, 395\n",
      "residual errors, 203\n",
      "residual learning, 471\n",
      "residual units, 471\n",
      "ResNet (Residual Network), 471\n",
      "ResNet-34 CNN, 478\n",
      "responsibilities (clustering), 262\n",
      "restoring models, 314\n",
      "restricted Boltzmann machines (RBMs), 13,\n",
      "\n",
      "349, 776\n",
      "\n",
      "reverse-mode autodiff, 290, 770\n",
      "rewards, 14\n",
      "Ridge Regression, 135\n",
      "RMSProp, 355\n",
      "\n",
      "814 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "S\n",
      "SAMME (Stagewise Additive Modeling using a\n",
      "Multiclass Exponential loss function), 203\n",
      "\n",
      "sample inefficiency, 625\n",
      "sampled softmax technique, 544\n",
      "sampling bias, 25\n",
      "sampling noise, 25\n",
      "SavedModel format, 669\n",
      "saving and restoring models, 314\n",
      "Scaled Dot-Product Attention layer, 559\n",
      "Scaled Exponential Linear Unit (SELU) func‐\n",
      "\n",
      "tion, 334, 337-338, 368\n",
      "\n",
      "Scikit-Learn [{'source': './data/ml.pdf', 'start_index': 1759399}]\n",
      "* Recurrent Neurons and Layers\n",
      "Up  to  now  we  have  focused  on  feedforward  neural  networks,  where  the  activations\n",
      "flow only in one direction, from the input layer to the output layer (a few exceptions\n",
      "are  discussed  in  Appendix  E).  A  recurrent  neural  network  looks  very  much  like  a\n",
      "feedforward neural network, except it also has connections pointing backward. Let’s\n",
      "look  at  the  simplest  possible  RNN,  composed  of  one  neuron  receiving  inputs,  pro‐\n",
      "ducing  an  output,  and  sending  that  output  back  to  itself,  as  shown  in  Figure  15-1\n",
      "(left). At each time step t (also called a frame), this recurrent neuron receives the inputs\n",
      "x(t) as well as its own output from the previous time step, y(t–1). Since there is no previ‐\n",
      "ous output at the first time step, it is generally set to 0. We can represent this tiny net‐\n",
      "work against the time axis, as shown in Figure 15-1 (right). This is called unrolling the [{'source': './data/ml.pdf', 'start_index': 1074772}]\n",
      "* Note that max pooling and average pooling can be performed along the depth dimen‐\n",
      "sion  rather  than  the  spatial  dimensions,  although  this  is  not  as  common.  This  can\n",
      "allow the CNN to learn to be invariant to various features. For example, it could learn\n",
      "multiple filters, each detecting a different rotation of the same pattern (such as hand-\n",
      "written digits; see Figure 14-10), and the depthwise max pooling layer would ensure\n",
      "that the output is the same regardless of the rotation. The CNN could similarly learn\n",
      "to be invariant to anything else: thickness, brightness, skew, color, and so on.\n",
      "\n",
      "Figure 14-10. Depthwise max pooling can help the CNN learn any invariance\n",
      "\n",
      "Pooling Layers \n",
      "\n",
      "| \n",
      "\n",
      "459 [{'source': './data/ml.pdf', 'start_index': 991953}]\n",
      "* Custom Models and Training with TensorFlow \n",
      "\n",
      "| \n",
      "\n",
      "375\n",
      "\n",
      "\fA Quick Tour of TensorFlow\n",
      "As you know, TensorFlow is a powerful library for numerical computation, particu‐\n",
      "larly well suited and fine-tuned for large-scale Machine Learning (but you could use\n",
      "it for anything else that requires heavy computations). It was developed by the Google\n",
      "Brain team and it powers many of Google’s large-scale services, such as Google Cloud\n",
      "Speech, Google Photos, and Google Search. It was open sourced in November 2015,\n",
      "and it is now the most popular Deep Learning library (in terms of citations in papers,\n",
      "adoption in companies, stars on GitHub, etc.). Countless projects use TensorFlow for\n",
      "all  sorts  of  Machine  Learning  tasks,  such  as  image  classification,  natural  language\n",
      "processing, recommender systems, and time series forecasting.\n",
      "\n",
      "So what does TensorFlow offer? Here’s a summary:\n",
      "\n",
      "• Its core is very similar to NumPy, but with GPU support. [{'source': './data/ml.pdf', 'start_index': 807404}]\n",
      "* diction,” arXiv preprint arXiv:1808.03867 (2018).\n",
      "\n",
      "31 Shuai Li et al., “Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN,”\n",
      "\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018): 5457–5466.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "565\n",
      "\n",
      "\f6. What  is  the  most  important  layer  in  the  Transformer  architecture?  What  is  its\n",
      "\n",
      "purpose?\n",
      "\n",
      "7. When would you need to use sampled softmax? [{'source': './data/ml.pdf', 'start_index': 1228088}]\n",
      "* Deep RNNs                                                                                                                506\n",
      "Forecasting Several Time Steps Ahead                                                                  508\n",
      "Handling Long Sequences                                                                                           511\n",
      "Fighting the Unstable Gradients Problem                                                            512\n",
      "Tackling the Short-Term Memory Problem                                                         514\n",
      "Exercises                                                                                                                        523 [{'source': './data/ml.pdf', 'start_index': 36936}]\n",
      "* Conference on Neural Information Processing Systems (2006): 153–160.\n",
      "\n",
      "4 Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,” Proceed‐\n",
      "\n",
      "ings of the 21st International Conference on Artificial Neural Networks 1 (2011): 52–59.\n",
      "\n",
      "Convolutional Autoencoders \n",
      "\n",
      "| \n",
      "\n",
      "579\n",
      "\n",
      "\f    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"same\",\n",
      "                                 activation=\"selu\"),\n",
      "    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\",\n",
      "                                 activation=\"sigmoid\"),\n",
      "    keras.layers.Reshape([28, 28])\n",
      "])\n",
      "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder]) [{'source': './data/ml.pdf', 'start_index': 1255774}]\n",
      "* 578 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fAs we discussed earlier, one of the triggers of the current tsunami of interest in Deep\n",
      "Learning  was  the  discovery  in  2006  by  Geoffrey  Hinton  et  al.  that  deep  neural  net‐\n",
      "works  can  be  pretrained  in  an  unsupervised  fashion,  using  this  greedy  layerwise\n",
      "approach. They used restricted Boltzmann machines (RBMs; see Appendix E) for this\n",
      "purpose, but in 2007 Yoshua Bengio et al. showed3 that autoencoders worked just as\n",
      "well. For several years this was the only efficient way to train deep nets, until many of\n",
      "the techniques introduced in Chapter 11 made it possible to just train a deep net in\n",
      "one shot.\n",
      "\n",
      "Autoencoders  are  not  limited  to  dense  networks:  you  can  also  build  convolutional\n",
      "autoencoders, or even recurrent autoencoders. Let’s look at these now. [{'source': './data/ml.pdf', 'start_index': 1253033}]\n",
      "* spurious patterns, 774\n",
      "stacked autoencoders\n",
      "overview of, 572\n",
      "stacked denoising autoencoders, 581\n",
      "unsupervised pretraining using, 576-579\n",
      "using Keras, 572\n",
      "visualizing Fashion MNIST Dataset, 574\n",
      "visualizing reconstructions, 574\n",
      "stacked denoising autoencoders, 581\n",
      "stacked generalization, 208\n",
      "stacking, 208\n",
      "stale gradients, 707\n",
      "standard correlation coefficient, 58\n",
      "standardization, 69\n",
      "start of sequence (SoS) token, 535\n",
      "state-action values, 628\n",
      "stateful metrics, 389\n",
      "stationary point, 761\n",
      "statistical mode, 193\n",
      "statistical significance, 182\n",
      "step function, 284\n",
      "Stochastic Gradient Boosting, 207\n",
      "Stochastic Gradient Descent (SGD), 88, 124\n",
      "stochastic neurons, 775\n",
      "stochastic policy, 612\n",
      "stratified sampling, 53\n",
      "streaming metrics, 389\n",
      "stride, 449\n",
      "string kernels, 161\n",
      "string subsequence kernel, 161\n",
      "string tensors, 383, 783\n",
      "strong learners, 190\n",
      "style mixing, 606\n",
      "style transfer, 604\n",
      "StyleGANs, 567, 604\n",
      "Subclassing API, 313\n",
      "subderivatives, 173\n",
      "subgradient vector, 140\n",
      "subsampling, 456\n",
      "subspace, 215 [{'source': './data/ml.pdf', 'start_index': 1763714}]\n",
      "* 572 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fMNIST  (loaded  and  normalized  as  in  Chapter  10),  using  the  SELU  activation\n",
      "function:\n",
      "\n",
      "stacked_encoder = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.Dense(100, activation=\"selu\"),\n",
      "    keras.layers.Dense(30, activation=\"selu\"),\n",
      "])\n",
      "stacked_decoder = keras.models.Sequential([\n",
      "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
      "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
      "    keras.layers.Reshape([28, 28])\n",
      "])\n",
      "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
      "stacked_ae.compile(loss=\"binary_crossentropy\",\n",
      "                   optimizer=keras.optimizers.SGD(lr=1.5))\n",
      "history = stacked_ae.fit(X_train, X_train, epochs=10,\n",
      "                         validation_data=[X_valid, X_valid])\n",
      "\n",
      "Let’s go through this code:\n",
      "\n",
      "• Just like earlier, we split the autoencoder model into two submodels: the encoder [{'source': './data/ml.pdf', 'start_index': 1241580}]\n",
      "* 2\n",
      "\n",
      "n\n",
      "\n",
      "U\n",
      "\n",
      "d\n",
      "\n",
      "T\n",
      "\n",
      "p\n",
      "\n",
      "e\n",
      "\n",
      "d\n",
      "\n",
      "E\n",
      "\n",
      "n\n",
      "\n",
      "a\n",
      "\n",
      "s\n",
      "\n",
      "t\n",
      "\n",
      "o\n",
      "\n",
      "e\n",
      "\n",
      "d\n",
      "\n",
      "iti\n",
      " f\n",
      "\n",
      "o\n",
      "\n",
      "r  \n",
      "\n",
      "2\n",
      "\n",
      "o\n",
      "\n",
      "n\n",
      "\n",
      "d\n",
      "\n",
      "r\n",
      "Fl\n",
      "\n",
      "o\n",
      "\n",
      "w\n",
      "\n",
      "Hands-on  \n",
      "Machine Learning  \n",
      " with Scikit-Learn,  \n",
      "Keras & TensorFlow\n",
      "\n",
      "Concepts, Tools, and Techniques  \n",
      "to Build Intelligent Systems\n",
      "\n",
      "TM\n",
      "\n",
      "Aurélien Géron\n",
      "\n",
      " \n",
      " \n",
      "\f\fSECOND EDITION\n",
      "\n",
      "Hands-On Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "\n",
      "Aurélien Géron\n",
      "\n",
      "Beijing\n",
      "Beijing\n",
      "\n",
      "Boston\n",
      "Boston\n",
      "\n",
      "Farnham Sebastopol\n",
      "Farnham Sebastopol\n",
      "\n",
      "Tokyo\n",
      "Tokyo\n",
      "\n",
      "\fHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "\n",
      "Copyright © 2019 Kiwisoft S.A.S. All rights reserved.\n",
      "\n",
      "Printed in Canada.\n",
      "\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. [{'source': './data/ml.pdf', 'start_index': 0}]\n",
      "* Conference on Acoustics, Speech, and Signal Processing (2016): 2657–2661.\n",
      "\n",
      "4 Jimmy Lei Ba et al., “Layer Normalization,” arXiv preprint arXiv:1607.06450 (2016).\n",
      "\n",
      "512 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\fscale and an offset parameter for each input. In an RNN, it is typically used right after\n",
      "the linear combination of the inputs and the hidden states. [{'source': './data/ml.pdf', 'start_index': 1107626}]\n",
      "* numbers  representing  the  overall  level  of  response  for  each  filter.  The  next  layer  is\n",
      "where  the  “squeeze”  happens:  this  layer  has  significantly  fewer  than  256  neurons—\n",
      "typically 16 times fewer than the number of feature maps (e.g., 16 neurons)—so the\n",
      "256 numbers get compressed into a small vector (e.g., 16 dimensions). This is a low-\n",
      "dimensional vector representation (i.e., an embedding) of the distribution of feature\n",
      "responses. This bottleneck step forces the SE block to learn a general representation\n",
      "of the feature combinations (we will see this principle in action again when we dis‐\n",
      "cuss autoencoders in Chapter 17). Finally, the output layer takes the embedding and\n",
      "outputs  a  recalibration  vector  containing  one  number  per  feature  map  (e.g.,  256),\n",
      "each between 0 and 1. The feature maps are then multiplied by this recalibration vec‐\n",
      "tor, so irrelevant features (with a low recalibration score) get scaled down while rele‐ [{'source': './data/ml.pdf', 'start_index': 1026283}]\n",
      "* ture a wide variety of patterns.\n",
      "\n",
      "• Next,  a  max  pooling  layer  reduces  the  image  height  and  width  by  2,  again  to\n",
      "\n",
      "speed up computations.\n",
      "\n",
      "• Then  comes  the  tall  stack  of  nine  inception  modules,  interleaved  with  a  couple\n",
      "\n",
      "max pooling layers to reduce dimensionality and speed up the net.\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "469 [{'source': './data/ml.pdf', 'start_index': 1012220}]\n",
      "* • Note that the number of filters grows as we climb up the CNN toward the output\n",
      "layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since the\n",
      "number  of  low-level  features  is  often  fairly  low  (e.g.,  small  circles,  horizontal\n",
      "lines), but there are many different ways to combine them into higher-level fea‐\n",
      "tures. It is a common practice to double the number of filters after each pooling\n",
      "layer: since a pooling layer divides each spatial dimension by a factor of 2, we can\n",
      "afford  to  double  the  number  of  feature  maps  in  the  next  layer  without  fear  of\n",
      "exploding the number of parameters, memory usage, or computational load.\n",
      "\n",
      "• Next is the fully connected network, composed of two hidden dense layers and a\n",
      "dense  output  layer.  Note  that  we  must  flatten  its  inputs,  since  a  dense  network\n",
      "expects a 1D array of features for each instance. We also add two dropout layers,\n",
      "with a dropout rate of 50% each, to reduce overfitting. [{'source': './data/ml.pdf', 'start_index': 997779}]\n",
      "* 450 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-5. Applying two different filters to get two feature maps [{'source': './data/ml.pdf', 'start_index': 975659}]\n",
      "* As you can see, there are quite a lot of things you need to get right, and it’s easy to\n",
      "make a mistake. But on the bright side, you get full control, so it’s your call.\n",
      "\n",
      "Now that you know how to customize any part of your models14 and training algo‐\n",
      "rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\n",
      "can speed up your custom code considerably, and it will also make it portable to any\n",
      "platform supported by TensorFlow (see Chapter 19).\n",
      "\n",
      "TensorFlow Functions and Graphs\n",
      "In TensorFlow 1, graphs were unavoidable (as were the complexities that came with\n",
      "them) because they were a central part of TensorFlow’s API. In TensorFlow 2, they are\n",
      "still there, but not as central, and they’re much (much!) simpler to use. To show just\n",
      "how simple, let’s start with a trivial function that computes the cube of its input:\n",
      "\n",
      "def cube(x):\n",
      "    return x ** 3\n",
      "\n",
      "13 The truth is we did not process every single instance in the training set, because we sampled instances ran‐ [{'source': './data/ml.pdf', 'start_index': 875146}]\n",
      "* 1 David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats,” The Journal of Physiology 147\n",
      "\n",
      "(1959): 226–238.\n",
      "\n",
      "2 David H. Hubel and Torsten N. Wiesel, “Receptive Fields of Single Neurons in the Cat’s Striate Cortex,” The\n",
      "\n",
      "Journal of Physiology 148 (1959): 574–591.\n",
      "\n",
      "3 David H. Hubel and Torsten N. Wiesel, “Receptive Fields and Functional Architecture of Monkey Striate Cor‐\n",
      "\n",
      "tex,” The Journal of Physiology 195 (1968): 215–243.\n",
      "\n",
      "446 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-1. Biological neurons in the visual cortex respond to specific patterns in small\n",
      "regions of the visual field called receptive fields; as the visual signal makes its way\n",
      "through consecutive brain modules, neurons respond to more complex patterns in larger\n",
      "receptive fields. [{'source': './data/ml.pdf', 'start_index': 968345}]\n",
      "* • The  GPT  paper26  by  Alec  Radford  and  other  OpenAI  researchers  also  demon‐\n",
      "strated  the  effectiveness  of  unsupervised  pretraining,  but  this  time  using  a\n",
      "\n",
      "24 Matthew Peters et al., “Deep Contextualized Word Representations,” Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1\n",
      "(2018): 2227–2237.\n",
      "\n",
      "25 Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification,” Pro‐\n",
      "ceedings of the 56th Annual Meeting of the Association for Computational Linguistics 1 (2018): 328–339.\n",
      "\n",
      "26 Alec Radford et al., “Improving Language Understanding by Generative Pre-Training” (2018).\n",
      "\n",
      "Recent Innovations in Language Models \n",
      "\n",
      "| \n",
      "\n",
      "563 [{'source': './data/ml.pdf', 'start_index': 1221909}]\n",
      "* Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "393\n",
      "\n",
      "\fThis layer may now be used like any other layer, but of course only using the Func‐\n",
      "tional and Subclassing APIs, not the Sequential API (which only accepts layers with\n",
      "one input and one output).\n",
      "\n",
      "If  your  layer  needs  to  have  a  different  behavior  during  training  and  during  testing\n",
      "(e.g., if it uses  Dropout or  BatchNormalization layers), then you must add a  train\n",
      "ing argument to the call() method and use this argument to decide what to do. For\n",
      "example, let’s create a layer that adds Gaussian noise during training (for regulariza‐\n",
      "tion)  but  does  nothing  during  testing  (Keras  has  a  layer  that  does  the  same  thing,\n",
      "keras.layers.GaussianNoise):\n",
      "\n",
      "class MyGaussianNoise(keras.layers.Layer):\n",
      "    def __init__(self, stddev, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.stddev = stddev [{'source': './data/ml.pdf', 'start_index': 849198}]\n",
      "* conv_encoder = keras.models.Sequential([\n",
      "    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n",
      "    keras.layers.Conv2D(16, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
      "    keras.layers.MaxPool2D(pool_size=2),\n",
      "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
      "    keras.layers.MaxPool2D(pool_size=2),\n",
      "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
      "    keras.layers.MaxPool2D(pool_size=2)\n",
      "])\n",
      "conv_decoder = keras.models.Sequential([\n",
      "    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"valid\",\n",
      "                                 activation=\"selu\",\n",
      "                                 input_shape=[3, 3, 64]),\n",
      "\n",
      "3 Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks,” Proceedings of the 19th International\n",
      "\n",
      "Conference on Neural Information Processing Systems (2006): 153–160.\n",
      "\n",
      "4 Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,” Proceed‐ [{'source': './data/ml.pdf', 'start_index': 1254968}]\n",
      "* To understand how a Multi-Head Attention layer works, we must first understand the\n",
      "Scaled  Dot-Product  Attention  layer,  which  it  is  based  on.  Let’s  suppose  the  encoder\n",
      "analyzed the input sentence “They played chess,” and it managed to understand that\n",
      "the word “They” is the subject and the word “played” is the verb, so it encoded this\n",
      "information  in  the  representations  of  these  words.  Now  suppose  the  decoder  has\n",
      "already translated the subject, and it thinks that it should translate the verb next. For\n",
      "this, it needs to fetch the verb from the input sentence. This is analog to a dictionary\n",
      "lookup: it’s as if the encoder created a dictionary {“subject”: “They”, “verb”: “played”,\n",
      "…} and the decoder wanted to look up the value that corresponds to the key “verb.”\n",
      "However, the model does not have discrete tokens to represent the keys (like “subject”\n",
      "or “verb”); it has vectorized representations of these concepts (which it learned dur‐ [{'source': './data/ml.pdf', 'start_index': 1211600}]\n",
      "* An Encoder–Decoder Network for Neural Machine Translation \n",
      "\n",
      "| \n",
      "\n",
      "545\n",
      "\n",
      "\fembedding  of  the  token  that  was  actually  output.  During  training,  it  should  be  the\n",
      "embedding of the previous target token: this is why we used the TrainingSampler. In\n",
      "practice, it is often a good idea to start training with the embedding of the target of\n",
      "the previous time step and gradually transition to using the embedding of the actual\n",
      "token that was output at the previous step. This idea was introduced in a 2015 paper12\n",
      "by  Samy  Bengio  et  al.  The  ScheduledEmbeddingTrainingSampler  will  randomly\n",
      "choose between the target or the actual output, with a probability that you can gradu‐\n",
      "ally change during training. [{'source': './data/ml.pdf', 'start_index': 1184048}]\n",
      "* 772 \n",
      "\n",
      "|  Appendix D: Autodiff\n",
      "\n",
      "\fAPPENDIX E\n",
      "Other Popular ANN Architectures\n",
      "\n",
      "In  this  appendix  I  will  give  a  quick  overview  of  a  few  historically  important  neural\n",
      "network architectures that are much less used today than deep Multilayer Perceptrons\n",
      "(Chapter 10), convolutional neural networks (Chapter 14), recurrent neural networks\n",
      "(Chapter 15), or autoencoders (Chapter 17). They are often mentioned in the litera‐\n",
      "ture, and some are still used in a range of applications, so it is worth knowing about\n",
      "them. Additionally, we will discuss deep belief nets, which were the state of the art in\n",
      "Deep Learning until the early 2010s. They are still the subject of very active research,\n",
      "so they may well come back with a vengeance in the future. [{'source': './data/ml.pdf', 'start_index': 1680029}]\n",
      "* Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "455\n",
      "\n",
      "\fMemory Requirements\n",
      "Another problem with CNNs is that the convolutional layers require a huge amount\n",
      "of RAM. This is especially true during training, because the reverse pass of backpro‐\n",
      "pagation requires all the intermediate values computed during the forward pass. [{'source': './data/ml.pdf', 'start_index': 984517}]\n",
      "* Unfortunately,  gradients  often  get  smaller  and  smaller  as  the  algorithm  progresses\n",
      "down to the lower layers. As a result, the Gradient Descent update leaves the lower\n",
      "layers’  connection  weights  virtually  unchanged,  and  training  never  converges  to  a\n",
      "good solution. We call this the vanishing gradients problem. In some cases, the oppo‐\n",
      "site  can  happen:  the  gradients  can  grow  bigger  and  bigger  until  layers  get  insanely\n",
      "large weight updates and the algorithm diverges. This is the exploding gradients prob‐\n",
      "lem,  which  surfaces  in  recurrent  neural  networks  (see  Chapter  15).  More  generally,\n",
      "deep  neural  networks  suffer  from  unstable  gradients;  different  layers  may  learn  at\n",
      "widely different speeds. [{'source': './data/ml.pdf', 'start_index': 704563}]\n",
      "* Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\n",
      "three  wonderful  children,  Alexandre,  Rémi,  and  Gabrielle,  for  encouraging  me  to\n",
      "work  hard  on  this  book.  I’m  also  thankful  to  them  for  their  insatiable  curiosity:\n",
      "explaining some of the most difficult concepts in this book to my wife and children\n",
      "helped me clarify my thoughts and directly improved many parts of it. And they keep\n",
      "bringing me cookies and coffee! What more can one dream of?\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xxv\n",
      "\n",
      "\f\fPART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "\n",
      "\f\fCHAPTER 1\n",
      "The Machine Learning Landscape [{'source': './data/ml.pdf', 'start_index': 73446}]\n",
      "* But despite all this exciting progress and all these tools and services, it still helps to\n",
      "have an idea of what values are reasonable for each hyperparameter so that you can\n",
      "build a quick prototype and restrict the search space. The following sections provide\n",
      "guidelines for choosing the number of hidden layers and neurons in an MLP and for\n",
      "selecting good values for some of the main hyperparameters.\n",
      "\n",
      "Number of Hidden Layers\n",
      "For  many  problems,  you  can  begin  with  a  single  hidden  layer  and  get  reasonable\n",
      "results.  An  MLP  with  just  one  hidden  layer  can  theoretically  model  even  the  most\n",
      "complex functions, provided it has enough neurons. But for complex problems, deep\n",
      "networks have a much higher parameter efficiency than shallow ones: they can model\n",
      "complex  functions  using  exponentially  fewer  neurons  than  shallow  nets,  allowing\n",
      "them to reach much better performance with the same amount of training data. [{'source': './data/ml.pdf', 'start_index': 684381}]\n",
      "* conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
      "                           padding=\"same\", activation=\"relu\")\n",
      "\n",
      "This code creates a Conv2D layer with 32 filters, each 3 × 3, using a stride of 1 (both\n",
      "horizontally  and  vertically)  and  \"same\"  padding,  and  applying  the  ReLU  activation\n",
      "function to its outputs. As you can see, convolutional layers have quite a few hyper‐\n",
      "parameters:  you  must  choose  the  number  of  filters,  their  height  and  width,  the\n",
      "strides, and the padding type. As always, you can use cross-validation to find the right\n",
      "hyperparameter  values,  but  this  is  very  time-consuming.  We  will  discuss  common\n",
      "CNN architectures later, to give you some idea of which hyperparameter values work\n",
      "best in practice.\n",
      "\n",
      "Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "455 [{'source': './data/ml.pdf', 'start_index': 983752}]\n",
      "* For the solutions to exercises 9 and 10, please see the Jupyter notebooks available at\n",
      "https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional\n",
      "Neural Networks\n",
      "\n",
      "1. These are the main advantages of a CNN over a fully connected DNN for image\n",
      "\n",
      "classification:\n",
      "\n",
      "• Because consecutive layers are only partially connected and because it heavily\n",
      "reuses its weights, a CNN has many fewer parameters than a fully connected\n",
      "DNN, which makes it much faster to train, reduces the risk of overfitting, and\n",
      "requires much less training data. [{'source': './data/ml.pdf', 'start_index': 1613300}]\n",
      "* compressed TFRecord files, 425\n",
      "lists of lists using SequenceExample Proto‐\n",
      "\n",
      "buf, 429\n",
      "\n",
      "loading and parsing examples, 428\n",
      "overview of, 424\n",
      "protocol buffers (protobufs), 425\n",
      "TensorFlow protobufs, 427\n",
      "\n",
      "Theano, 295\n",
      "theoretical information criterion, 267\n",
      "thermal equilibrium, 775\n",
      "threshold logic unit (TLU), 284\n",
      "Tikhonov regularization, 135\n",
      "time series data\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "817\n",
      "\n",
      "\fadditional models for, 506\n",
      "baseline metrics, 505\n",
      "deep RNNS, 506\n",
      "forecasting several steps ahead, 508\n",
      "overview of, 503\n",
      "RNNS for, 497\n",
      "simple RNNs, 505 [{'source': './data/ml.pdf', 'start_index': 1768180}]\n",
      "* • The last layers are self-explanatory: dropout for regularization, then a fully con‐\n",
      "nected layer with 1,000 units (since there are 1,000 classes) and a softmax activa‐\n",
      "tion function to output estimated class probabilities.\n",
      "\n",
      "This diagram is slightly simplified: the original GoogLeNet architecture also included\n",
      "two  auxiliary  classifiers  plugged  on  top  of  the  third  and  sixth  inception  modules.\n",
      "They were both composed of one average pooling layer, one convolutional layer, two\n",
      "fully  connected  layers,  and  a  softmax  activation  layer.  During  training,  their  loss\n",
      "(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\n",
      "ing gradients problem and regularize the network. However, it was later shown that\n",
      "their effect was relatively minor. [{'source': './data/ml.pdf', 'start_index': 1013389}]\n",
      "* With tensors, operations, variables, and various data structures at your disposal, you\n",
      "are now ready to customize your models and training algorithms!\n",
      "\n",
      "Customizing Models and Training Algorithms\n",
      "Let’s start by creating a custom loss function, which is a simple and common use case. [{'source': './data/ml.pdf', 'start_index': 824338}]\n",
      "* For more best practices regarding tuning neural network hyperparameters, check out\n",
      "the excellent 2018 paper27 by Leslie Smith.\n",
      "\n",
      "This concludes our introduction to artificial neural networks and their implementa‐\n",
      "tion  with  Keras.  In  the  next  few  chapters,  we  will  discuss  techniques  to  train  very\n",
      "deep nets. We will also explore how to customize models using TensorFlow’s lower-\n",
      "level API and how to load and preprocess data efficiently using the Data API. And we\n",
      "will dive into other popular neural network architectures: convolutional neural net‐\n",
      "works  for  image  processing,  recurrent  neural  networks  for  sequential  data,  autoen‐\n",
      "coders for representation learning, and generative adversarial networks to model and\n",
      "generate data.28\n",
      "\n",
      "Exercises [{'source': './data/ml.pdf', 'start_index': 695315}]\n",
      "* 316 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fUsing TensorBoard for Visualization\n",
      "TensorBoard  is  a  great  interactive  visualization  tool  that  you  can  use  to  view  the\n",
      "learning curves during training, compare learning curves between multiple runs, vis‐\n",
      "ualize  the  computation  graph,  analyze  training  statistics,  view  images  generated  by\n",
      "your  model,  visualize  complex  multidimensional  data  projected  down  to  3D  and\n",
      "automatically clustered for you, and more! This tool is installed automatically when\n",
      "you install TensorFlow, so you already have it. [{'source': './data/ml.pdf', 'start_index': 668174}]\n",
      "* of the International Conference on Learning Representations (2018).\n",
      "\n",
      "Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "601\n",
      "\n",
      "\fThe extra layers get added at the end of the generator and at the beginning of the dis‐\n",
      "criminator, and previously trained layers remain trainable. [{'source': './data/ml.pdf', 'start_index': 1301491}]\n",
      "* pooling layer, 456\n",
      "positional embeddings, 556\n",
      "post-training quantization, 686\n",
      "power scheduling, 360\n",
      "pre-images, 228\n",
      "precision, 91-97\n",
      "prediction problems, 8, 17, 189\n",
      "prediction service\n",
      "\n",
      "creating on GCP AI, 677-681\n",
      "using, 682-685\n",
      "\n",
      "predictors, 65\n",
      "preprocessing, 251, 430-439\n",
      "pretraining\n",
      "\n",
      "for transfer learning, 481\n",
      "greedy layer-wise pretraining, 349\n",
      "models from Keras, 479\n",
      "on auxiliary tasks, 350\n",
      "reusing pretrained embeddings, 540\n",
      "reusing pretrained layers, 345-351\n",
      "unsupervised pretraining, 349\n",
      "using stacked autoencoders, 576-579\n",
      "\n",
      "primal problem, 168\n",
      "prioritized experience replay (PER), 640\n",
      "probabilistic autoencoders, 586\n",
      "probability density function (PDF), 236, 264\n",
      "projection, 215\n",
      "propositional logic, 280\n",
      "protocol buffers (protobufs), 425\n",
      "Proximal Policy Optimization (PPO), 663\n",
      "pruning, 182\n",
      "PyTorch library, 296\n",
      "\n",
      "Q\n",
      "Q-Learning\n",
      "\n",
      "Approximate Q-Learning and Deep Q- [{'source': './data/ml.pdf', 'start_index': 1756106}]\n",
      "* 494 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-28. Skip layers recover some spatial resolution from lower layers [{'source': './data/ml.pdf', 'start_index': 1067810}]\n",
      "* Attention Is All You Need: The Transformer Architecture                                554\n",
      "Recent Innovations in Language Models                                                                  563\n",
      "Exercises                                                                                                                        565 [{'source': './data/ml.pdf', 'start_index': 39545}]\n",
      "* 23 This is the right part of figure 2 from the paper, reproduced with the kind authorization of the authors.\n",
      "\n",
      "562 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fverb, another linear layer will extract just the fact that it is past tense, and so on. Then\n",
      "the Scaled Dot-Product Attention layers implement the lookup phase, and finally we\n",
      "concatenate all the results and project them back to the original space.\n",
      "\n",
      "At  the  time  of  this  writing,  there  is  no  Transformer  class  or  MultiHeadAttention\n",
      "class available for TensorFlow 2. However, you can check out TensorFlow’s great tuto‐\n",
      "rial for building a Transformer model for language understanding. Moreover, the TF\n",
      "Hub team is currently porting several Transformer-based modules to TensorFlow 2,\n",
      "and they should be available soon. In the meantime, I hope I have demonstrated that\n",
      "it  is  not  that  hard  to  implement  a  Transformer  yourself,  and  it  is  certainly  a  great\n",
      "exercise! [{'source': './data/ml.pdf', 'start_index': 1219529}]\n",
      "* upper Multi-Head Attention layer is where the decoder pays attention to the\n",
      "words in the input sentence. For example, the decoder will probably pay close\n",
      "attention to the word “Queen” in the input sentence when it is about to output\n",
      "this word’s translation. [{'source': './data/ml.pdf', 'start_index': 1206051}]\n",
      "* Equation E-2. Contrastive divergence weight update\n",
      "\n",
      "wi, j\n",
      "\n",
      "wi, j + η xh⊺ − x′h′⊺\n",
      "\n",
      "The great benefit of this algorithm is that it does not require waiting for the network\n",
      "to reach thermal equilibrium: it just goes forward, backward, and forward again, and\n",
      "that’s it. This makes it incomparably more efficient than previous algorithms, and it\n",
      "was a key ingredient to the first success of Deep Learning based on multiple stacked\n",
      "RBMs.\n",
      "\n",
      "Deep Belief Nets\n",
      "Several layers of RBMs can be stacked; the hidden units of the first-level RBM serve\n",
      "as the visible units for the second-layer RBM, and so on. Such an RBM stack is called\n",
      "a deep belief net (DBN).\n",
      "\n",
      "1 Miguel Á. Carreira-Perpiñán and Geoffrey E. Hinton, “On Contrastive Divergence Learning,” Proceedings of\n",
      "\n",
      "the 10th International Workshop on Artificial Intelligence and Statistics (2005): 59–66.\n",
      "\n",
      "Other Popular ANN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "777 [{'source': './data/ml.pdf', 'start_index': 1688710}]\n",
      "* Looking  at  the  logistic  activation  function  (see  Figure  11-1),  you  can  see  that  when\n",
      "inputs  become  large  (negative  or  positive),  the  function  saturates  at  0  or  1,  with  a\n",
      "derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually\n",
      "no  gradient  to  propagate  back  through  the  network;  and  what  little  gradient  exists\n",
      "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
      "there is really nothing left for the lower layers.\n",
      "\n",
      "1 Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural Net‐\n",
      "\n",
      "works,” Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 249–256.\n",
      "\n",
      "332 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fFigure 11-1. Logistic activation function saturation [{'source': './data/ml.pdf', 'start_index': 706531}]\n",
      "* Figure 14-27. Upsampling using a transposed convolutional layer\n",
      "\n",
      "33 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati‐\n",
      "\n",
      "cians call a deconvolution, so this name should be avoided.\n",
      "\n",
      "Semantic Segmentation \n",
      "\n",
      "| \n",
      "\n",
      "493\n",
      "\n",
      "\fIn  a  transposed  convolutional  layer,  the  stride  defines  how  much\n",
      "the  input  will  be  stretched,  not  the  size  of  the  filter  steps,  so  the\n",
      "larger the stride, the larger the output (unlike for convolutional lay‐\n",
      "ers or pooling layers).\n",
      "\n",
      "TensorFlow Convolution Operations\n",
      "TensorFlow also offers a few other kinds of convolutional layers:\n",
      "\n",
      "keras.layers.Conv1D\n",
      "\n",
      "Creates a convolutional layer for 1D inputs, such as time series or text (sequences\n",
      "of letters or words), as we will see in Chapter 15.\n",
      "\n",
      "keras.layers.Conv3D\n",
      "\n",
      "Creates a convolutional layer for 3D inputs, such as 3D PET scans.\n",
      "\n",
      "dilation_rate [{'source': './data/ml.pdf', 'start_index': 1064986}]\n",
      "* xviii \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\fChanges in the Second Edition\n",
      "This second edition has six main objectives:\n",
      "\n",
      "1. Cover  additional  ML  topics:  more  unsupervised  learning  techniques  (including\n",
      "clustering,  anomaly  detection,  density  estimation,  and  mixture  models);  more\n",
      "techniques  for  training  deep  nets  (including  self-normalized  networks);  addi‐\n",
      "tional  computer  vision  techniques  (including  Xception,  SENet,  object  detection\n",
      "with  YOLO,  and  semantic  segmentation  using  R-CNN);  handling  sequences\n",
      "using  covolutional  neural  networks  (CNNs,  including  WaveNet);  natural  lan‐\n",
      "guage  processing  using  recurrent  neural  networks  (RNNs),  CNNs,  and  Trans‐\n",
      "formers; and GANs. [{'source': './data/ml.pdf', 'start_index': 57976}]\n",
      "* Mapping network\n",
      "\n",
      "An eight-layer MLP that maps the latent representations z (i.e., the codings) to a\n",
      "vector  w.  This  vector  is  then  sent  through  multiple  affine  transformations  (i.e.,\n",
      "Dense  layers  with  no  activation  functions,  represented  by  the  “A”  boxes  in\n",
      "Figure 17-20), which produces multiple vectors. These vectors control the style of\n",
      "the generated image at different levels, from fine-grained texture (e.g., hair color)\n",
      "to high-level features (e.g., adult or child). In short, the mapping network maps\n",
      "the codings to multiple style vectors.\n",
      "\n",
      "Synthesis network [{'source': './data/ml.pdf', 'start_index': 1307671}]\n",
      "* 1D  convolutional  layer  with  a  stride  of  1  and  \"same\"  padding,  then  the  output\n",
      "sequence  will  have  the  same  length  as  the  input  sequence.  But  if  you  use  \"valid\"\n",
      "padding or a stride greater than 1, then the output sequence will be shorter than the\n",
      "input sequence, so make sure you adjust the targets accordingly. For example, the fol‐\n",
      "lowing model is the same as earlier, except it starts with a 1D convolutional layer that\n",
      "downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is\n",
      "larger  than  the  stride,  so  all  inputs  will  be  used  to  compute  the  layer’s  output,  and\n",
      "therefore the model can learn to preserve the useful information, dropping only the\n",
      "unimportant  details.  By  shortening  the  sequences,  the  convolutional  layer  may  help\n",
      "the GRU layers detect longer patterns. Note that we must also crop off the first three [{'source': './data/ml.pdf', 'start_index': 1123785}]\n",
      "* GoogLeNet\n",
      "The GoogLeNet architecture was developed by Christian Szegedy et al. from Google\n",
      "Research,13 and it won the ILSVRC 2014 challenge by pushing the top-five error rate\n",
      "below 7%. This great performance came in large part from the fact that the network\n",
      "was much deeper than previous CNNs (as you’ll see in Figure 14-14). This was made\n",
      "\n",
      "12 Matthew D. Zeiler and Rob Fergus, “Visualizing and Understanding Convolutional Networks,” Proceedings of\n",
      "\n",
      "the European Conference on Computer Vision (2014): 818-833.\n",
      "\n",
      "13 Christian Szegedy et al., “Going Deeper with Convolutions,” Proceedings of the IEEE Conference on Computer\n",
      "\n",
      "Vision and Pattern Recognition (2015): 1–9.\n",
      "\n",
      "466 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks [{'source': './data/ml.pdf', 'start_index': 1007193}]\n",
      "* keras.layers.Conv3D\n",
      "\n",
      "Creates a convolutional layer for 3D inputs, such as 3D PET scans.\n",
      "\n",
      "dilation_rate\n",
      "\n",
      "Setting the dilation_rate hyperparameter of any convolutional layer to a value\n",
      "of 2 or more creates an à-trous convolutional layer (“à trous” is French for “with\n",
      "holes”). This is equivalent to using a regular convolutional layer with a filter dila‐\n",
      "ted by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter\n",
      "equal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated\n",
      "filter of [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This lets the convolutional layer have\n",
      "a larger receptive field at no computational price and using no extra parameters.\n",
      "\n",
      "tf.nn.depthwise_conv2d()\n",
      "\n",
      "Can be used to create a depthwise convolutional layer (but you need to create the\n",
      "variables yourself). It applies every filter to every individual input channel inde‐\n",
      "pendently. Thus, if there are fn filters and fn′ input channels, then this will output\n",
      "fn × fn′ feature maps. [{'source': './data/ml.pdf', 'start_index': 1065773}]\n",
      "* For example, here is a small transformer class that adds the combined attributes we\n",
      "discussed earlier:\n",
      "\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
      "\n",
      "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
      "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
      "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
      "    def fit(self, X, y=None):\n",
      "        return self  # nothing else to do\n",
      "    def transform(self, X):\n",
      "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
      "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
      "        if self.add_bedrooms_per_room:\n",
      "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
      "            return np.c_[X, rooms_per_household, population_per_household,\n",
      "                         bedrooms_per_room]\n",
      "\n",
      "68 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project [{'source': './data/ml.pdf', 'start_index': 200623}]\n",
      "* 17 The short path can also be used to provide manually engineered features to the neural network.\n",
      "\n",
      "308 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fthus,  simple  patterns  in  the  data  may  end  up  being  distorted  by  this  sequence  of\n",
      "transformations.\n",
      "\n",
      "Figure 10-14. Wide & Deep neural network\n",
      "\n",
      "Let’s build such a neural network to tackle the California housing problem:\n",
      "\n",
      "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
      "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
      "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
      "concat = keras.layers.Concatenate()([input_, hidden2])\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "model = keras.Model(inputs=[input_], outputs=[output])\n",
      "\n",
      "Let’s go through each line of this code: [{'source': './data/ml.pdf', 'start_index': 650353}]\n",
      "* • The  call()  method  processes  the  inputs  through  all  five  hidden  layers,  then\n",
      "passes  the  result  through  the  reconstruction  layer,  which  produces  the  recon‐\n",
      "struction.\n",
      "\n",
      "• Then  the  call()  method  computes  the  reconstruction  loss  (the  mean  squared\n",
      "difference between the reconstruction and the inputs), and adds it to the model’s\n",
      "list  of  losses  using  the  add_loss()  method.11  Notice  that  we  scale  down  the\n",
      "reconstruction  loss  by  multiplying  it  by  0.05  (this  is  a  hyperparameter  you  can\n",
      "tune). This ensures that the reconstruction loss does not dominate the main loss.\n",
      "• Finally, the call() method passes the output of the hidden layers to the output\n",
      "\n",
      "layer and returns its output. [{'source': './data/ml.pdf', 'start_index': 858088}]\n",
      "* Dueling DQN                                                                                                            641\n",
      "The TF-Agents Library                                                                                                642\n",
      "Installing TF-Agents                                                                                                643\n",
      "TF-Agents Environments                                                                                        643\n",
      "Environment Specifications                                                                                    644\n",
      "Environment Wrappers and Atari Preprocessing                                                645\n",
      "Training Architecture                                                                                              649\n",
      "Creating the Deep Q-Network                                                                                650 [{'source': './data/ml.pdf', 'start_index': 44358}]\n",
      "* Why  did  I  cheat?  It  turns  out  that  transfer  learning  does  not  work  very  well  with\n",
      "small  dense  networks,  presumably  because  small  networks  learn  few  patterns,  and\n",
      "dense networks learn very specific patterns, which are unlikely to be useful in other\n",
      "tasks. Transfer learning works best with deep convolutional neural networks, which\n",
      "tend  to  learn  feature  detectors  that  are  much  more  general  (especially  in  the  lower\n",
      "layers). We will revisit transfer learning in Chapter 14, using the techniques we just\n",
      "discussed (and this time there will be no cheating, I promise!). [{'source': './data/ml.pdf', 'start_index': 747049}]\n",
      "* arXiv preprint arXiv:1207.0580 (2012).\n",
      "\n",
      "24 Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” Journal of\n",
      "\n",
      "Machine Learning Research 15 (2014): 1929–1958.\n",
      "\n",
      "Avoiding Overfitting Through Regularization \n",
      "\n",
      "| \n",
      "\n",
      "365\n",
      "\n",
      "\fFigure 11-9. With dropout regularization, at each training iteration a random subset of\n",
      "all neurons in one or more layers—except the output layer—are “dropped out”; these\n",
      "neurons output 0 at this iteration (represented by the dashed arrows) [{'source': './data/ml.pdf', 'start_index': 786131}]\n",
      "* — The positional embeddings are simply dense vectors (much like word embed‐\n",
      "dings) that represent the position of a word in the sentence. The nth positional\n",
      "embedding is added to the word embedding of the nth word in each sentence.\n",
      "This gives the model access to each word’s position, which is needed because\n",
      "the Multi-Head Attention layers do not consider the order or the position of\n",
      "the words; they only look at their relationships. Since all the other layers are\n",
      "\n",
      "556 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\ftime-distributed,  they  have  no  way  of  knowing  the  position  of  each  word\n",
      "(either  relative  or  absolute).  Obviously,  the  relative  and  absolute  word  posi‐\n",
      "tions  are  important,  so  we  need  to  give  this  information  to  the  Transformer\n",
      "somehow, and positional embeddings are a good way to do this.\n",
      "\n",
      "Let’s look a bit closer at both these novel components of the Transformer architecture,\n",
      "starting with the positional embeddings. [{'source': './data/ml.pdf', 'start_index': 1206310}]\n",
      "* In  Chapter  14,  we  saw  that  a  2D  convolutional  layer  works  by  sliding  several  fairly\n",
      "small kernels (or filters) across an image, producing multiple 2D feature maps (one\n",
      "per  kernel).  Similarly,  a  1D  convolutional  layer  slides  several  kernels  across  a\n",
      "sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a\n",
      "single very short sequential pattern (no longer than the kernel size). If you use 10 ker‐\n",
      "nels, then the layer’s output will be composed of 10 1-dimensional sequences (all of\n",
      "the same length), or equivalently you can view this output as a single 10-dimensional\n",
      "sequence.  This  means  that  you  can  build  a  neural  network  composed  of  a  mix  of\n",
      "recurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a\n",
      "1D  convolutional  layer  with  a  stride  of  1  and  \"same\"  padding,  then  the  output\n",
      "sequence  will  have  the  same  length  as  the  input  sequence.  But  if  you  use  \"valid\" [{'source': './data/ml.pdf', 'start_index': 1122987}]\n",
      "* 16 It could also just be that the rewards are noisy, in which case there are better methods for estimating an expe‐\n",
      "\n",
      "rience’s importance (see the paper for some examples).\n",
      "\n",
      "17 Ziyu Wang et al., “Dueling Network Architectures for Deep Reinforcement Learning,” arXiv preprint arXiv:\n",
      "\n",
      "1511.06581 (2015).\n",
      "\n",
      "Deep Q-Learning Variants \n",
      "\n",
      "| \n",
      "\n",
      "641\n",
      "\n",
      "\fdicted  advantages.  Here  is  a  simple  Dueling  DQN  model,  implemented  using  the\n",
      "Functional API:\n",
      "\n",
      "K = keras.backend\n",
      "input_states = keras.layers.Input(shape=[4])\n",
      "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
      "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
      "state_values = keras.layers.Dense(1)(hidden2)\n",
      "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
      "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
      "Q_values = state_values + advantages\n",
      "model = keras.Model(inputs=[input_states], outputs=[Q_values]) [{'source': './data/ml.pdf', 'start_index': 1388936}]\n",
      "* 497\n",
      "\n",
      "\fwork quite well too. We will discuss both of these possibilities, and we will finish this\n",
      "chapter by implementing a WaveNet: this is a CNN architecture capable of handling\n",
      "sequences  of  tens  of  thousands  of  time  steps.  In  Chapter  16,  we  will  continue  to\n",
      "explore RNNs and see how to use them for natural language processing, along with\n",
      "more recent architectures based on attention mechanisms. Let’s get started! [{'source': './data/ml.pdf', 'start_index': 1074341}]\n",
      "* Figure 14-18. Skip connection when changing feature map size and depth\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "473\n",
      "\n",
      "\fResNet-34  is  the  ResNet  with  34  layers  (only  counting  the  convolutional  layers  and\n",
      "the fully connected layer)17 containing 3 residual units that output 64 feature maps, 4\n",
      "RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\n",
      "ment this architecture later in this chapter. [{'source': './data/ml.pdf', 'start_index': 1018432}]\n",
      "* transposed convolutions (in the generator).\n",
      "\n",
      "• Use Batch Normalization in both the generator and the discriminator, except in\n",
      "\n",
      "the generator’s output layer and the discriminator’s input layer.\n",
      "\n",
      "• Remove fully connected hidden layers for deeper architectures.\n",
      "\n",
      "• Use ReLU activation in the generator for all layers except the output layer, which\n",
      "\n",
      "should use tanh.\n",
      "\n",
      "• Use leaky ReLU activation in the discriminator for all layers.\n",
      "\n",
      "These  guidelines  will  work  in  many  cases,  but  not  always,  so  you  may  still  need  to\n",
      "experiment  with  different  hyperparameters  (in  fact,  just  changing  the  random  seed\n",
      "and training the same model again will sometimes work). For example, here is a small\n",
      "DCGAN that works reasonably well with Fashion MNIST:\n",
      "\n",
      "13 Alec Radford et al., “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial\n",
      "\n",
      "Networks,” arXiv preprint arXiv:1511.06434 (2015).\n",
      "\n",
      "598 \n",
      "\n",
      "| [{'source': './data/ml.pdf', 'start_index': 1294565}]\n",
      "* These  popular  activation  functions  and  their  derivatives  are  represented  in\n",
      "Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if\n",
      "you  chain  several  linear  transformations,  all  you  get  is  a  linear  transformation.  For\n",
      "example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions\n",
      "gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t\n",
      "have some nonlinearity between layers, then even a deep stack of layers is equivalent\n",
      "to a single layer, and you can’t solve very complex problems with that. Conversely, a\n",
      "large enough DNN with nonlinear activations can theoretically approximate any con‐\n",
      "tinuous function.\n",
      "\n",
      "Figure 10-8. Activation functions and their derivatives\n",
      "\n",
      "OK! You know where neural nets came from, what their architecture is, and how to\n",
      "compute their outputs. You’ve also learned about the backpropagation algorithm. But\n",
      "what exactly can you do with them? [{'source': './data/ml.pdf', 'start_index': 612339}]\n",
      "* Figure 14-6. Convolutional layers with multiple feature maps, and images with three\n",
      "color channels\n",
      "\n",
      "Specifically, a neuron located in row i, column j of the feature map k in a given convo‐\n",
      "lutional layer l is connected to the outputs of the neurons in the previous layer l – 1,\n",
      "located in rows i × sh to i × sh + fh – 1 and columns j × sw to j × sw + fw – 1, across all\n",
      "feature maps (in layer l – 1). Note that all neurons located in the same row i and col‐\n",
      "umn  j  but  in  different  feature  maps  are  connected  to  the  outputs  of  the  exact  same\n",
      "neurons in the previous layer.\n",
      "\n",
      "Equation 14-1 summarizes the preceding explanations in one big mathematical equa‐\n",
      "tion: it shows how to compute the output of a given neuron in a convolutional layer.\n",
      "\n",
      "452 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fIt is a bit ugly due to all the different indices, but all it does is calculate the weighted\n",
      "sum of all the inputs, plus the bias term. [{'source': './data/ml.pdf', 'start_index': 977417}]\n",
      "* 318 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fFigure 10-17. Visualizing learning curves with TensorBoard\n",
      "\n",
      "You can also visualize the whole graph, the learned weights (projected to 3D), or the\n",
      "profiling traces. The TensorBoard() callback has options to log extra data too, such\n",
      "as embeddings (see Chapter 13).\n",
      "\n",
      "Additionally,  TensorFlow  offers  a  lower-level  API  in  the  tf.summary  package.  The\n",
      "following  code  creates  a  SummaryWriter  using  the  create_file_writer()  function,\n",
      "and it uses this writer as a context to log scalars, histograms, images, audio, and text,\n",
      "all of which can then be visualized using TensorBoard (give it a try!): [{'source': './data/ml.pdf', 'start_index': 673625}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "747\n",
      "\n",
      "\fmodels), to generate other types of data (such as text, audio, and time series), and\n",
      "to identify the weaknesses in other models and strengthen them.\n",
      "\n",
      "8. Training  GANs  is  notoriously  difficult,  because  of  the  complex  dynamics\n",
      "between the generator and the discriminator. The biggest difficulty is mode col‐\n",
      "lapse, where the generator produces outputs with very little diversity. Moreover,\n",
      "training  can  be  terribly  unstable:  it  may  start  out  fine  and  then  suddenly  start\n",
      "oscillating or diverging, without any apparent reason. GANs are also very sensi‐\n",
      "tive to the choice of hyperparameters.\n",
      "\n",
      "For the solutions to exercises 9, 10, and 11, please see the Jupyter notebooks available\n",
      "at https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Chapter 18: Reinforcement Learning [{'source': './data/ml.pdf', 'start_index': 1639391}]\n",
      "* • A model with millions of parameters would severely risk overfitting the training\n",
      "set, especially if there are not enough training instances or if they are too noisy.\n",
      "\n",
      "In this chapter we will go through each of these problems and present techniques to\n",
      "solve  them.  We  will  start  by  exploring  the  vanishing  and  exploding  gradients  prob‐\n",
      "lems and some of their most popular solutions. Next, we will look at transfer learning\n",
      "and  unsupervised  pretraining,  which  can  help  you  tackle  complex  tasks  even  when\n",
      "you have little labeled data. Then we will discuss various optimizers that can speed up\n",
      "training large models tremendously. Finally, we will go through a few popular regula‐\n",
      "rization techniques for large neural networks.\n",
      "\n",
      "With these tools, you will be able to train very deep nets. Welcome to Deep Learning!\n",
      "\n",
      "331 [{'source': './data/ml.pdf', 'start_index': 703297}]\n",
      "* 564 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fNext sentence prediction (NSP)\n",
      "\n",
      "The  model  is  trained  to  predict  whether  two  sentences  are  consecutive  or\n",
      "not.  For  example,  it  should  predict  that  “The  dog  sleeps”  and  “It  snores\n",
      "loudly”  are  consecutive  sentences,  while  “The  dog  sleeps”  and  “The  Earth\n",
      "orbits the Sun” are not consecutive. This is a challenging task, and it signifi‐\n",
      "cantly improves the performance of the model when it is fine-tuned on tasks\n",
      "such as question answering or entailment. [{'source': './data/ml.pdf', 'start_index': 1225783}]\n",
      "* 360 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fPiecewise constant scheduling\n",
      "\n",
      "Use a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs),\n",
      "then a smaller learning rate for another number of epochs (e.g., η1 = 0.001 for 50\n",
      "epochs),  and  so  on.  Although  this  solution  can  work  very  well,  it  requires  fid‐\n",
      "dling around to figure out the right sequence of learning rates and how long to\n",
      "use each of them.\n",
      "\n",
      "Performance scheduling\n",
      "\n",
      "Measure  the  validation  error  every  N  steps  (just  like  for  early  stopping),  and\n",
      "reduce the learning rate by a factor of λ when the error stops dropping.\n",
      "\n",
      "1cycle scheduling [{'source': './data/ml.pdf', 'start_index': 773184}]\n",
      "* 21 Xingyu Zeng et al., “Crafting GBD-Net for Object Detection,” IEEE Transactions on Pattern Analysis and\n",
      "\n",
      "Machine Intelligence 40, no. 9 (2018): 2109–2123.\n",
      "\n",
      "22 Jie Hu et al., “Squeeze-and-Excitation Networks,” Proceedings of the IEEE Conference on Computer Vision and\n",
      "\n",
      "Pattern Recognition (2018): 7132–7141.\n",
      "\n",
      "476 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks [{'source': './data/ml.pdf', 'start_index': 1024492}]\n",
      "* Here is how you can implement a simple CNN to tackle the Fashion MNIST dataset\n",
      "(introduced in Chapter 10):\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\",\n",
      "                        input_shape=[28, 28, 1]),\n",
      "    keras.layers.MaxPooling2D(2),\n",
      "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
      "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
      "    keras.layers.MaxPooling2D(2),\n",
      "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
      "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
      "    keras.layers.MaxPooling2D(2),\n",
      "    keras.layers.Flatten(),\n",
      "    keras.layers.Dense(128, activation=\"relu\"),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(64, activation=\"relu\"),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(10, activation=\"softmax\")\n",
      "])\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "461\n",
      "\n",
      "\fLet’s go through this model: [{'source': './data/ml.pdf', 'start_index': 996244}]\n",
      "* International Conference on Neural Information Processing Systems 2 (2012): 2951–2959.\n",
      "\n",
      "Machine Learning Project Checklist \n",
      "\n",
      "| \n",
      "\n",
      "759\n",
      "\n",
      "\f5. Ensure your key findings are communicated through beautiful visualizations or\n",
      "easy-to-remember statements (e.g., “the median income is the number-one pre‐\n",
      "dictor of housing prices”).\n",
      "\n",
      "Launch!\n",
      "\n",
      "1. Get your solution ready for production (plug into production data inputs, write\n",
      "\n",
      "unit tests, etc.).\n",
      "\n",
      "2. Write monitoring code to check your system’s live performance at regular inter‐\n",
      "\n",
      "vals and trigger alerts when it drops.\n",
      "\n",
      "• Beware of slow degradation: models tend to “rot” as data evolves.\n",
      "\n",
      "• Measuring performance may require a human pipeline (e.g., via a crowdsourc‐\n",
      "\n",
      "ing service).\n",
      "\n",
      "• Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐\n",
      "dom  values,  or  another  team’s  output  becoming  stale).  This  is  particularly\n",
      "important for online learning systems. [{'source': './data/ml.pdf', 'start_index': 1661423}]\n",
      "* My greatest hope is that this book will inspire you to build a wonderful ML applica‐\n",
      "tion that will benefit all of us! What will it be?\n",
      "\n",
      "—Aurélien Géron, June 17, 2019\n",
      "\n",
      "718 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
      "\n",
      "\fAPPENDIX A\n",
      "Exercise Solutions\n",
      "\n",
      "Solutions to the coding exercises are available in the online Jupyter\n",
      "notebooks at https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "1. Machine Learning is about building systems that can learn from data. Learning\n",
      "\n",
      "means getting better at some task, given some performance measure.\n",
      "\n",
      "2. Machine Learning is great for complex problems for which we have no algorith‐\n",
      "mic solution, to replace long lists of hand-tuned rules, to build systems that adapt\n",
      "to fluctuating environments, and finally to help humans learn (e.g., data mining).\n",
      "\n",
      "3. A labeled training set is a training set that contains the desired solution (a.k.a. a\n",
      "\n",
      "label) for each instance. [{'source': './data/ml.pdf', 'start_index': 1556500}]\n",
      "* Similarly, you could create a custom cell to apply dropout between each time step. But\n",
      "there’s a simpler way: all recurrent layers (except for keras.layers.RNN) and all cells\n",
      "provided by Keras have a dropout hyperparameter and a recurrent_dropout hyper‐\n",
      "parameter:  the  former  defines  the  dropout  rate  to  apply  to  the  inputs  (at  each  time\n",
      "step), and the latter defines the dropout rate for the hidden states (also at each time\n",
      "step). No need to create a custom cell to apply dropout at each time step in an RNN.\n",
      "\n",
      "With these techniques, you can alleviate the unstable gradients problem and train an\n",
      "RNN much more efficiently. Now let’s look at how to deal with the short-term mem‐\n",
      "ory problem. [{'source': './data/ml.pdf', 'start_index': 1111472}]\n",
      "* Why  not  simply  use  a  deep  neural  network  with  fully  connected\n",
      "layers  for  image  recognition  tasks?  Unfortunately,  although  this\n",
      "works  fine  for  small  images  (e.g.,  MNIST),  it  breaks  down  for\n",
      "larger  images  because  of  the  huge  number  of  parameters  it\n",
      "requires. For example, a 100 × 100–pixel image has 10,000 pixels,\n",
      "and if the first layer has just 1,000 neurons (which already severely\n",
      "restricts the amount of information transmitted to the next layer),\n",
      "this means a total of 10 million connections. And that’s just the first\n",
      "layer. CNNs solve this problem using partially connected layers and\n",
      "weight sharing.\n",
      "\n",
      "4 Kunihiko Fukushima, “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern\n",
      "\n",
      "Recognition Unaffected by Shift in Position,” Biological Cybernetics 36 (1980): 193–202.\n",
      "\n",
      "5 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,\n",
      "\n",
      "no. 11 (1998): 2278–2324. [{'source': './data/ml.pdf', 'start_index': 969792}]\n",
      "* Up  to  now,  in  order  to  force  the  autoencoder  to  learn  interesting  features,  we  have\n",
      "limited  the  size  of  the  coding  layer,  making  it  undercomplete.  There  are  actually\n",
      "many other kinds of constraints that can be used, including ones that allow the cod‐\n",
      "ing layer to be just as large as the inputs, or even larger, resulting in an overcomplete\n",
      "autoencoder. Let’s look at some of those approaches now.\n",
      "\n",
      "580 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs [{'source': './data/ml.pdf', 'start_index': 1258297}]\n",
      "* 15 Recall that a time-distributed Dense layer is equivalent to a regular Dense layer that you apply independently\n",
      "\n",
      "at each time step (only much faster).\n",
      "\n",
      "550 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fIf the input sentence is n words long, and assuming the output sen‐\n",
      "tence is about as long, then this model will need to compute about\n",
      "n2 weights. Fortunately, this quadratic computational complexity is\n",
      "still tractable because even long sentences don’t have thousands of\n",
      "words. [{'source': './data/ml.pdf', 'start_index': 1195552}]\n",
      "* ResNets  deeper  than  that,  such  as  ResNet-152,  use  slightly  different  residual  units.\n",
      "Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three\n",
      "convolutional  layers:  first  a  1  ×  1  convolutional  layer  with  just  64  feature  maps  (4\n",
      "times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\n",
      "with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\n",
      "maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs\n",
      "that  output  256  maps,  then  8  RUs  with  512  maps,  a  whopping  36  RUs  with  1,024\n",
      "maps, and finally 3 RUs with 2,048 maps.\n",
      "\n",
      "Google’s  Inception-v418  architecture  merged  the  ideas  of  GoogLe‐\n",
      "Net and ResNet and achieved a top-five error rate of close to 3% on\n",
      "ImageNet classification. [{'source': './data/ml.pdf', 'start_index': 1018844}]\n",
      "* 34 Kaiming He et al., “Mask R-CNN,” arXiv preprint arXiv:1703.06870 (2017).\n",
      "\n",
      "35 Geoffrey Hinton et al., “Matrix Capsules with EM Routing,” Proceedings of the International Conference on\n",
      "\n",
      "Learning Representations (2018).\n",
      "\n",
      "Semantic Segmentation \n",
      "\n",
      "| \n",
      "\n",
      "495\n",
      "\n",
      "\fExercises\n",
      "\n",
      "1. What are the advantages of a CNN over a fully connected DNN for image classi‐\n",
      "\n",
      "fication?\n",
      "\n",
      "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
      "a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the\n",
      "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
      "images of 200 × 300 pixels.\n",
      "\n",
      "What is the total number of parameters in the CNN? If we are using 32-bit floats,\n",
      "at least how much RAM will this network require when making a prediction for a\n",
      "single instance? What about when training on a mini-batch of 50 images?\n",
      "\n",
      "3. If your GPU runs out of memory while training a CNN, what are five things you\n",
      "\n",
      "could try to solve the problem? [{'source': './data/ml.pdf', 'start_index': 1069987}]\n",
      "* Under the hood, a QNetwork is composed of two parts: an encoding network that pro‐\n",
      "cesses  the  observations,  followed  by  a  dense  output  layer  that  outputs  one  Q-Value\n",
      "per action. TF-Agent’s EncodingNetwork class implements a neural network architec‐\n",
      "ture found in various agents (see Figure 18-14). [{'source': './data/ml.pdf', 'start_index': 1410123}]\n",
      "* Figure 17-9. Noisy images (top) and their reconstructions (bottom)\n",
      "\n",
      "Sparse Autoencoders\n",
      "Another kind of constraint that often leads to good feature extraction is sparsity: by\n",
      "adding an appropriate term to the cost function, the autoencoder is pushed to reduce\n",
      "the number of active neurons in the coding layer. For example, it may be pushed to\n",
      "have on average only 5% significantly active neurons in the coding layer. This forces\n",
      "the autoencoder to represent each input as a combination of a small number of acti‐\n",
      "vations. As a result, each neuron in the coding layer typically ends up representing a\n",
      "useful feature (if you could speak only a few words per month, you would probably\n",
      "try to make them worth listening to).\n",
      "\n",
      "A  simple  approach  is  to  use  the  sigmoid  activation  function  in  the  coding  layer  (to\n",
      "constrain the codings to values between 0 and 1), use a large coding layer (e.g., with\n",
      "\n",
      "582 \n",
      "\n",
      "| [{'source': './data/ml.pdf', 'start_index': 1261410}]\n",
      "* Fortunately, in real-world problems, it is often possible to reduce the number of fea‐\n",
      "tures considerably, turning an intractable problem into a tractable one. For example,\n",
      "consider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐\n",
      "ders  are  almost  always  white,  so  you  could  completely  drop  these  pixels  from  the\n",
      "training  set  without  losing  much  information.  Figure  7-6  confirms  that  these  pixels\n",
      "are utterly unimportant for the classification task. Additionally, two neighboring pix‐\n",
      "els are often highly correlated: if you merge them into a single pixel (e.g., by taking\n",
      "the mean of the two pixel intensities), you will not lose much information. [{'source': './data/ml.pdf', 'start_index': 461737}]\n",
      "* Figure 15-1. A recurrent neuron (left) unrolled through time (right)\n",
      "\n",
      "You can easily create a layer of recurrent neurons. At each time step t, every neuron\n",
      "receives both the input vector x(t) and the output vector from the previous time step\n",
      "y(t–1), as shown in Figure 15-2. Note that both the inputs and outputs are vectors now\n",
      "(when there was just a single neuron, the output was a scalar).\n",
      "\n",
      "498 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\fFigure 15-2. A layer of recurrent neurons (left) unrolled through time (right) [{'source': './data/ml.pdf', 'start_index': 1075802}]\n",
      "* In the encoder, this equation is applied to every input sentence in the batch, with Q,\n",
      "K, and V all equal to the list of words in the input sentence (so each word in the sen‐\n",
      "tence  will  be  compared  to  every  word  in  the  same  sentence,  including  itself).  Simi‐\n",
      "larly,  in  the  decoder’s  masked  attention  layer,  the  equation  will  be  applied  to  every\n",
      "target sentence in the batch, with Q, K, and V all equal to the list of words in the tar‐\n",
      "get sentence, but this time using a mask to prevent any word from comparing itself to\n",
      "words located after it (at inference time the decoder will only have access to the words\n",
      "it  already  output,  not  to  future  words,  so  during  training  we  must  mask  out  future\n",
      "output tokens). In the upper attention layer of the decoder, the keys K and values V\n",
      "are simply the list of word encodings produced by the encoder, and the queries Q are\n",
      "the list of word encodings produced by the decoder. [{'source': './data/ml.pdf', 'start_index': 1214883}]\n",
      "* For  example,  when  growing  the  generator’s  outputs  from  4  ×  4  to  8  ×  8  (see\n",
      "Figure 17-19), an upsampling layer (using nearest neighbor filtering) is added to the\n",
      "existing convolutional layer, so it outputs 8 × 8 feature maps, which are then fed to\n",
      "the new convolutional layer (which uses \"same\" padding and strides of 1, so its out‐\n",
      "puts are also 8 × 8). This new layer is followed by a new output convolutional layer:\n",
      "this is a regular convolutional layer with kernel size 1 that projects the outputs down\n",
      "to  the  desired  number  of  color  channels  (e.g.,  3).  To  avoid  breaking  the  trained\n",
      "weights of the first convolutional layer when the new convolutional layer is added, the\n",
      "final output is a weighted sum of the original output layer (which now outputs 8 × 8\n",
      "feature maps) and the new output layer. The weight of the new outputs is α, while the\n",
      "weight of the original outputs is 1 – α, and α is slowly increased from 0 to 1. In other [{'source': './data/ml.pdf', 'start_index': 1301753}]\n",
      "* Let’s go through this code:\n",
      "\n",
      "• Just like earlier, we split the autoencoder model into two submodels: the encoder\n",
      "\n",
      "and the decoder.\n",
      "\n",
      "• The  encoder  takes  28  ×  28–pixel  grayscale  images,  flattens  them  so  that  each\n",
      "image is represented as a vector of size 784, then processes these vectors through\n",
      "two  Dense  layers  of  diminishing  sizes  (100  units  then  30  units),  both  using  the\n",
      "SELU activation function (you may want to add LeCun normal initialization as\n",
      "well, but the network is not very deep so it won’t make a big difference). For each\n",
      "input image, the encoder outputs a vector of size 30.\n",
      "\n",
      "• The decoder takes codings of size 30 (output by the encoder) and processes them\n",
      "through  two  Dense  layers  of  increasing  sizes  (100  units  then  784  units),  and  it\n",
      "reshapes  the  final  vectors  into  28  ×  28  arrays  so  the  decoder’s  outputs  have  the\n",
      "same shape as the encoder’s inputs. [{'source': './data/ml.pdf', 'start_index': 1242464}]\n",
      "* 27 This assumes we used only \"same\" padding in the network: indeed, \"valid\" padding would reduce the size of\n",
      "the feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round‐\n",
      "ing error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\n",
      "feature maps may end up being smaller.\n",
      "\n",
      "488 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-25. The same fully convolutional network processing a small image (left) and a\n",
      "large one (right)\n",
      "\n",
      "You Only Look Once (YOLO)\n",
      "YOLO  is  an  extremely  fast  and  accurate  object  detection  architecture  proposed  by\n",
      "Joseph  Redmon  et  al.  in  a  2015  paper,28  and  subsequently  improved  in  201629\n",
      "(YOLOv2) and in 201830 (YOLOv3). It is so fast that it can run in real time on a video,\n",
      "as seen in Redmon’s demo. [{'source': './data/ml.pdf', 'start_index': 1053313}]\n",
      "* G. TensorFlow Graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      " 791\n",
      "\n",
      "Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   801\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "xiii\n",
      "\n",
      "\f\fPreface [{'source': './data/ml.pdf', 'start_index': 49455}]\n",
      "* Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all\n",
      "the deep neural networks they experimented with, leading to a huge improvement in\n",
      "the ImageNet classification task (ImageNet is a large database of images classified into\n",
      "many  classes,  commonly  used  to  evaluate  computer  vision  systems).  The  vanishing\n",
      "gradients problem was strongly reduced, to the point that they could use saturating\n",
      "activation  functions  such  as  the  tanh  and  even  the  logistic  activation  function.  The\n",
      "networks were also much less sensitive to the weight initialization. The authors were\n",
      "able to use much larger learning rates, significantly speeding up the learning process.\n",
      "Specifically, they note that:\n",
      "\n",
      "340 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks [{'source': './data/ml.pdf', 'start_index': 725489}]\n",
      "* Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "311\n",
      "\n",
      "\fFigure 10-16. Handling multiple outputs, in this example to add an auxiliary output for\n",
      "regularization\n",
      "\n",
      "Adding  extra  outputs  is  quite  easy:  just  connect  them  to  the  appropriate  layers  and\n",
      "add them to your model’s list of outputs. For example, the following code builds the\n",
      "network represented in Figure 10-16:\n",
      "\n",
      "[...] # Same as above, up to the main output layer\n",
      "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
      "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
      "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output]) [{'source': './data/ml.pdf', 'start_index': 656299}]\n",
      "* Stacked Autoencoders \n",
      "\n",
      "| \n",
      "\n",
      "573\n",
      "\n",
      "\fVisualizing the Reconstructions\n",
      "One way to ensure that an autoencoder is properly trained is to compare the inputs\n",
      "and the outputs: the differences should not be too significant. Let’s plot a few images\n",
      "from the validation set, as well as their reconstructions:\n",
      "\n",
      "def plot_image(image):\n",
      "    plt.imshow(image, cmap=\"binary\")\n",
      "    plt.axis(\"off\")\n",
      "\n",
      "def show_reconstructions(model, n_images=5):\n",
      "    reconstructions = model.predict(X_valid[:n_images])\n",
      "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
      "    for image_index in range(n_images):\n",
      "        plt.subplot(2, n_images, 1 + image_index)\n",
      "        plot_image(X_valid[image_index])\n",
      "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
      "        plot_image(reconstructions[image_index])\n",
      "\n",
      "show_reconstructions(stacked_ae)\n",
      "\n",
      "Figure 17-4 shows the resulting images.\n",
      "\n",
      "Figure 17-4. Original images (top) and their reconstructions (bottom) [{'source': './data/ml.pdf', 'start_index': 1244257}]\n",
      "* • Another use case is as a regularization technique (i.e., a training constraint whose\n",
      "objective is to reduce overfitting and thus improve the model’s ability to general‐\n",
      "ize). For example, you may want to add some auxiliary outputs in a neural net‐\n",
      "work  architecture  (see  Figure  10-16)  to  ensure  that  the  underlying  part  of  the\n",
      "network  learns  something  useful  on  its  own,  without  relying  on  the  rest  of  the\n",
      "network.\n",
      "\n",
      "19 Alternatively, you can pass a dictionary mapping the input names to the input values, like {\"wide_input\":\n",
      "\n",
      "X_train_A, \"deep_input\": X_train_B}. This is especially useful when there are many inputs, to avoid get‐\n",
      "ting the order wrong.\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "311\n",
      "\n",
      "\fFigure 10-16. Handling multiple outputs, in this example to add an auxiliary output for\n",
      "regularization [{'source': './data/ml.pdf', 'start_index': 655618}]\n",
      "* GoogLeNet                                                                                                                 466\n",
      "VGGNet                                                                                                                     470\n",
      "ResNet                                                                                                                        471\n",
      "Xception                                                                                                                     474\n",
      "SENet                                                                                                                          476\n",
      "Implementing a ResNet-34 CNN Using Keras                                                        478\n",
      "Using Pretrained Models from Keras                                                                        479\n",
      "Pretrained Models for Transfer Learning                                                                 481 [{'source': './data/ml.pdf', 'start_index': 34305}]\n",
      "* • GANs  are  composed  of  two  neural  networks:  a  generator  that  tries  to  generate\n",
      "data  that  looks  similar  to  the  training  data,  and  a  discriminator  that  tries  to  tell\n",
      "real  data  from  fake  data.  This  architecture  is  very  original  in  Deep  Learning  in\n",
      "that  the  generator  and  the  discriminator  compete  against  each  other  during\n",
      "training:  the  generator  is  often  compared  to  a  criminal  trying  to  make  realistic\n",
      "counterfeit money, while the discriminator is like the police investigator trying to\n",
      "tell  real  money  from  fake.  Adversarial  training  (training  competing  neural  net‐\n",
      "works) is widely considered as one of the most important ideas in recent years. In\n",
      "2016, Yann LeCun even said that it was “the most interesting idea in the last 10\n",
      "years in Machine Learning.” [{'source': './data/ml.pdf', 'start_index': 1232371}]\n",
      "* model-based learning, 18\n",
      "models (see also custom models)\n",
      "\n",
      "causal models, 510\n",
      "complex using Functional API, 308-313\n",
      "custom with TensorFlow, 384-405\n",
      "defined, 20\n",
      "dynamic using Subclassing API, 313\n",
      "fine-tuning, 75-80\n",
      "parametric versus nonparametric, 181\n",
      "pretrained models for transfer learning, 481\n",
      "pretrained models from Keras, 479\n",
      "saving and restoring, 314\n",
      "sequence-to-sequence models, 510\n",
      "training, 20, 72 (see also training models)\n",
      "training across multiple devices, 701-717\n",
      "training sparse models, 359\n",
      "using callbacks, 315\n",
      "using TensorBoard for visualization, 317\n",
      "white versus black box, 178\n",
      "\n",
      "modules, 540\n",
      "momentum optimization, 351\n",
      "momentum vector, 352\n",
      "Monte Carlo (MC) dropout, 368\n",
      "Multi-Head Attention layer, 556, 559\n",
      "multibackend Keras, 295\n",
      "multiclass classification, 100\n",
      "Multidimensional Scaling (MDS), 232\n",
      "multilabel classification, 106\n",
      "Multilayer Perceptrons (MLPs) [{'source': './data/ml.pdf', 'start_index': 1750997}]\n",
      "* In this chapter we will start by exploring in more depth how autoencoders work and\n",
      "how to use them for dimensionality reduction, feature extraction, unsupervised pre‐\n",
      "training, or as generative models. This will naturally lead us to GANs. We will start by\n",
      "building a simple GAN to generate fake images, but we will see that training is often\n",
      "quite difficult. We will discuss the main difficulties you will encounter with adversa‐\n",
      "rial training, as well as some of the main techniques to work around these difficulties.\n",
      "Let’s start with autoencoders!\n",
      "\n",
      "568 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fEfficient Data Representations\n",
      "Which of the following number sequences do you find the easiest to memorize?\n",
      "\n",
      "• 40, 27, 25, 36, 81, 57, 10, 73, 19, 68\n",
      "\n",
      "• 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14 [{'source': './data/ml.pdf', 'start_index': 1233201}]\n",
      "* Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "599\n",
      "\n",
      "\fshown  in  Figure  17-17.  It’s  still  not  perfect,  but  many  of  these  images  are  pretty\n",
      "convincing.\n",
      "\n",
      "Figure 17-17. Images generated by the DCGAN after 50 epochs of training [{'source': './data/ml.pdf', 'start_index': 1298330}]\n",
      "* Avoiding Overfitting Through Regularization \n",
      "\n",
      "| \n",
      "\n",
      "367\n",
      "\n",
      "\fDropout does tend to significantly slow down convergence, but it usually results in a\n",
      "much better model when tuned properly. So, it is generally well worth the extra time\n",
      "and effort.\n",
      "\n",
      "If you want to regularize a self-normalizing network based on the\n",
      "SELU  activation  function  (as  discussed  earlier),  you  should  use\n",
      "alpha dropout: this is a variant of dropout that preserves the mean\n",
      "and standard deviation of its inputs (it was introduced in the same\n",
      "paper as SELU, as regular dropout would break self-normalization).\n",
      "\n",
      "Monte Carlo (MC) Dropout\n",
      "In 2016, a paper25 by Yarin Gal and Zoubin Ghahramani added a few more good rea‐\n",
      "sons to use dropout:\n",
      "\n",
      "• First,  the  paper  established  a  profound  connection  between  dropout  networks\n",
      "(i.e., neural networks containing a Dropout layer before every weight layer) and\n",
      "approximate Bayesian inference,26 giving dropout a solid mathematical justifica‐\n",
      "tion. [{'source': './data/ml.pdf', 'start_index': 791234}]\n",
      "* • The  BERT  paper29  by  Jacob  Devlin  and  other  Google  researchers  also  demon‐\n",
      "strates the effectiveness of self-supervised pretraining on a large corpus, using a\n",
      "similar architecture to GPT but non-masked Multi-Head Attention layers (like in\n",
      "the Transformer’s encoder). This means that the model is naturally bidirectional;\n",
      "hence the B in BERT (Bidirectional Encoder Representations from Transformers).\n",
      "Most importantly, the authors proposed two pretraining tasks that explain most\n",
      "of the model’s strength:\n",
      "\n",
      "Masked language model (MLM) [{'source': './data/ml.pdf', 'start_index': 1223915}]\n",
      "* 504 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\fBaseline Metrics\n",
      "Before we start using RNNs, it is often a good idea to have a few baseline metrics, or\n",
      "else  we  may  end  up  thinking  our  model  works  great  when  in  fact  it  is  doing  worse\n",
      "than basic models. For example, the simplest approach is to predict the last value in\n",
      "each series. This is called naive forecasting, and it is sometimes surprisingly difficult to\n",
      "outperform. In this case, it gives us a mean squared error of about 0.020:\n",
      "\n",
      ">>> y_pred = X_valid[:, -1]\n",
      ">>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))\n",
      "0.020211367\n",
      "\n",
      "Another simple approach is to use a fully connected network. Since it expects a flat\n",
      "list of features for each input, we need to add a Flatten layer. Let’s just use a simple\n",
      "Linear Regression model so that each prediction will be a linear combination of the\n",
      "values in the time series: [{'source': './data/ml.pdf', 'start_index': 1087509}]\n",
      "* 7. An  RNN  layer  is  fundamentally  sequential:  in  order  to  compute  the  outputs  at\n",
      "time step t, it has to first compute the outputs at all earlier time steps. This makes\n",
      "it  impossible  to  parallelize.  On  the  other  hand,  a  1D  convolutional  layer  lends\n",
      "itself well to parallelization since it does not hold a state between time steps. In\n",
      "other  words,  it  has  no  memory:  the  output  at  any  time  step  can  be  computed\n",
      "based only on a small window of values from the inputs without having to know\n",
      "all the past values. Moreover, since a 1D convolutional layer is not recurrent, it\n",
      "\n",
      "Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "743 [{'source': './data/ml.pdf', 'start_index': 1626664}]\n",
      "* Several  variants  of  the  GoogLeNet  architecture  were  later  proposed  by  Google\n",
      "researchers, including Inception-v3 and Inception-v4, using slightly different incep‐\n",
      "tion modules and reaching even better performance.\n",
      "\n",
      "VGGNet\n",
      "The  runner-up  in  the  ILSVRC  2014  challenge  was  VGGNet,15  developed  by  Karen\n",
      "Simonyan and Andrew Zisserman from the Visual Geometry Group (VGG) research\n",
      "lab at Oxford University. It had a very simple and classical architecture, with 2 or 3\n",
      "convolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a\n",
      "pooling  layer,  and  so  on  (reaching  a  total  of  just  16  or  19  convolutional  layers,\n",
      "depending on the VGG variant), plus a final dense network with 2 hidden layers and\n",
      "the output layer. It used only 3 × 3 filters, but many filters.\n",
      "\n",
      "15 Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recog‐\n",
      "\n",
      "nition,” arXiv preprint arXiv:1409.1556 (2014).\n",
      "\n",
      "470 \n",
      "\n",
      "| [{'source': './data/ml.pdf', 'start_index': 1014180}]\n",
      "* Computing Gradients Using Autodiff                                                                   399\n",
      "Custom Training Loops                                                                                           402\n",
      "TensorFlow Functions and Graphs                                                                            405\n",
      "AutoGraph and Tracing                                                                                           407\n",
      "TF Function Rules                                                                                                    409\n",
      "Exercises                                                                                                                        410 [{'source': './data/ml.pdf', 'start_index': 29748}]\n",
      "* Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "321\n",
      "\n",
      "\f>>> rnd_search_cv.best_params_\n",
      "{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}\n",
      ">>> rnd_search_cv.best_score_\n",
      "-0.3189529188278931\n",
      ">>> model = rnd_search_cv.best_estimator_.model [{'source': './data/ml.pdf', 'start_index': 680708}]\n",
      "* Next,  we  can  build  the  ResNet-34  using  a  Sequential  model,  since  it’s  really  just  a\n",
      "long sequence of layers (we can treat each residual unit as a single layer now that we\n",
      "have the ResidualUnit class):\n",
      "\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],\n",
      "                              padding=\"same\", use_bias=False))\n",
      "model.add(keras.layers.BatchNormalization())\n",
      "model.add(keras.layers.Activation(\"relu\"))\n",
      "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
      "prev_filters = 64\n",
      "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
      "    strides = 1 if filters == prev_filters else 2\n",
      "    model.add(ResidualUnit(filters, strides=strides))\n",
      "    prev_filters = filters\n",
      "model.add(keras.layers.GlobalAvgPool2D())\n",
      "model.add(keras.layers.Flatten())\n",
      "model.add(keras.layers.Dense(10, activation=\"softmax\")) [{'source': './data/ml.pdf', 'start_index': 1029330}]\n",
      "* This QNetwork takes an observation as input and outputs one Q-Value per action, so\n",
      "we must give it the specifications of the observations and the actions. It starts with a\n",
      "preprocessing layer: a simple Lambda layer that casts the observations to 32-bit floats\n",
      "and normalizes them (the values will range from 0.0 to 1.0). The observations contain\n",
      "unsigned bytes, which use 4 times less space than 32-bit floats, which is why we did\n",
      "not cast the observations to 32-bit floats earlier; we want to save RAM in the replay\n",
      "buffer. Next, the network applies three convolutional layers: the first has 32 8 × 8 fil‐\n",
      "ters and uses a stride of 4, the second has 64 4 × 4 filters and a stride of 2, and the\n",
      "third  has  64  3  ×  3  filters  and  a  stride  of  1.  Lastly,  it  applies  a  dense  layer  with  512\n",
      "units, followed by a dense output layer with 4 units, one per Q-Value to output (i.e.,\n",
      "one per action). All convolutional layers and all dense layers except the output layer [{'source': './data/ml.pdf', 'start_index': 1408985}]\n",
      "* This  custom  layer  acts  like  a  regular  Dense  layer,  but  it  uses  another  Dense  layer’s\n",
      "weights, transposed (setting transpose_b=True is equivalent to transposing the sec‐\n",
      "ond argument, but it’s more efficient as it performs the transposition on the fly within\n",
      "the  matmul()  operation).  However,  it  uses  its  own  bias  vector.  Next,  we  can  build  a\n",
      "new stacked autoencoder, much like the previous one, but with the decoder’s Dense\n",
      "layers tied to the encoder’s Dense layers:\n",
      "\n",
      "dense_1 = keras.layers.Dense(100, activation=\"selu\")\n",
      "dense_2 = keras.layers.Dense(30, activation=\"selu\")\n",
      "\n",
      "tied_encoder = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    dense_1,\n",
      "    dense_2\n",
      "])\n",
      "\n",
      "Stacked Autoencoders \n",
      "\n",
      "| \n",
      "\n",
      "577\n",
      "\n",
      "\ftied_decoder = keras.models.Sequential([\n",
      "    DenseTranspose(dense_2, activation=\"selu\"),\n",
      "    DenseTranspose(dense_1, activation=\"sigmoid\"),\n",
      "    keras.layers.Reshape([28, 28])\n",
      "])\n",
      "\n",
      "tied_ae = keras.models.Sequential([tied_encoder, tied_decoder]) [{'source': './data/ml.pdf', 'start_index': 1250654}]\n",
      "* That’s it for losses! That wasn’t too hard, was it? Just as simple are custom activation\n",
      "functions, initializers, regularizers, and constraints. Let’s look at these now.\n",
      "\n",
      "Custom Activation Functions, Initializers, Regularizers, and\n",
      "Constraints\n",
      "Most Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\n",
      "rics,  activation  functions,  layers,  and  even  full  models,  can  be  customized  in  very\n",
      "much the same way. Most of the time, you will just need to write a simple function\n",
      "with  the  appropriate  inputs  and  outputs.  Here  are  examples  of  a  custom  activation\n",
      "function  (equivalent  to  keras.activations.softplus()  or  tf.nn.softplus()),  a\n",
      "custom  Glorot  initializer  (equivalent  to  keras.initializers.glorot_normal()),  a\n",
      "custom  ℓ1  regularizer  (equivalent  to  keras.regularizers.l1(0.01)),  and  a  custom\n",
      "constraint  that  ensures  weights  are  all  positive  (equivalent  to  keras.con\n",
      "straints.nonneg() or tf.nn.relu()): [{'source': './data/ml.pdf', 'start_index': 831798}]\n",
      "* Pretrained Models for Transfer Learning                                                                 481\n",
      "Classification and Localization                                                                                   483\n",
      "Object Detection                                                                                                          485\n",
      "Fully Convolutional Networks                                                                                487\n",
      "You Only Look Once (YOLO)                                                                                489\n",
      "Semantic Segmentation                                                                                               492\n",
      "Exercises                                                                                                                        496 [{'source': './data/ml.pdf', 'start_index': 35158}]\n",
      "* • For  a  vector-to-sequence  RNN:  image  captioning,  creating  a  music  playlist\n",
      "based on an embedding of the current artist, generating a melody based on a\n",
      "set of parameters, locating pedestrians in a picture (e.g., a video frame from a\n",
      "self-driving car’s camera) [{'source': './data/ml.pdf', 'start_index': 1622873}]\n",
      "* with a Local Denoising Criterion,” Journal of Machine Learning Research 11 (2010): 3371–3408.\n",
      "\n",
      "Denoising Autoencoders \n",
      "\n",
      "| \n",
      "\n",
      "581\n",
      "\n",
      "\fdropout_encoder = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(100, activation=\"selu\"),\n",
      "    keras.layers.Dense(30, activation=\"selu\")\n",
      "])\n",
      "dropout_decoder = keras.models.Sequential([\n",
      "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
      "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
      "    keras.layers.Reshape([28, 28])\n",
      "])\n",
      "dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder]) [{'source': './data/ml.pdf', 'start_index': 1260201}]\n",
      "* input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
      "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
      "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
      "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
      "concat = keras.layers.concatenate([input_A, hidden2])\n",
      "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
      "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
      "\n",
      "Figure 10-15. Handling multiple inputs\n",
      "\n",
      "310 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras [{'source': './data/ml.pdf', 'start_index': 653114}]\n",
      "* Data API (TensorFlow)\n",
      "\n",
      "chaining transformations, 415\n",
      "helper function creation, 420\n",
      "overview of, 414\n",
      "prefetching data, 421\n",
      "preprocessing data, 419\n",
      "shuffling data, 416\n",
      "using datasets with tf.keras, 423\n",
      "\n",
      "data augmentation, 464\n",
      "data parallelism, 701, 704\n",
      "data preparation\n",
      "\n",
      "benefits of functions for, 62\n",
      "custom transformers, 68\n",
      "data cleaning, 63\n",
      "feature scaling, 69\n",
      "handling text and categorical attributes, 65\n",
      "transformation pipelines, 70\n",
      "\n",
      "data snooping bias, 51\n",
      "data visualization\n",
      "\n",
      "attribute combinations, 61\n",
      "computing correlations, 58\n",
      "dimensionality reduction, 213\n",
      "geographical data, 56\n",
      "test, training, and exploration sets, 56\n",
      "using TensorBoard for, 317\n",
      "visualizing Fashion MNIST Dataset, 574\n",
      "visualizing reconstructions, 574\n",
      "\n",
      "datasets, defined, 414\n",
      "DataViz (see data visualization)\n",
      "DBSCAN (density-based spatial clustering of\n",
      "\n",
      "applications with noise), 255\n",
      "\n",
      "decision boundaries, 145\n",
      "decision function, 93\n",
      "Decision Stumps, 203\n",
      "Decision Trees [{'source': './data/ml.pdf', 'start_index': 1736676}]\n",
      "* Figure 14-10. Depthwise max pooling can help the CNN learn any invariance\n",
      "\n",
      "Pooling Layers \n",
      "\n",
      "| \n",
      "\n",
      "459\n",
      "\n",
      "\fKeras  does  not  include  a  depthwise  max  pooling  layer,  but  TensorFlow’s  low-level\n",
      "Deep  Learning  API  does:  just  use  the  tf.nn.max_pool()  function,  and  specify  the\n",
      "kernel size and strides as 4-tuples (i.e., tuples of size 4). The first three values of each\n",
      "should be 1: this indicates that the kernel size and stride along the batch, height, and\n",
      "width  dimensions  should  be  1.  The  last  value  should  be  whatever  kernel  size  and\n",
      "stride you want along the depth dimension—for example, 3 (this must be a divisor of\n",
      "the input depth; it will not work if the previous layer outputs 20 feature maps, since\n",
      "20 is not a multiple of 3):\n",
      "\n",
      "output = tf.nn.max_pool(images,\n",
      "                        ksize=(1, 1, 1, 3),\n",
      "                        strides=(1, 1, 1, 3),\n",
      "                        padding=\"valid\") [{'source': './data/ml.pdf', 'start_index': 992559}]\n",
      "* When using RNNs, it is generally not necessary to do all this, but it may improve per‐\n",
      "formance  in  some  cases,  since  the  model  will  not  have  to  learn  the  trend  or  the\n",
      "seasonality.\n",
      "\n",
      "Apparently our simple RNN was too simple to get good performance. So let’s try to\n",
      "add more recurrent layers!\n",
      "\n",
      "Deep RNNs\n",
      "It  is  quite  common  to  stack  multiple  layers  of  cells,  as  shown  in  Figure  15-7.  This\n",
      "gives you a deep RNN.\n",
      "\n",
      "506 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\fFigure 15-7. Deep RNN (left) unrolled through time (right)\n",
      "\n",
      "Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In\n",
      "this  example,  we  use  three  SimpleRNN  layers  (but  we  could  add  any  other  type  of\n",
      "recurrent layer, such as an LSTM layer or a GRU layer, which we will discuss shortly): [{'source': './data/ml.pdf', 'start_index': 1092158}]\n",
      "* Index \n",
      "\n",
      "| \n",
      "\n",
      "811\n",
      "\n",
      "\fNVIDIA Collective Communications Library\n",
      "\n",
      "limiting risk of, 457\n",
      "\n",
      "(NCCL), 710\n",
      "\n",
      "Nvidia GPU cards, 690\n",
      "\n",
      "O\n",
      "object detection\n",
      "\n",
      "fully convolutional networks (FCNs), 487\n",
      "overview of, 485\n",
      "You Only Look Once (YOLO), 489\n",
      "\n",
      "objectness output, 486\n",
      "observed variables, 262\n",
      "observers, 654\n",
      "off-policy algorithms, 632\n",
      "offline learning, 15\n",
      "on-policy algorithms, 632\n",
      "one-class SVM algorithm, 275\n",
      "one-hot encoding, 67\n",
      "one-hot vectors, 431\n",
      "one-versus-all (OvA) strategy, 100\n",
      "one-versus-one (OvO) strategy, 100\n",
      "one-versus-the-rest (OvR) strategy, 100\n",
      "online learning, 15, 88\n",
      "online model, 639\n",
      "online SVMs, 172\n",
      "OpenAI Gym, 613-617\n",
      "Optical Character Recognition (OCR), 1\n",
      "optimal state value, 627\n",
      "optimizers\n",
      "\n",
      "AdaGrad, 354\n",
      "Adam and Nadam optimization, 356\n",
      "creating faster, 351\n",
      "first- and second-order partial derivatives,\n",
      "\n",
      "358\n",
      "\n",
      "learning rate scheduling, 359\n",
      "momentum optimization, 351\n",
      "Nesterov Accelerated Gradient (NAG), 353\n",
      "RMSProp, 355\n",
      "Stochastic Gradient Descent (SGD), 88, 124 [{'source': './data/ml.pdf', 'start_index': 1753636}]\n",
      "* core instances, 255\n",
      "corpus development, 24\n",
      "correlation coefficient, 58\n",
      "cost functions\n",
      "\n",
      "cross-entropy loss (log loss), 149\n",
      "hinge loss, 155, 173\n",
      "mean absolute error (MAE), 41, 293\n",
      "mean squared error, 120, 293, 308, 384, 570,\n",
      "\n",
      "573, 583, 636\n",
      "\n",
      "804 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "role of, 20\n",
      "\n",
      "credit assignment problem, 619\n",
      "cross-entropy loss (log loss), 149, 295\n",
      "cross-validation, 31, 73, 89\n",
      "CUDA Deep Neural Network library (cuDNN),\n",
      "\n",
      "690\n",
      "\n",
      "curiosity-based exploration, 664\n",
      "curse of dimensionality, 214\n",
      "custom models\n",
      "about, 375\n",
      "activation functions, initializers, regulariz‐\n",
      "\n",
      "ers, and constraints, 387\n",
      "\n",
      "computing gradients using Autodiff, 399,\n",
      "\n",
      "765-772\n",
      "layers, 391\n",
      "loss functions, 384\n",
      "losses and metrics, 397\n",
      "metrics, 388\n",
      "models, 394\n",
      "saving and loading, 385\n",
      "training loops, 402\n",
      "\n",
      "customer segmentation, 237\n",
      "\n",
      "D\n",
      "data (see also data preparation; data visualiza‐ [{'source': './data/ml.pdf', 'start_index': 1734995}]\n",
      "* 338 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fIn  a  2015  paper,8  Sergey  Ioffe  and  Christian  Szegedy  proposed  a  technique  called\n",
      "Batch  Normalization  (BN)  that  addresses  these  problems.  The  technique  consists  of\n",
      "adding an operation in the model just before or after the activation function of each\n",
      "hidden  layer.  This  operation  simply  zero-centers  and  normalizes  each  input,  then\n",
      "scales and shifts the result using two new parameter vectors per layer: one for scaling,\n",
      "the other for shifting. In other words, the operation lets the model learn the optimal\n",
      "scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as\n",
      "the very first layer of your neural network, you do not need to standardize your train‐\n",
      "ing set (e.g., using a StandardScaler); the BN layer will do it for you (well, approxi‐\n",
      "mately, since it only looks at one batch at a time, and it can also rescale and shift each\n",
      "input feature). [{'source': './data/ml.pdf', 'start_index': 720997}]\n",
      "* Efficient Data Representations \n",
      "\n",
      "| \n",
      "\n",
      "569\n",
      "\n",
      "\fFigure 17-1. The chess memory experiment (left) and a simple autoencoder (right)\n",
      "\n",
      "As you can see, an autoencoder typically has the same architecture as a Multi-Layer\n",
      "Perceptron (MLP; see Chapter 10), except that the number of neurons in the output\n",
      "layer must be equal to the number of inputs. In this example, there is just one hidden\n",
      "layer  composed  of  two  neurons  (the  encoder),  and  one  output  layer  composed  of\n",
      "three neurons (the decoder). The outputs are often called the reconstructions because\n",
      "the  autoencoder  tries  to  reconstruct  the  inputs,  and  the  cost  function  contains  a\n",
      "reconstruction  loss  that  penalizes  the  model  when  the  reconstructions  are  different\n",
      "from the inputs. [{'source': './data/ml.pdf', 'start_index': 1236339}]\n",
      "* Input\n",
      "\n",
      "120\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "Size\n",
      "10\n",
      "\n",
      "84\n",
      "\n",
      "1 × 1\n",
      "\n",
      "5 × 5\n",
      "\n",
      "–\n",
      "\n",
      "5 × 5\n",
      "\n",
      "2 × 2\n",
      "\n",
      "10 × 10\n",
      "\n",
      "5 × 5\n",
      "\n",
      "14 × 14\n",
      "\n",
      "2 × 2\n",
      "\n",
      "28 × 28\n",
      "\n",
      "5 × 5\n",
      "\n",
      "32 × 32 –\n",
      "\n",
      "–\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "–\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "–\n",
      "\n",
      "There are a few extra details to be noted:\n",
      "\n",
      "• MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\n",
      "normalized before being fed to the network. The rest of the network does not use\n",
      "any  padding,  which  is  why  the  size  keeps  shrinking  as  the  image  progresses\n",
      "through the network.\n",
      "\n",
      "• The  average  pooling  layers  are  slightly  more  complex  than  usual:  each  neuron\n",
      "computes the mean of its inputs, then multiplies the result by a learnable coeffi‐\n",
      "cient  (one  per  map)  and  adds  a  learnable  bias  term  (again,  one  per  map),  then\n",
      "finally applies the activation function.\n",
      "\n",
      "• Most  neurons  in  C3  maps  are  connected  to  neurons  in  only  three  or  four  S2\n",
      "maps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\n",
      "details. [{'source': './data/ml.pdf', 'start_index': 1000450}]\n",
      "* map k′ (or channel k′ if the previous layer is the input layer).\n",
      "\n",
      "• bk is the bias term for feature map k (in layer l). You can think of it as a knob that\n",
      "\n",
      "tweaks the overall brightness of the feature map k.\n",
      "\n",
      "• wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\n",
      "l and its input located at row u, column v (relative to the neuron’s receptive field),\n",
      "and feature map k′.\n",
      "\n",
      "TensorFlow Implementation\n",
      "In  TensorFlow,  each  input  image  is  typically  represented  as  a  3D  tensor  of  shape\n",
      "[height, width, channels]. A mini-batch is represented as a 4D tensor of shape [mini-\n",
      "batch  size,  height,  width,  channels].  The  weights  of  a  convolutional  layer  are  repre‐\n",
      "sented as a 4D tensor of shape [fh, fw, fn′, fn]. The bias terms of a convolutional layer\n",
      "are simply represented as a 1D tensor of shape [fn]. [{'source': './data/ml.pdf', 'start_index': 979049}]\n",
      "* Moreover, the authors showed that some neurons react only to images of horizontal\n",
      "lines,  while  others  react  only  to  lines  with  different  orientations  (two  neurons  may\n",
      "have  the  same  receptive  field  but  react  to  different  line  orientations).  They  also\n",
      "noticed that some neurons have larger receptive fields, and they react to more com‐\n",
      "plex  patterns  that  are  combinations  of  the  lower-level  patterns.  These  observations\n",
      "led to the idea that the higher-level neurons are based on the outputs of neighboring\n",
      "lower-level neurons (in Figure 14-1, notice that each neuron is connected only to a\n",
      "few neurons from the previous layer). This powerful architecture is able to detect all\n",
      "sorts of complex patterns in any area of the visual field.\n",
      "\n",
      "1 David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats,” The Journal of Physiology 147\n",
      "\n",
      "(1959): 226–238. [{'source': './data/ml.pdf', 'start_index': 967576}]\n",
      "* Let’s  look  at  a  simple  example.  The  following  code  loads  two  sample  images,  using\n",
      "Scikit-Learn’s load_sample_image() (which loads two color images, one of a Chinese\n",
      "temple, and the other of a flower), then it creates two filters and applies them to both\n",
      "images, and finally it displays one of the resulting feature maps. Note that you must\n",
      "pip install the Pillow package to use load_sample_image().\n",
      "\n",
      "from sklearn.datasets import load_sample_image\n",
      "\n",
      "# Load sample images\n",
      "china = load_sample_image(\"china.jpg\") / 255\n",
      "flower = load_sample_image(\"flower.jpg\") / 255\n",
      "\n",
      "Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "453\n",
      "\n",
      "\fimages = np.array([china, flower])\n",
      "batch_size, height, width, channels = images.shape\n",
      "\n",
      "# Create 2 filters\n",
      "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
      "filters[:, 3, :, 0] = 1  # vertical line\n",
      "filters[3, :, :, 1] = 1  # horizontal line\n",
      "\n",
      "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\") [{'source': './data/ml.pdf', 'start_index': 979903}]\n",
      "* Figure 17-14. Semantic interpolation\n",
      "\n",
      "For several years, variational autoencoders were quite popular, but GANs eventually\n",
      "took the lead, in particular because they are capable of generating much more realistic\n",
      "and crisp images. So let’s turn our attention to GANs.\n",
      "\n",
      "Variational Autoencoders \n",
      "\n",
      "| \n",
      "\n",
      "591\n",
      "\n",
      "\fGenerative Adversarial Networks\n",
      "Generative adversarial networks were proposed in a 2014 paper10 by Ian Goodfellow\n",
      "et  al.,  and  although  the  idea  got  researchers  excited  almost  instantly,  it  took  a  few\n",
      "years to overcome some of the difficulties of training GANs. Like many great ideas, it\n",
      "seems simple in hindsight: make neural networks compete against each other in the\n",
      "hope that this competition will push them to excel. As shown in Figure 17-15, a GAN\n",
      "is composed of two neural networks:\n",
      "\n",
      "Generator [{'source': './data/ml.pdf', 'start_index': 1279908}]\n",
      "* Recurrent Neurons and Layers \n",
      "\n",
      "| \n",
      "\n",
      "501\n",
      "\n",
      "\fFigure 15-4. Seq-to-seq (top left), seq-to-vector (top right), vector-to-seq (bottom left),\n",
      "and Encoder–Decoder (bottom right) networks\n",
      "\n",
      "Sounds promising, but how do you train a recurrent neural network?\n",
      "\n",
      "Training RNNs\n",
      "To  train  an  RNN,  the  trick  is  to  unroll  it  through  time  (like  we  just  did)  and  then\n",
      "simply use regular backpropagation (see Figure 15-5). This strategy is called backpro‐\n",
      "pagation through time (BPTT). [{'source': './data/ml.pdf', 'start_index': 1082600}]\n",
      "* class ResidualBlock(keras.layers.Layer):\n",
      "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
      "                                          kernel_initializer=\"he_normal\")\n",
      "                       for _ in range(n_layers)]\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        return inputs + Z\n",
      "\n",
      "This layer is a bit special since it contains other layers. This is handled transparently\n",
      "by Keras: it automatically detects that the hidden attribute contains trackable objects\n",
      "(layers  in  this  case),  so  their  variables  are  automatically  added  to  this  layer’s  list  of\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "395\n",
      "\n",
      "\fvariables. The rest of this class is self-explanatory. Next, let’s use the Subclassing API\n",
      "to define the model itself: [{'source': './data/ml.pdf', 'start_index': 851969}]\n",
      "* To convert a dense layer to a convolutional layer, the number of fil‐\n",
      "ters in the convolutional layer must be equal to the number of units\n",
      "in  the  dense  layer,  the  filter  size  must  be  equal  to  the  size  of  the\n",
      "input feature maps, and you must use \"valid\" padding. The stride\n",
      "may be set to 1 or more, as we will see shortly.\n",
      "\n",
      "Why is this important? Well, while a dense layer expects a specific input size (since it\n",
      "has one weight per input feature), a convolutional layer will happily process images of\n",
      "any  size26  (however,  it  does  expect  its  inputs  to  have  a  specific  number  of  channels,\n",
      "\n",
      "25 Jonathan Long et al., “Fully Convolutional Networks for Semantic Segmentation,” Proceedings of the IEEE\n",
      "\n",
      "Conference on Computer Vision and Pattern Recognition (2015): 3431–3440.\n",
      "\n",
      "26 There is one small exception: a convolutional layer using \"valid\" padding will complain if the input size is\n",
      "\n",
      "smaller than the kernel size.\n",
      "\n",
      "Object Detection \n",
      "\n",
      "| \n",
      "\n",
      "487 [{'source': './data/ml.pdf', 'start_index': 1049924}]\n",
      "* Tying Weights                                                                                                            577\n",
      "Training One Autoencoder at a Time                                                                    578\n",
      "Convolutional Autoencoders                                                                                      579\n",
      "Recurrent Autoencoders                                                                                             580\n",
      "Denoising Autoencoders                                                                                             581\n",
      "Sparse Autoencoders                                                                                                    582\n",
      "Variational Autoencoders                                                                                           586\n",
      "Generating Fashion MNIST Images                                                                      590 [{'source': './data/ml.pdf', 'start_index': 40690}]\n",
      "* Main Challenges of Machine Learning \n",
      "\n",
      "| \n",
      "\n",
      "27\n",
      "\n",
      "\fComplex models such as deep neural networks can detect subtle patterns in the data,\n",
      "but if the training set is noisy, or if it is too small (which introduces sampling noise),\n",
      "then the model is likely to detect patterns in the noise itself. Obviously these patterns\n",
      "will not generalize to new instances. For example, say you feed your life satisfaction\n",
      "model  many  more  attributes,  including  uninformative  ones  such  as  the  country’s\n",
      "name. In that case, a complex model may detect patterns like the fact that all coun‐\n",
      "tries in the training data with a w in their name have a life satisfaction greater than 7:\n",
      "New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\n",
      "are you that the w-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\n",
      "this pattern occurred in the training data by pure chance, but the model has no way\n",
      "to tell whether a pattern is real or simply the result of noise in the data. [{'source': './data/ml.pdf', 'start_index': 120540}]\n",
      "* benefits of, xvi\n",
      "complex architectures, 314\n",
      "gradient clipping in, 345\n",
      "implementing Batch Normalization with,\n",
      "\n",
      "341\n",
      "\n",
      "implementing dropout using, 367\n",
      "implementing MLPs with, 295-320\n",
      "implementing ResNet-34 with, 478\n",
      "keras.callbacks package, 316\n",
      "loading datasets with, 297\n",
      "low-level API, 381\n",
      "multibackend Keras, 295\n",
      "preprocessing layers, 437\n",
      "saving and restoring models in, 314\n",
      "stacked autoencoders using, 572\n",
      "transfer learning with, 347\n",
      "using code examples from keras.io, 300\n",
      "using pretrained models from, 479\n",
      "\n",
      "Keras Tuner, 322\n",
      "Kernel PCA (kPCA), 226-230\n",
      "kernel trick, 158, 228\n",
      "kernelized SVM, 169\n",
      "kernels, 170, 226, 377\n",
      "kopt library, 322\n",
      "Kullback–Leibler divergence, 150\n",
      "\n",
      "L\n",
      "label propagation, 254\n",
      "labels, 8, 39, 239\n",
      "Lagrange multipliers, 761\n",
      "landmarks, 159\n",
      "language models, 563 (see also natural language\n",
      "\n",
      "processing (NLP))\n",
      "\n",
      "large margin classification, 153\n",
      "Lasso Regression, 137\n",
      "latent loss, 587\n",
      "latent representations, 567\n",
      "latent variables, 262\n",
      "law of large numbers, 191\n",
      "Layer Normalization, 512 [{'source': './data/ml.pdf', 'start_index': 1746327}]\n",
      "* processing (NLP))\n",
      "\n",
      "large margin classification, 153\n",
      "Lasso Regression, 137\n",
      "latent loss, 587\n",
      "latent representations, 567\n",
      "latent variables, 262\n",
      "law of large numbers, 191\n",
      "Layer Normalization, 512\n",
      "\n",
      "layers\n",
      "\n",
      "1D convolutional layer, 520\n",
      "adaptive instance normalization (AdaIN),\n",
      "\n",
      "604\n",
      "\n",
      "bidirectional recurrent layer, 546\n",
      "convolutional layer, 448-456\n",
      "dense (fully connected) layer, 285\n",
      "hidden layer, 289\n",
      "input layer, 289\n",
      "Masked Multi-Head Attention layer, 556\n",
      "minibatch standard deviation layer, 603\n",
      "Multi-Head Attention layer, 556, 559\n",
      "output layer, 289\n",
      "pooling layer, 456\n",
      "recurrent, 498-502\n",
      "reusing pretrained, 345-351\n",
      "Scaled Dot-Product Attention layer, 559 [{'source': './data/ml.pdf', 'start_index': 1747130}]\n",
      "* parameter vector v), and they observed that the dot product variants performed bet‐\n",
      "ter than concatenative attention. For this reason, concatenative attention is much less\n",
      "used  now.  The  equations  for  these  three  attention  mechanisms  are  summarized  in\n",
      "Equation 16-1. [{'source': './data/ml.pdf', 'start_index': 1197646}]\n",
      "* encoding using embeddings, 433\n",
      "encoding using one-hot vectors, 431\n",
      "\n",
      "causal models, 510\n",
      "centroids, 238\n",
      "chain rule, 290\n",
      "chaining transformations, 415\n",
      "character RNNs (Char-RNNs)\n",
      "building and training, 530\n",
      "\n",
      "chopping sequential datasets, 528\n",
      "generating Shakespearean text, 531\n",
      "overview of, 526\n",
      "splitting sequential datasets, 527\n",
      "stateful RNNs and, 532\n",
      "training dataset creation, 527\n",
      "using, 531\n",
      "chatbots, 525\n",
      "chi-squared test, 182\n",
      "Classification and Regression Tree (CART),\n",
      "\n",
      "177, 179\n",
      "\n",
      "classification problems\n",
      "\n",
      "AdaBoost classifiers, 200\n",
      "binary classifiers, 88\n",
      "classification and localization, 483\n",
      "classification MLPs, 294\n",
      "error analysis, 102\n",
      "example of, 8\n",
      "Extra-Trees classifier, 198\n",
      "hard margin classification, 154\n",
      "image classifiers using Sequential APIs,\n",
      "\n",
      "297-307 [{'source': './data/ml.pdf', 'start_index': 1732471}]\n",
      "* The  batter  hits  the  ball.  The  outfielder  immediately  starts  running,  anticipating  the\n",
      "ball’s  trajectory.  He  tracks  it,  adapts  his  movements,  and  finally  catches  it  (under  a\n",
      "thunder of applause). Predicting the future is something you do all the time, whether\n",
      "you are finishing a friend’s sentence or anticipating the smell of coffee at breakfast. In\n",
      "this chapter we will discuss recurrent neural networks (RNNs), a class of nets that can\n",
      "predict the future (well, up to a point, of course). They can analyze time series data\n",
      "such as stock prices, and tell you when to buy or sell. In autonomous driving systems,\n",
      "they can anticipate car trajectories and help avoid accidents. More generally, they can\n",
      "work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the\n",
      "nets we have considered so far. For example, they can take sentences, documents, or\n",
      "audio samples as input, making them extremely useful for natural language process‐ [{'source': './data/ml.pdf', 'start_index': 1072490}]\n",
      "* Y\n",
      "You Only Look Once (YOLO), 489\n",
      "\n",
      "Z\n",
      "zero padding, 449\n",
      "zero-shot learning (ZSL), 564\n",
      "ZF Net, 466\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "819\n",
      "\n",
      "\fAbout the Author\n",
      "\n",
      "Aurélien Géron is a Machine Learning consultant and lecturer. A former Googler, he\n",
      "led YouTube’s video classification team from 2013 to 2016. He’s been a founder of and\n",
      "CTO at a few different companies: Wifirst, a leading wireless ISP in France; Polycon‐\n",
      "seil, a consulting firm focused on telecoms, media, and strategy; and Kiwisoft, a con‐\n",
      "sulting firm focused on Machine Learning and data privacy.\n",
      "\n",
      "Before all that he worked as an engineer in a variety of domains: finance (JP Morgan\n",
      "and Société Générale), defense (Canada’s DOD), and healthcare (blood transfusion).\n",
      "He also published a few technical books (on C++, WiFi, and internet architectures)\n",
      "and lectured about computer science at a French engineering school. [{'source': './data/ml.pdf', 'start_index': 1771246}]\n",
      "* Now  let’s  compute  how  much  RAM  this  neural  network  will  require  (at  least)\n",
      "when  making  a  prediction  for  a  single  instance.  First  let’s  compute  the  feature\n",
      "map size for each layer. Since we are using a stride of 2 and \"same\" padding, the\n",
      "horizontal and vertical dimensions of the feature maps are divided by 2 at each\n",
      "layer (rounding up if necessary). So, as the input channels are 200 × 300 pixels,\n",
      "the first layer’s feature maps are 100 × 150, the second layer’s feature maps are 50\n",
      "× 75, and the third layer’s feature maps are 25 × 38. Since 32 bits is 4 bytes and\n",
      "the first convolutional layer has 100 feature maps, this first layer takes up 4 × 100\n",
      "× 150 × 100 = 6 million bytes (6 MB). The second layer takes up 4 × 50 × 75 ×\n",
      "200 = 3 million bytes (3 MB). Finally, the third layer takes up 4 × 25 × 38 × 400 =\n",
      "1,520,000  bytes  (about  1.5  MB).  However,  once  a  layer  has  been  computed,  the [{'source': './data/ml.pdf', 'start_index': 1615806}]\n",
      "* 1 You can get the best of both worlds by being open to biological inspirations without being afraid to create\n",
      "\n",
      "biologically unrealistic models, as long as they work well.\n",
      "\n",
      "279\n",
      "\n",
      "\flevel API for building, training, evaluating, and running neural networks. But don’t\n",
      "be fooled by its simplicity: it is expressive and flexible enough to let you build a wide\n",
      "variety of neural network architectures. In fact, it will probably be sufficient for most\n",
      "of  your  use  cases.  And  should  you  ever  need  extra  flexibility,  you  can  always  write\n",
      "custom Keras components using its lower-level API, as we will see in Chapter 12.\n",
      "\n",
      "But first, let’s go back in time to see how artificial neural networks came to be! [{'source': './data/ml.pdf', 'start_index': 587187}]\n",
      "* model = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    RegularizedDense(300),\n",
      "    RegularizedDense(100),\n",
      "    RegularizedDense(10, activation=\"softmax\",\n",
      "                     kernel_initializer=\"glorot_uniform\")\n",
      "])\n",
      "\n",
      "Dropout\n",
      "Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  net‐\n",
      "works. It was proposed in a paper23 by Geoffrey Hinton in 2012 and further detailed\n",
      "in a 2014 paper24 by Nitish Srivastava et al., and it has proven to be highly successful:\n",
      "even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding\n",
      "dropout. This may not sound like a lot, but when a model already has 95% accuracy,\n",
      "getting  a  2%  accuracy  boost  means  dropping  the  error  rate  by  almost  40%  (going\n",
      "from 5% error to roughly 3%). [{'source': './data/ml.pdf', 'start_index': 784497}]\n",
      "* Q\n",
      "Q-Learning\n",
      "\n",
      "Approximate Q-Learning and Deep Q-\n",
      "\n",
      "Learning, 633\n",
      "exploration policy, 632\n",
      "implementing, 631\n",
      "overview of, 630\n",
      "Q-Value Iteration, 628\n",
      "Q-Values, 628\n",
      "Quadratic Programming (QP) problems, 167\n",
      "quantization-aware training, 687\n",
      "queries per second (QPS), 667\n",
      "questions and comments, xxiii, 718\n",
      "queues, 383, 788\n",
      "\n",
      "R\n",
      "Radial Basis Function (RBF), 159\n",
      "\n",
      "ragged tensors, 383, 784\n",
      "Rainbow agent, 642\n",
      "Random Forests\n",
      "\n",
      "benefits of, 189\n",
      "Extra-Trees, 198\n",
      "feature importance, 198\n",
      "overview of, 197\n",
      "\n",
      "random initialization, 118\n",
      "random patches and random subspaces, 196\n",
      "random projections, 232\n",
      "randomized leaky ReLU (RReLU), 335\n",
      "Randomized PCA, 225\n",
      "recall, 91-97\n",
      "receiver operating characteristic (ROC) curve,\n",
      "\n",
      "97\n",
      "\n",
      "recognition network, 569\n",
      "recommender systems, 237\n",
      "reconstruction error, 224\n",
      "reconstruction loss, 397, 570\n",
      "reconstruction pre-images, 228\n",
      "reconstructions, 570\n",
      "Rectified Linear Unit function (ReLU), 292-293\n",
      "recurrent autoencoders, 580\n",
      "recurrent neural networks (RNNs) [{'source': './data/ml.pdf', 'start_index': 1756925}]\n",
      "* Frequency (TF-IDF), 439\n",
      "\n",
      "terminal state, 626\n",
      "test sets, 30, 51\n",
      "\n",
      "building and training models for, 530\n",
      "chopping sequential datasets, 528\n",
      "generating Shakespearean text, 531\n",
      "overview of, 526\n",
      "splitting sequential datasets, 527\n",
      "stateful RNNs and, 532\n",
      "training dataset creation, 527\n",
      "using models for, 531\n",
      "TF Datasets (TFDS), 414, 441\n",
      "TF Functions\n",
      "\n",
      "graphs generated by, 791-799\n",
      "rules, 409\n",
      "\n",
      "TF Transform (tf.Transform), 414, 439\n",
      "TF-Agents library\n",
      "\n",
      "collect driver, 656\n",
      "datasets, 658\n",
      "deep Q-networks (DQNs), 650\n",
      "DQN agents, 652\n",
      "environment specifications, 644\n",
      "environment wrappers, 645\n",
      "environments, 643\n",
      "installing, 643\n",
      "overview of, 642\n",
      "replay buffer and observer, 654\n",
      "training architecture, 649\n",
      "training loops, 661\n",
      "training metrics, 655\n",
      "tf.keras, 295, 363, 363, 423\n",
      "tf.summary package, 319\n",
      "TF.Text library, 536\n",
      "TFRecord format\n",
      "\n",
      "compressed TFRecord files, 425\n",
      "lists of lists using SequenceExample Proto‐\n",
      "\n",
      "buf, 429 [{'source': './data/ml.pdf', 'start_index': 1767361}]\n",
      "* f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the\n",
      "bottom-right dataset under “DATA”), and change the network architecture to\n",
      "have  four  hidden  layers  with  eight  neurons  each.  Notice  that  training  takes\n",
      "much  longer  and  often  gets  stuck  on  plateaus  for  long  periods  of  time.  Also\n",
      "notice  that  the  neurons  in  the  highest  layers  (on  the  right)  tend  to  evolve\n",
      "faster than the neurons in the lowest layers (on the left). This problem, called\n",
      "the “vanishing gradients” problem, can be alleviated with better weight initial‐\n",
      "ization and other techniques, better optimizers (such as AdaGrad or Adam),\n",
      "or Batch Normalization (discussed in Chapter 11).\n",
      "\n",
      "g. Go further. Take an hour or so to play around with other parameters and get a\n",
      "feel  for  what  they  do,  to  build  an  intuitive  understanding  about  neural\n",
      "networks. [{'source': './data/ml.pdf', 'start_index': 698661}]\n",
      "* 68 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\f        else:\n",
      "            return np.c_[X, rooms_per_household, population_per_household]\n",
      "\n",
      "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
      "housing_extra_attribs = attr_adder.transform(housing.values)\n",
      "\n",
      "In  this  example  the  transformer  has  one  hyperparameter,  add_bedrooms_per_room,\n",
      "set to True by default (it is often helpful to provide sensible defaults). This hyperpara‐\n",
      "meter  will  allow  you  to  easily  find  out  whether  adding  this  attribute  helps  the\n",
      "Machine Learning algorithms or not. More generally, you can add a hyperparameter\n",
      "to  gate  any  data  preparation  step  that  you  are  not  100%  sure  about.  The  more  you\n",
      "automate these data preparation steps, the more combinations you can automatically\n",
      "try out, making it much more likely that you will find a great combination (and sav‐\n",
      "ing you a lot of time). [{'source': './data/ml.pdf', 'start_index': 201520}]\n",
      "* (2016).\n",
      "\n",
      "Handling Long Sequences \n",
      "\n",
      "| \n",
      "\n",
      "521\n",
      "\n",
      "\fFigure 15-11. WaveNet architecture\n",
      "\n",
      "In the WaveNet paper, the authors actually stacked 10 convolutional layers with dila‐\n",
      "tion rates of 1, 2, 4, 8, …, 256, 512, then they stacked another group of 10 identical\n",
      "layers  (also  with  dilation  rates  1,  2,  4,  8,  …,  256,  512),  then  again  another  identical\n",
      "group of 10 layers. They justified this architecture by pointing out that a single stack\n",
      "of 10 convolutional layers with these dilation rates will act like a super-efficient con‐\n",
      "volutional  layer  with  a  kernel  of  size  1,024  (except  way  faster,  more  powerful,  and\n",
      "using significantly fewer parameters), which is why they stacked 3 such blocks. They\n",
      "also left-padded the input sequences with a number of zeros equal to the dilation rate\n",
      "before  every  layer,  to  preserve  the  same  sequence  length  throughout  the  network.\n",
      "Here  is  how  to  implement  a  simplified  WaveNet  to  tackle  the  same  sequences  as\n",
      "earlier:14 [{'source': './data/ml.pdf', 'start_index': 1126506}]\n",
      "* 497\n",
      "Recurrent Neurons and Layers                                                                                   498\n",
      "Memory Cells                                                                                                            500\n",
      "Input and Output Sequences                                                                                  501\n",
      "Training RNNs                                                                                                              502\n",
      "Forecasting a Time Series                                                                                            503\n",
      "Baseline Metrics                                                                                                        505\n",
      "Implementing a Simple RNN                                                                                  505\n",
      "Deep RNNs                                                                                                                506 [{'source': './data/ml.pdf', 'start_index': 36098}]\n",
      "* Just  like  in  convolutional  layers,  each  neuron  in  a  pooling  layer  is  connected  to  the\n",
      "outputs of a limited number of neurons in the previous layer, located within a small\n",
      "rectangular receptive field. You must define its size, the stride, and the padding type,\n",
      "just like before. However, a pooling neuron has no weights; all it does is aggregate the\n",
      "inputs using an aggregation function such as the max or mean. Figure 14-8 shows a\n",
      "max pooling layer, which is the most common type of pooling layer. In this example,\n",
      "we use a 2 × 2 pooling kernel,9 with a stride of 2 and no padding. Only the max input\n",
      "value  in  each  receptive  field  makes  it  to  the  next  layer,  while  the  other  inputs  are\n",
      "dropped. For example, in the lower-left receptive field in Figure 14-8, the input values\n",
      "are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the\n",
      "stride of 2, the output image has half the height and half the width of the input image [{'source': './data/ml.pdf', 'start_index': 987299}]\n",
      "* WaveNet\n",
      "\n",
      "In a 2016 paper,13 Aaron van den Oord and other DeepMind researchers introduced\n",
      "an architecture called WaveNet. They stacked 1D convolutional layers, doubling the\n",
      "dilation rate (how spread apart each neuron’s inputs are) at every layer: the first con‐\n",
      "volutional layer gets a glimpse of just two time steps at a time, while the next one sees\n",
      "four time steps (its receptive field is four time steps long), the next one sees eight time\n",
      "steps, and so on (see Figure 15-11). This way, the lower layers learn short-term pat‐\n",
      "terns, while the higher layers learn long-term patterns. Thanks to the doubling dila‐\n",
      "tion rate, the network can process extremely large sequences very efficiently.\n",
      "\n",
      "13 Aaron van den Oord et al., “WaveNet: A Generative Model for Raw Audio,” arXiv preprint arXiv:1609.03499\n",
      "\n",
      "(2016).\n",
      "\n",
      "Handling Long Sequences \n",
      "\n",
      "| \n",
      "\n",
      "521\n",
      "\n",
      "\fFigure 15-11. WaveNet architecture [{'source': './data/ml.pdf', 'start_index': 1125703}]\n",
      "* Figure 14-12. Generating new training instances from existing ones\n",
      "\n",
      "AlexNet also uses a competitive normalization step immediately after the ReLU step\n",
      "of layers C1 and C3, called local response normalization (LRN): the most strongly acti‐\n",
      "vated neurons inhibit other neurons located at the same position in neighboring fea‐\n",
      "ture  maps  (such  competitive  activation  has  been  observed  in  biological  neurons).\n",
      "This encourages different feature maps to specialize, pushing them apart and forcing\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "465\n",
      "\n",
      "\fthem to explore a wider range of features, ultimately improving generalization. Equa‐\n",
      "tion 14-2 shows how to apply LRN.\n",
      "\n",
      "Equation 14-2. Local response normalization (LRN)\n",
      "\n",
      "j\n",
      "\n",
      "high\n",
      "bi = ai k + α ∑\n",
      "j = j\n",
      "\n",
      "low\n",
      "\n",
      "−β\n",
      "\n",
      "2\n",
      "\n",
      "a j\n",
      "\n",
      "with\n",
      "\n",
      "jhigh = min i +\n",
      "\n",
      "r\n",
      "2\n",
      "\n",
      ", f n − 1\n",
      "\n",
      "jlow = max 0, i −\n",
      "\n",
      "r\n",
      "2\n",
      "\n",
      "In this equation: [{'source': './data/ml.pdf', 'start_index': 1005238}]\n",
      "* 570 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\ffrom tensorflow import keras\n",
      "\n",
      "encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\n",
      "decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n",
      "autoencoder = keras.models.Sequential([encoder, decoder])\n",
      "\n",
      "autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.1))\n",
      "\n",
      "This code is really not very different from all the MLPs we built in past chapters, but\n",
      "there are a few things to note:\n",
      "\n",
      "• We  organized  the  autoencoder  into  two  subcomponents:  the  encoder  and  the\n",
      "decoder. Both are regular Sequential models with a single Dense layer each, and\n",
      "the autoencoder is a Sequential model containing the encoder followed by the\n",
      "decoder (remember that a model can be used as a layer in another model).\n",
      "\n",
      "• The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3). [{'source': './data/ml.pdf', 'start_index': 1237975}]\n",
      "* 600 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fFigure 17-18. Vector arithmetic for visual concepts (part of figure 7 from the DCGAN\n",
      "paper)14\n",
      "\n",
      "If you add each image’s class as an extra input to both the generator\n",
      "and  the  discriminator,  they  will  both  learn  what  each  class  looks\n",
      "like,  and  thus  you  will  be  able  to  control  the  class  of  each  image\n",
      "produced  by  the  generator.  This  is  called  a  conditional  GAN15\n",
      "(CGAN).\n",
      "\n",
      "DCGANs  aren’t  perfect,  though.  For  example,  when  you  try  to  generate  very  large\n",
      "images using DCGANs, you often end up with locally convincing features but overall\n",
      "inconsistencies (such as shirts with one sleeve much longer than the other). How can\n",
      "you fix this? [{'source': './data/ml.pdf', 'start_index': 1299961}]\n",
      "* The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1\n",
      "\n",
      "Just  like  the  logistic  function,  this  activation  function  is  S-shaped,  continuous,\n",
      "and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in\n",
      "the  case  of  the  logistic  function).  That  range  tends  to  make  each  layer’s  output\n",
      "more  or  less  centered  around  0  at  the  beginning  of  training,  which  often  helps\n",
      "speed up convergence.\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "291\n",
      "\n",
      "\fThe Rectified Linear Unit function: ReLU(z) = max(0, z) [{'source': './data/ml.pdf', 'start_index': 611291}]\n",
      "* Not only does this hierarchical architecture help DNNs converge faster to a good sol‐\n",
      "ution, but it also improves their ability to generalize to new datasets. For example, if\n",
      "you have already trained a model to recognize faces in pictures and you now want to\n",
      "train a new neural network to recognize hairstyles, you can kickstart the training by\n",
      "reusing  the  lower  layers  of  the  first  network.  Instead  of  randomly  initializing  the\n",
      "weights and biases of the first few layers of the new neural network, you can initialize\n",
      "them to the values of the weights and biases of the lower layers of the first network.\n",
      "This way the network will not have to learn from scratch all the low-level structures\n",
      "that occur in most pictures; it will only have to learn the higher-level structures (e.g.,\n",
      "hairstyles). This is called transfer learning. [{'source': './data/ml.pdf', 'start_index': 686662}]\n",
      "* TensorFlow, data loading and preprocessing\n",
      "\n",
      "Data API, 414-424\n",
      "overview of, 413\n",
      "preprocessing input features, 430-439\n",
      "TensorFlow Datasets (TFDS) Project, 441,\n",
      "\n",
      "441\n",
      "\n",
      "TF Transform, 439\n",
      "TFRecord format, 424-430\n",
      "TensorFlow, functions and graphs\n",
      "\n",
      "AutoGraph and tracing, 407, 791-799\n",
      "overview of, 405\n",
      "TF Function rules, 409\n",
      "\n",
      "TensorFlow, model deployment at scale\n",
      "deploying on AI platforms, 81\n",
      "deploying to mobile and embedded devices,\n",
      "\n",
      "685-688\n",
      "overview of, 667\n",
      "serving TensorFlow models, 668-685\n",
      "training models across multiple devices,\n",
      "\n",
      "701-717\n",
      "\n",
      "using GPUs to speed computations, 689-701\n",
      "\n",
      "TensorFlow, NumPy-like operations\n",
      "\n",
      "other data structures, 383\n",
      "tensors and NumPy, 381\n",
      "tensors and operations, 379\n",
      "type conversions, 381\n",
      "variables, 382\n",
      "TensorFlow.js, 378\n",
      "tensors, 379\n",
      "Term-Frequency × Inverse-Document-\n",
      "\n",
      "Frequency (TF-IDF), 439\n",
      "\n",
      "terminal state, 626\n",
      "test sets, 30, 51 [{'source': './data/ml.pdf', 'start_index': 1766560}]\n",
      "* 366 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fIn practice, you can usually apply dropout only to the neurons in\n",
      "the top one to three layers (excluding the output layer).\n",
      "\n",
      "There  is  one  small  but  important  technical  detail.  Suppose  p  =  50%,  in  which  case\n",
      "during  testing  a  neuron  would  be  connected  to  twice  as  many  input  neurons  as  it\n",
      "would be (on average) during training. To compensate for this fact, we need to multi‐\n",
      "ply  each  neuron’s  input  connection  weights  by  0.5  after  training.  If  we  don’t,  each\n",
      "neuron  will  get  a  total  input  signal  roughly  twice  as  large  as  what  the  network  was\n",
      "trained on and will be unlikely to perform well. More generally, we need to multiply\n",
      "each input connection weight by the keep probability (1 – p) after training. Alterna‐\n",
      "tively,  we  can  divide  each  neuron’s  output  by  the  keep  probability  during  training\n",
      "(these alternatives are not perfectly equivalent, but they work equally well). [{'source': './data/ml.pdf', 'start_index': 788666}]\n",
      "* To tie weights between layers using Keras, let’s define a custom layer:\n",
      "\n",
      "class DenseTranspose(keras.layers.Layer):\n",
      "    def __init__(self, dense, activation=None, **kwargs):\n",
      "        self.dense = dense\n",
      "        self.activation = keras.activations.get(activation)\n",
      "        super().__init__(**kwargs)\n",
      "    def build(self, batch_input_shape):\n",
      "        self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\",\n",
      "                                      shape=[self.dense.input_shape[-1]])\n",
      "        super().build(batch_input_shape)\n",
      "    def call(self, inputs):\n",
      "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
      "        return self.activation(z + self.biases) [{'source': './data/ml.pdf', 'start_index': 1249984}]\n",
      "* in MLPs, 289\n",
      "neurons per hidden layer, 324\n",
      "number of, 323\n",
      "hidden units, 775\n",
      "hierarchical clustering algorithms, 10\n",
      "Hierarchical DBSCAN (HDBSCAN), 258\n",
      "high-dimensional training sets, 213\n",
      "hinge loss function, 155, 173\n",
      "Hinton, Geoffrey, xv\n",
      "histograms, 50\n",
      "hold outs, 31\n",
      "holdout validation, 31\n",
      "Hopfield networks, 773\n",
      "Huber loss, 293, 384\n",
      "Hyperas, 322\n",
      "Hyperband, 323\n",
      "hyperbolic tangent function (tanh), 291\n",
      "Hyperopt, 322\n",
      "hyperparameters\n",
      "defined, 29\n",
      "fine-tuning for neural networks, 320-327\n",
      "hyperparameter tuning, 31, 75\n",
      "learning rate, 118\n",
      "Python libraries for optimization, 322\n",
      "regularization hyperparameters, 181\n",
      "\n",
      "hyperplanes, 165\n",
      "hypothesis boosting, 199\n",
      "\n",
      "I\n",
      "identity matrix, 137\n",
      "image classification\n",
      "\n",
      "multitask classification, 311\n",
      "using Sequential API, 297-307\n",
      "\n",
      "image generation, 495\n",
      "image segmentation, 238, 249\n",
      "importance sampling (IS), 640\n",
      "impurity, 177, 180\n",
      "\n",
      "808 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "imputation, 503\n",
      "incremental learning, 16\n",
      "Incremental PCA (IPCA), 225\n",
      "independent and identically distributed (IID),\n",
      "\n",
      "126 [{'source': './data/ml.pdf', 'start_index': 1744111}]\n",
      "* 5 It would have been simpler to inherit from SimpleRNNCell instead so that we wouldn’t have to create an inter‐\n",
      "nal SimpleRNNCell or handle the state_size and output_size attributes, but the goal here was to show how\n",
      "to create a custom cell from scratch.\n",
      "\n",
      "Handling Long Sequences \n",
      "\n",
      "| \n",
      "\n",
      "513\n",
      "\n",
      "\fby the activation function. Finally, it returns the outputs twice (once as the outputs,\n",
      "and once as the new hidden states). To use this custom cell, all we need to do is create\n",
      "a keras.layers.RNN layer, passing it a cell instance:\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
      "                     input_shape=[None, 1]),\n",
      "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
      "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
      "]) [{'source': './data/ml.pdf', 'start_index': 1110677}]\n",
      "* global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
      "\n",
      "It’s equivalent to this simple Lambda layer, which computes the mean over the spatial\n",
      "dimensions (height and width):\n",
      "\n",
      "global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\n",
      "\n",
      "Now you know all the building blocks to create convolutional neural networks. Let’s\n",
      "see how to assemble them.\n",
      "\n",
      "CNN Architectures\n",
      "Typical  CNN  architectures  stack  a  few  convolutional  layers  (each  one  generally  fol‐\n",
      "lowed by a ReLU layer), then a pooling layer, then another few convolutional layers\n",
      "(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\n",
      "as it progresses through the network, but it also typically gets deeper and deeper (i.e.,\n",
      "with more feature maps), thanks to the convolutional layers (see Figure 14-11). At the\n",
      "top of the stack, a regular feedforward neural network is added, composed of a few\n",
      "\n",
      "460 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks [{'source': './data/ml.pdf', 'start_index': 994437}]\n",
      "* Index \n",
      "\n",
      "| \n",
      "\n",
      "817\n",
      "\n",
      "\fadditional models for, 506\n",
      "baseline metrics, 505\n",
      "deep RNNS, 506\n",
      "forecasting several steps ahead, 508\n",
      "overview of, 503\n",
      "RNNS for, 497\n",
      "simple RNNs, 505\n",
      "\n",
      "time step, 498\n",
      "tokenization, 536\n",
      "tolerance, 123\n",
      "TPUs (tensor processing units), 377\n",
      "train-dev sets, 32\n",
      "training data\n",
      "defined, 2\n",
      "hold outs, 31\n",
      "insufficient quantity of, 23\n",
      "irrelevant features, 27\n",
      "nonrepresentative, 25\n",
      "overfitting, 27\n",
      "poor quality, 26\n",
      "training dataset creation, 527\n",
      "underfitting, 29\n",
      "training instances, 2, 215\n",
      "training models\n",
      "defined, 20\n",
      "example project, 72\n",
      "Gradient Descent, 118-128\n",
      "learning curves, 130-134\n",
      "Linear Regression, 112-117\n",
      "Logistic Regression, 142-151\n",
      "overview of, 111\n",
      "Polynomial Regression, 128-130\n",
      "regularized linear models, 134-142\n",
      "\n",
      "training samples, 2\n",
      "training set rotation, 185\n",
      "training sets, 2, 30, 213\n",
      "training/serving skew, 440\n",
      "trajectories, 649\n",
      "trajectory, 650\n",
      "transfer learning, 324, 345, 481\n",
      "transformations [{'source': './data/ml.pdf', 'start_index': 1768533}]\n",
      "* Creating the Deep Q-Network                                                                                650\n",
      "Creating the DQN Agent                                                                                         652\n",
      "Creating the Replay Buffer and the Corresponding Observer                           654\n",
      "Creating Training Metrics                                                                                       655\n",
      "Creating the Collect Driver                                                                                     656\n",
      "Creating the Dataset                                                                                                 658\n",
      "Creating the Training Loop                                                                                     661\n",
      "Overview of Some Popular RL Algorithms                                                              662 [{'source': './data/ml.pdf', 'start_index': 45165}]\n",
      "* with a set of images along with their captions; it learns to associate high-level features\n",
      "in  images  with  high-level  features  in  captions.  Next,  if  you  feed  the  image  DBN  an\n",
      "image  of  a  car,  the  signal  will  propagate  through  the  network,  up  to  the  top-level\n",
      "RBM, and back down to the bottom of the caption DBN, producing a caption. Due to\n",
      "the stochastic nature of RBMs and DBNs, the caption will keep changing randomly,\n",
      "but it will generally be appropriate for the image. If you generate a few hundred cap‐\n",
      "tions,  the  most  frequently  generated  ones  will  likely  be  a  good  description  of  the\n",
      "image.3 [{'source': './data/ml.pdf', 'start_index': 1693513}]\n",
      "* Let’s use tf.keras to implement Layer Normalization within a simple memory cell. For\n",
      "this, we need to define a custom memory cell. It is just like a regular layer, except its\n",
      "call() method takes two arguments: the inputs at the current time step and the hid‐\n",
      "den states from the previous time step. Note that the states argument is a list con‐\n",
      "taining  one  or  more  tensors.  In  the  case  of  a  simple  RNN  cell  it  contains  a  single\n",
      "tensor equal to the outputs of the previous time step, but other cells may have multi‐\n",
      "ple state tensors (e.g., an LSTMCell has a long-term state and a short-term state, as we\n",
      "will  see  shortly).  A  cell  must  also  have  a  state_size  attribute  and  an  output_size\n",
      "attribute. In a simple RNN, both are simply equal to the number of units. The follow‐\n",
      "ing code implements a custom memory cell which will behave like a SimpleRNNCell,\n",
      "except it will also apply Layer Normalization at each time step: [{'source': './data/ml.pdf', 'start_index': 1108004}]\n",
      "* 590 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fin a 3 × 4 grid, and we use TensorFlow’s tf.image.resize() function to resize this\n",
      "grid to 5 × 7. By default, the resize() function will perform bilinear interpolation, so\n",
      "every  other  row  and  column  will  contain  interpolated  codings.  We  then  use  the\n",
      "decoder to produce all the images:\n",
      "\n",
      "codings_grid = tf.reshape(codings, [1, 3, 4, codings_size])\n",
      "larger_grid = tf.image.resize(codings_grid, size=[5, 7])\n",
      "interpolated_codings = tf.reshape(larger_grid, [-1, codings_size])\n",
      "images = variational_decoder(interpolated_codings).numpy()\n",
      "\n",
      "Figure 17-14 shows the resulting images. The original images are framed, and the rest\n",
      "are the result of semantic interpolation between the nearby images. Notice, for exam‐\n",
      "ple, how the shoe in the fourth row and fifth column is a nice interpolation between\n",
      "the two shoes located above and below it.\n",
      "\n",
      "Figure 17-14. Semantic interpolation [{'source': './data/ml.pdf', 'start_index': 1278966}]\n",
      "* string tensors, 383, 783\n",
      "strong learners, 190\n",
      "style mixing, 606\n",
      "style transfer, 604\n",
      "StyleGANs, 567, 604\n",
      "Subclassing API, 313\n",
      "subderivatives, 173\n",
      "subgradient vector, 140\n",
      "subsampling, 456\n",
      "subspace, 215\n",
      "summaries (TensorFlow), 317\n",
      "supervised learning [{'source': './data/ml.pdf', 'start_index': 1764505}]\n",
      "* 12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   375\n",
      "A Quick Tour of TensorFlow                                                                                      376\n",
      "Using TensorFlow like NumPy                                                                                   379\n",
      "Tensors and Operations                                                                                           379\n",
      "Tensors and NumPy                                                                                                 381\n",
      "Type Conversions                                                                                                     381\n",
      "Variables                                                                                                                     382\n",
      "Other Data Structures                                                                                              383\n",
      "\n",
      "viii \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents [{'source': './data/ml.pdf', 'start_index': 27915}]\n",
      "* There  is  no  PositionalEmbedding  layer  in  TensorFlow,  but  it  is  easy  to  create  one.\n",
      "For efficiency reasons, we precompute the positional embedding matrix in the con‐\n",
      "structor  (so  we  need  to  know  the  maximum  sentence  length,  max_steps,  and  the\n",
      "number  of  dimensions  for  each  word  representation,  max_dims).  Then  the  call()\n",
      "method  crops  this  embedding  matrix  to  the  size  of  the  inputs,  and  it  adds  it  to  the\n",
      "inputs. Since we added an extra first dimension of size 1 when creating the positional\n",
      "embedding matrix, the rules of broadcasting will ensure that the matrix gets added to\n",
      "every sentence in the inputs: [{'source': './data/ml.pdf', 'start_index': 1209449}]\n",
      "* neurons\n",
      "\n",
      "bias neurons, 285\n",
      "fan-in/fan-out numbers, 333\n",
      "from biological to artificial, 280-295\n",
      "input neurons, 285\n",
      "logical computations with, 283\n",
      "per hidden layer, 324\n",
      "recurrent neurons, 498-502\n",
      "stochastic neurons, 775\n",
      "\n",
      "Newton's difference quotient, 766\n",
      "next sentence prediction (NSP), 565\n",
      "No Free Lunch (NFL) theorem, 33\n",
      "noisy data, 19\n",
      "non-max suppression, 486\n",
      "nonlinear dimensionality reduction (NLDR),\n",
      "\n",
      "230\n",
      "\n",
      "nonlinear SVM classification, 157-162\n",
      "nonparametric models, 181\n",
      "nonsaturating activation functions, 335\n",
      "nonsequential neural networks, 308\n",
      "Normal Equation, 114\n",
      "normalization, 69, 339, 603\n",
      "normalized exponential, 148\n",
      "novelty detection, 12, 267, 274\n",
      "NP-Complete problem, 180\n",
      "null hypothesis, 182\n",
      "NumPy\n",
      "\n",
      "array_split() function, 226\n",
      "dense arrays, 67\n",
      "installing, 42\n",
      "inv() function, 115\n",
      "memmap class, 226\n",
      "randint() function, 107\n",
      "serializing large arrays, 75\n",
      "svd() function, 221\n",
      "using TensorFlow like, 379-384\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "811\n",
      "\n",
      "\fNVIDIA Collective Communications Library\n",
      "\n",
      "limiting risk of, 457 [{'source': './data/ml.pdf', 'start_index': 1752723}]\n",
      "* Now that we have built a character-level model, it’s time to look at word-level models\n",
      "and tackle a common natural language processing task: sentiment analysis. In the pro‐\n",
      "cess we will learn how to handle sequences of variable lengths using masking.\n",
      "\n",
      "Sentiment Analysis\n",
      "If MNIST is the “hello world” of computer vision, then the IMDb reviews dataset is\n",
      "the “hello world” of natural language processing: it consists of 50,000 movie reviews\n",
      "in English (25,000 for training, 25,000 for testing) extracted from the famous Internet\n",
      "Movie Database, along with a simple binary target for each review indicating whether\n",
      "it is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular\n",
      "for good reasons: it is simple enough to be tackled on a laptop in a reasonable amount\n",
      "\n",
      "534 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fof  time,  but  challenging  enough  to  be  fun  and  rewarding.  Keras  provides  a  simple\n",
      "function to load it: [{'source': './data/ml.pdf', 'start_index': 1156310}]\n",
      "* With all this, you can get good translations for fairly short sentences (especially if you\n",
      "use  pretrained  word  embeddings).  Unfortunately,  this  model  will  be  really  bad  at\n",
      "translating  long  sentences.  Once  again,  the  problem  comes  from  the  limited  short-\n",
      "term memory of RNNs. Attention mechanisms are the game-changing innovation that\n",
      "addressed this problem.\n",
      "\n",
      "548 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fAttention Mechanisms\n",
      "Consider the path from the word “milk” to its translation “lait” in Figure 16-3: it is\n",
      "quite  long!  This  means  that  a  representation  of  this  word  (along  with  all  the  other\n",
      "words) needs to be carried over many steps before it is actually used. Can’t we make\n",
      "this path shorter? [{'source': './data/ml.pdf', 'start_index': 1190947}]\n",
      "* Part II.  Neural Networks and Deep Learning\n",
      "\n",
      "10. [{'source': './data/ml.pdf', 'start_index': 22634}]\n",
      "* First,  the  current  input  vector  x(t)  and  the  previous  short-term  state  h(t–1)  are  fed  to\n",
      "four different fully connected layers. They all serve a different purpose:\n",
      "\n",
      "• The main layer is the one that outputs g(t). It has the usual role of analyzing the\n",
      "current inputs x(t) and the previous (short-term) state h(t–1). In a basic cell, there is\n",
      "nothing other than this layer, and its output goes straight out to y(t) and h(t). In\n",
      "contrast, in an LSTM cell this layer’s output does not go straight out, but instead\n",
      "its  most  important  parts  are  stored  in  the  long-term  state  (and  the  rest  is\n",
      "dropped).\n",
      "\n",
      "• The  three  other  layers  are  gate  controllers.  Since  they  use  the  logistic  activation\n",
      "function, their outputs range from 0 to 1. As you can see, their outputs are fed to\n",
      "\n",
      "516 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs [{'source': './data/ml.pdf', 'start_index': 1116170}]\n",
      "* After training the model for a few epochs, its validation accuracy should reach about\n",
      "75–80%  and  stop  making  much  progress.  This  means  that  the  top  layers  are  now\n",
      "pretty  well  trained,  so  we  are  ready  to  unfreeze  all  the  layers  (or  you  could  try\n",
      "unfreezing  just  the  top  ones)  and  continue  training  (don’t  forget  to  compile  the\n",
      "model when you freeze or unfreeze layers). This time we use a much lower learning\n",
      "rate to avoid damaging the pretrained weights:\n",
      "\n",
      "for layer in base_model.layers:\n",
      "    layer.trainable = True\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
      "model.compile(...)\n",
      "history = model.fit(...)\n",
      "\n",
      "It will take a while, but this model should reach around 95% accuracy on the test set.\n",
      "With that, you can start training amazing image classifiers! But there’s more to com‐\n",
      "puter vision than just classification. For example, what if you also want to know where\n",
      "the flower is in the picture? Let’s look at this now. [{'source': './data/ml.pdf', 'start_index': 1040012}]\n",
      "* Preface \n",
      "\n",
      "| \n",
      "\n",
      "xxv\n",
      "\n",
      "\f\fPART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "\n",
      "\f\fCHAPTER 1\n",
      "The Machine Learning Landscape\n",
      "\n",
      "When most people hear “Machine Learning,” they picture a robot: a dependable but‐\n",
      "ler or a deadly Terminator, depending on who you ask. But Machine Learning is not\n",
      "just  a  futuristic  fantasy;  it’s  already  here.  In  fact,  it  has  been  around  for  decades  in\n",
      "some specialized applications, such as Optical Character Recognition (OCR). But the\n",
      "first ML application that really became mainstream, improving the lives of hundreds\n",
      "of millions of people, took over the world back in the 1990s: the spam filter. It’s not\n",
      "exactly a self-aware Skynet, but it does technically qualify as Machine Learning (it has\n",
      "actually learned so well that you seldom need to flag an email as spam anymore). It\n",
      "was  followed  by  hundreds  of  ML  applications  that  now  quietly  power  hundreds  of\n",
      "products and features that you use regularly, from better recommendations to voice\n",
      "search. [{'source': './data/ml.pdf', 'start_index': 73948}]\n",
      "* • Reducing  the  dimensionality  of  the  training  data  to  fight  the  “curse  of  dimen‐\n",
      "\n",
      "sionality”\n",
      "\n",
      "• Other  unsupervised  learning  techniques,  including  clustering,  density  estima‐\n",
      "\n",
      "tion, and anomaly detection\n",
      "\n",
      "Part II, Neural Networks and Deep Learning, covers the following topics:\n",
      "\n",
      "• What neural nets are and what they’re good for\n",
      "\n",
      "• Building and training neural nets using TensorFlow and Keras\n",
      "\n",
      "• The most important neural net architectures: feedforward neural nets for tabular\n",
      "data, convolutional nets for computer vision, recurrent nets and long short-term\n",
      "memory  (LSTM)  nets  for  sequence  processing,  encoder/decoders  and  Trans‐\n",
      "formers for natural language processing, autoencoders and generative adversarial\n",
      "networks (GANs) for generative learning\n",
      "\n",
      "• Techniques for training deep neural nets\n",
      "\n",
      "• How  to  build  an  agent  (e.g.,  a  bot  in  a  game)  that  can  learn  good  strategies\n",
      "\n",
      "through trial and error, using Reinforcement Learning [{'source': './data/ml.pdf', 'start_index': 56265}]\n",
      "* • If  you  need  a  low-latency  model  (one  that  performs  lightning-fast  predictions),\n",
      "you  may  need  to  use  fewer  layers,  fold  the  Batch  Normalization  layers  into  the\n",
      "previous layers, and possibly use a faster activation function such as leaky ReLU\n",
      "or  just  ReLU.  Having  a  sparse  model  will  also  help.  Finally,  you  may  want  to\n",
      "reduce  the  float  precision  from  32  bits  to  16  or  even  8  bits  (see  “Deploying  a\n",
      "Model  to  a  Mobile  or  Embedded  Device”  on  page  685).  Again,  check  out  TF-\n",
      "MOT.\n",
      "\n",
      "• If  you  are  building  a  risk-sensitive  application,  or  inference  latency  is  not  very\n",
      "important  in  your  application,  you  can  use  MC  Dropout  to  boost  performance\n",
      "and get more reliable probability estimates, along with uncertainty estimates. [{'source': './data/ml.pdf', 'start_index': 802359}]\n",
      "* 594 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
      "\n",
      "\fdef train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):\n",
      "    generator, discriminator = gan.layers\n",
      "    for epoch in range(n_epochs):\n",
      "        for X_batch in dataset:\n",
      "            # phase 1 - training the discriminator\n",
      "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
      "            generated_images = generator(noise)\n",
      "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
      "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
      "            discriminator.trainable = True\n",
      "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
      "            # phase 2 - training the generator\n",
      "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
      "            y2 = tf.constant([[1.]] * batch_size)\n",
      "            discriminator.trainable = False\n",
      "            gan.train_on_batch(noise, y2) [{'source': './data/ml.pdf', 'start_index': 1286160}]\n",
      "* Launch, Monitor, and Maintain Your System \n",
      "\n",
      "| \n",
      "\n",
      "81\n",
      "\n",
      "\fEven a model trained to classify pictures of cats and dogs may need\n",
      "to  be  retrained  regularly,  not  because  cats  and  dogs  will  mutate\n",
      "overnight,  but  because  cameras  keep  changing,  along  with  image\n",
      "formats,  sharpness,  brightness,  and  size  ratios.  Moreover,  people\n",
      "may  love  different  breeds  next  year,  or  they  may  decide  to  dress\n",
      "their pets with tiny hats—who knows? [{'source': './data/ml.pdf', 'start_index': 234265}]\n",
      "* 567\n",
      "\n",
      "\fAutoencoders  and  GANs  are  both  unsupervised,  they  both  learn  dense  representa‐\n",
      "tions, they can both be used as generative models, and they have many similar appli‐\n",
      "cations. However, they work very differently:\n",
      "\n",
      "• Autoencoders simply learn to copy their inputs to their outputs. This may sound\n",
      "like a trivial task, but we will see that constraining the network in various ways\n",
      "can make it rather difficult. For example, you can limit the size of the latent rep‐\n",
      "resentations, or you can add noise to the inputs and train the network to recover\n",
      "the  original  inputs.  These  constraints  prevent  the  autoencoder  from  trivially\n",
      "copying the inputs directly to the outputs, which forces it to learn efficient ways\n",
      "of representing the data. In short, the codings are byproducts of the autoencoder\n",
      "learning the identity function under some constraints. [{'source': './data/ml.pdf', 'start_index': 1231503}]\n",
      "* Serving a TensorFlow Model \n",
      "\n",
      "| \n",
      "\n",
      "677\n",
      "\n",
      "\fFigure 19-3. Google Cloud Platform console\n",
      "\n",
      "3. If you have used GCP before and your free trial has expired, then the services you\n",
      "will  use  in  this  chapter  will  cost  you  some  money.  It  should  not  be  too  much,\n",
      "especially if you remember to turn off the services when you do not need them\n",
      "anymore. Make sure you understand and agree to the pricing conditions before\n",
      "you run any service. I hereby decline any responsibility if services end up costing\n",
      "more than you expected! Also make sure your billing account is active. To check,\n",
      "open the navigation menu on the left and click Billing, and make sure you have\n",
      "set up a payment method and that the billing account is active. [{'source': './data/ml.pdf', 'start_index': 1467609}]\n",
      "* Encoding Categorical Features Using Embeddings\n",
      "An  embedding  is  a  trainable  dense  vector  that  represents  a  category.  By  default,\n",
      "embeddings are initialized randomly, so for example the \"NEAR BAY\" category could\n",
      "be represented initially by a random vector such as [0.131, 0.890], while the \"NEAR\n",
      "OCEAN\"  category  might  be  represented  by  another  random  vector  such  as  [0.631,\n",
      "0.791]. In this example, we use 2D embeddings, but the number of dimensions is a\n",
      "hyperparameter you can tweak. Since these embeddings are trainable, they will grad‐\n",
      "ually improve during training; and as they represent fairly similar categories, Gradi‐\n",
      "ent Descent will certainly end up pushing them closer together, while it will tend to\n",
      "move them away from the \"INLAND\" category’s embedding (see Figure 13-4). Indeed,\n",
      "the  better  the  representation,  the  easier  it  will  be  for  the  neural  network  to  make\n",
      "accurate predictions, so training tends to make embeddings useful representations of [{'source': './data/ml.pdf', 'start_index': 938415}]\n",
      "* However,  having  a  good  understanding  of  how  things  work  can  help  you  quickly\n",
      "home in on the appropriate model, the right training algorithm to use, and a good set\n",
      "of hyperparameters for your task. Understanding what’s under the hood will also help\n",
      "you debug issues and perform error analysis more efficiently. Lastly, most of the top‐\n",
      "ics discussed in this chapter will be essential in understanding, building, and training\n",
      "neural networks (discussed in Part II of this book).\n",
      "\n",
      "In  this  chapter  we  will  start  by  looking  at  the  Linear  Regression  model,  one  of  the\n",
      "simplest models there is. We will discuss two very different ways to train it:\n",
      "\n",
      "• Using a direct “closed-form” equation that directly computes the model parame‐\n",
      "ters  that  best  fit  the  model  to  the  training  set  (i.e.,  the  model  parameters  that\n",
      "minimize the cost function over the training set). [{'source': './data/ml.pdf', 'start_index': 289251}]\n",
      "* 6. The main innovations in AlexNet compared to LeNet-5 are that it is much larger\n",
      "and  deeper,  and  it  stacks  convolutional  layers  directly  on  top  of  each  other,\n",
      "instead of stacking a pooling layer on top of each convolutional layer. The main\n",
      "innovation in GoogLeNet is the introduction of inception modules, which make it\n",
      "possible to have a much deeper net than previous CNN architectures, with fewer\n",
      "parameters.  ResNet’s  main  innovation  is  the  introduction  of  skip  connections,\n",
      "which make it possible to go well beyond 100 layers. Arguably, its simplicity and\n",
      "consistency  are  also  rather  innovative.  SENet’s  main  innovation  was  the  idea  of\n",
      "using an SE block (a two-layer dense network) after every inception module in\n",
      "an inception network or every residual unit in a ResNet to recalibrate the relative\n",
      "importance  of  feature  maps.  Finally,  Xception’s  main  innovation  was  the  use  of [{'source': './data/ml.pdf', 'start_index': 1619365}]\n",
      "* In this chapter we will first look at the fundamental concepts underlying RNNs and\n",
      "how  to  train  them  using  backpropagation  through  time,  then  we  will  use  them  to\n",
      "forecast  a  time  series.  After  that  we’ll  explore  the  two  main  difficulties  that  RNNs\n",
      "face:\n",
      "\n",
      "• Unstable gradients (discussed in Chapter 11), which can be alleviated using vari‐\n",
      "ous techniques, including recurrent dropout and recurrent layer normalization\n",
      "\n",
      "• A  (very)  limited  short-term  memory,  which  can  be  extended  using  LSTM  and\n",
      "\n",
      "GRU cells\n",
      "\n",
      "RNNs are not the only types of neural networks capable of handling sequential data:\n",
      "for  small  sequences,  a  regular  dense  network  can  do  the  trick;  and  for  very  long\n",
      "sequences, such as audio samples or text, convolutional neural networks can actually\n",
      "\n",
      "497 [{'source': './data/ml.pdf', 'start_index': 1073535}]\n",
      "* The output looks like this:\n",
      "\n",
      "Image #0\n",
      "  n03877845 - palace       42.87%\n",
      "  n02825657 - bell_cote    40.57%\n",
      "  n03781244 - monastery    14.56%\n",
      "\n",
      "23 In the ImageNet dataset, each image is associated to a word in the WordNet dataset: the class ID is just a\n",
      "\n",
      "WordNet ID.\n",
      "\n",
      "480 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fImage #1\n",
      "  n04522168 - vase         46.83%\n",
      "  n07930864 - cup          7.78%\n",
      "  n11939491 - daisy        4.87%\n",
      "\n",
      "The  correct  classes  (monastery  and  daisy)  appear  in  the  top  three  results  for  both\n",
      "images.  That’s  pretty  good,  considering  that  the  model  had  to  choose  from  among\n",
      "1,000 classes. [{'source': './data/ml.pdf', 'start_index': 1033534}]\n",
      "* TensorFlow  2.0  (beta)  was  released  in  June  2019,  making  Tensor‐\n",
      "Flow much easier to use. The first edition of this book used TF 1,\n",
      "while this edition uses TF 2.\n",
      "\n",
      "Custom Models and Training with TensorFlow \n",
      "\n",
      "| \n",
      "\n",
      "375 [{'source': './data/ml.pdf', 'start_index': 807233}]\n",
      "* applications with noise), 255\n",
      "\n",
      "decision boundaries, 145\n",
      "decision function, 93\n",
      "Decision Stumps, 203\n",
      "Decision Trees\n",
      "\n",
      "benefits of, 175\n",
      "CART training algorithm, 179\n",
      "computational complexity, 180\n",
      "estimating class probabilities, 178\n",
      "evaluating, 73\n",
      "\n",
      "Gini impurity versus entropy, 180\n",
      "instability drawbacks, 185\n",
      "making predictions, 176\n",
      "regression tasks, 183\n",
      "regularization hyperparameters, 181\n",
      "training and visualizing, 175\n",
      "\n",
      "decoders, 501, 569\n",
      "decompression, 224\n",
      "deep autoencoders, 572\n",
      "deep belief networks (DBNs), 13, 777\n",
      "deep computer vision (see Convolutional Neu‐\n",
      "\n",
      "ral Networks (CNNs))\n",
      "deep convolutional GANs, 598\n",
      "Deep Learning VM Images, 692\n",
      "deep neural networks (DNNs)\n",
      "\n",
      "avoiding overfitting, 364-371\n",
      "default configuration, 371\n",
      "defined, xv, 289\n",
      "faster optimizers, 351-364\n",
      "overview of, 331\n",
      "reusing pretrained layers, 345-351\n",
      "vanishing/exploding gradients problems,\n",
      "\n",
      "332-345\n",
      "\n",
      "Deep Neuroevolution, 323\n",
      "Deep Q-Learning [{'source': './data/ml.pdf', 'start_index': 1737503}]\n",
      "* The Architecture of the Visual Cortex\n",
      "David  H.  Hubel  and  Torsten  Wiesel  performed  a  series  of  experiments  on  cats  in\n",
      "19581 and 19592 (and a few years later on monkeys3), giving crucial insights into the\n",
      "structure of the visual cortex (the authors received the Nobel Prize in Physiology or\n",
      "Medicine  in  1981  for  their  work).  In  particular,  they  showed  that  many  neurons  in\n",
      "the visual cortex have a small local receptive field, meaning they react only to visual\n",
      "stimuli  located  in  a  limited  region  of  the  visual  field  (see  Figure  14-1,  in  which  the\n",
      "local receptive fields of five neurons are represented by dashed circles). The receptive\n",
      "fields of different neurons may overlap, and together they tile the whole visual field. [{'source': './data/ml.pdf', 'start_index': 966811}]\n",
      "* Preface \n",
      "\n",
      "| \n",
      "\n",
      "xxiii\n",
      "\n",
      "\fFrançois’s book Deep Learning with Python (Manning): it has the conciseness, clarity,\n",
      "and  depth  of  the  Keras  library  itself.  Special  thanks  as  well  to  Ankur  Patel,  who\n",
      "reviewed every chapter of this second edition and gave me excellent feedback, in par‐\n",
      "ticular on Chapter 9, which covers unsupervised learning techniques. He could write\n",
      "a whole book on the topic… oh, wait, he did! Do check out Hands-On Unsupervised\n",
      "Learning Using Python: How to Build Applied Machine Learning Solutions from Unla‐\n",
      "beled Data (O’Reilly). Huge thanks as well to Olzhas Akpambetov, who reviewed all\n",
      "the  chapters  in  the  second  part  of  the  book,  tested  much  of  the  code,  and  offered\n",
      "many  great  suggestions.  I’m  grateful  to  Mark  Daoust,  Jon  Krohn,  Dominic  Monn,\n",
      "and  Josh  Patterson  for  reviewing  the  second  part  of  this  book  so  thoroughly  and\n",
      "offering their expertise. They left no stone unturned and provided amazingly useful\n",
      "feedback. [{'source': './data/ml.pdf', 'start_index': 68161}]\n",
      "* outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
      "\n",
      "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
      "plt.show()\n",
      "\n",
      "Let’s go through this code:\n",
      "\n",
      "• The pixel intensity for each color channel is represented as a byte from 0 to 255,\n",
      "so we scale these features simply by dividing by 255, to get floats ranging from 0\n",
      "to 1.\n",
      "\n",
      "• Then we create two 7 × 7 filters (one with a vertical white line in the middle, and\n",
      "\n",
      "the other with a horizontal white line in the middle).\n",
      "\n",
      "• We apply them to both images using the tf.nn.conv2d() function, which is part\n",
      "of TensorFlow’s low-level Deep Learning API. In this example, we use zero pad‐\n",
      "ding (padding=\"SAME\") and a stride of 1.\n",
      "\n",
      "• Finally, we plot one of the resulting feature maps (similar to the top-right image\n",
      "\n",
      "in Figure 14-5).\n",
      "\n",
      "The tf.nn.conv2d() line deserves a bit more explanation: [{'source': './data/ml.pdf', 'start_index': 980766}]\n",
      "* Evaluating Actions: The Credit Assignment Problem\n",
      "If we knew what the best action was at each step, we could train the neural network as\n",
      "usual,  by  minimizing  the  cross  entropy  between  the  estimated  probability  distribu‐\n",
      "tion and the target probability distribution. It would just be regular supervised learn‐\n",
      "ing. However, in Reinforcement Learning the only guidance the agent gets is through\n",
      "rewards, and rewards are typically sparse and delayed. For example, if the agent man‐\n",
      "ages  to  balance  the  pole  for  100  steps,  how  can  it  know  which  of  the  100  actions  it\n",
      "took were good, and which of them were bad? All it knows is that the pole fell after\n",
      "the last action, but surely this last action is not entirely responsible. This is called the\n",
      "credit  assignment  problem:  when  the  agent  gets  a  reward,  it  is  hard  for  it  to  know\n",
      "which actions should get credited (or blamed) for it. Think of a dog that gets rewar‐ [{'source': './data/ml.pdf', 'start_index': 1335400}]\n",
      "* 520 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\ftime steps in the targets (since the kernel’s size is 4, the first output of the convolu‐\n",
      "tional layer will be based on the input time steps 0 to 3), and downsample the targets\n",
      "by a factor of 2:\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
      "                        input_shape=[None, 1]),\n",
      "    keras.layers.GRU(20, return_sequences=True),\n",
      "    keras.layers.GRU(20, return_sequences=True),\n",
      "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
      "])\n",
      "\n",
      "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
      "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
      "                    validation_data=(X_valid, Y_valid[:, 3::2])) [{'source': './data/ml.pdf', 'start_index': 1124688}]\n",
      "* The Transformer architecture is represented in Figure 16-8.\n",
      "\n",
      "20 Ashish Vaswani et al., “Attention Is All You Need,” Proceedings of the 31st International Conference on Neural\n",
      "\n",
      "Information Processing Systems (2017): 6000–6010.\n",
      "\n",
      "21 Since the Transformer uses time-distributed Dense layers, you could argue that it uses 1D convolutional layers\n",
      "\n",
      "with a kernel size of 1.\n",
      "\n",
      "554 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fFigure 16-8. The Transformer architecture22\n",
      "\n",
      "Let’s walk through this figure:\n",
      "\n",
      "• The lefthand part is the encoder. Just like earlier, it takes as input a batch of sen‐\n",
      "tences represented as sequences of word IDs (the input shape is [batch size, max\n",
      "input sentence length]), and it encodes each word into a 512-dimensional repre‐\n",
      "sentation (so the encoder’s output shape is [batch size, max input sentence length,\n",
      "512]).  Note  that  the  top  part  of  the  encoder  is  stacked  N  times  (in  the  paper,\n",
      "N = 6). [{'source': './data/ml.pdf', 'start_index': 1202452}]\n",
      "* In the next chapter, we will look at how to efficiently load and preprocess data with\n",
      "TensorFlow.\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. How would you describe TensorFlow in a short sentence? What are its main fea‐\n",
      "\n",
      "tures? Can you name other popular Deep Learning libraries?\n",
      "\n",
      "2. Is TensorFlow a drop-in replacement for NumPy? What are the main differences\n",
      "\n",
      "between the two?\n",
      "\n",
      "3. Do  you  get  the  same  result  with  tf.range(10)  and  tf.constant(np.ara\n",
      "\n",
      "nge(10))?\n",
      "\n",
      "4. Can you name six other data structures available in TensorFlow, beyond regular\n",
      "\n",
      "tensors?\n",
      "\n",
      "410 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\f5. A custom loss function can be defined by writing a function or by subclassing the\n",
      "\n",
      "keras.losses.Loss class. When would you use each option?\n",
      "\n",
      "6. Similarly,  a  custom  metric  can  be  defined  in  a  function  or  a  subclass  of\n",
      "\n",
      "keras.metrics.Metric. When would you use each option?\n",
      "\n",
      "7. When should you create a custom layer versus a custom model? [{'source': './data/ml.pdf', 'start_index': 887566}]\n",
      "* 3. The fact that an autoencoder perfectly reconstructs its inputs does not necessarily\n",
      "mean that it is a good autoencoder; perhaps it is simply an overcomplete autoen‐\n",
      "coder that learned to copy its inputs to the codings layer and then to the outputs.\n",
      "In fact, even if the codings layer contained a single neuron, it would be possible\n",
      "for a very deep autoencoder to learn to map each training instance to a different\n",
      "coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the\n",
      "third to 0.003, and so on), and it could learn “by heart” to reconstruct the right\n",
      "training  instance  for  each  coding.  It  would  perfectly  reconstruct  its  inputs\n",
      "\n",
      "746 \n",
      "\n",
      "|  Appendix A: Exercise Solutions [{'source': './data/ml.pdf', 'start_index': 1635421}]\n",
      "* • Techniques for training deep neural nets\n",
      "\n",
      "• How  to  build  an  agent  (e.g.,  a  bot  in  a  game)  that  can  learn  good  strategies\n",
      "\n",
      "through trial and error, using Reinforcement Learning\n",
      "\n",
      "• Loading and preprocessing large amounts of data efficiently\n",
      "\n",
      "• Training and deploying TensorFlow models at scale\n",
      "\n",
      "The first part is based mostly on Scikit-Learn, while the second part uses TensorFlow\n",
      "and Keras.\n",
      "\n",
      "Don’t jump into deep waters too hastily: while Deep Learning is no\n",
      "doubt  one  of  the  most  exciting  areas  in  Machine  Learning,  you\n",
      "should  master  the  fundamentals  first.  Moreover,  most  problems\n",
      "can be solved quite well using simpler techniques such as Random\n",
      "Forests and Ensemble methods (discussed in Part I). Deep Learn‐\n",
      "ing is best suited for complex problems such as image recognition,\n",
      "speech  recognition,  or  natural  language  processing,  provided  you\n",
      "have enough data, computing power, and patience.\n",
      "\n",
      "xviii \n",
      "\n",
      "|  Preface [{'source': './data/ml.pdf', 'start_index': 57042}]\n",
      "* Classification MLPs                                                                                                  294\n",
      "Implementing MLPs with Keras                                                                                295\n",
      "Installing TensorFlow 2                                                                                           296\n",
      "Building an Image Classifier Using the Sequential API                                      297\n",
      "Building a Regression MLP Using the Sequential API                                        307\n",
      "Building Complex Models Using the Functional API                                         308\n",
      "Using the Subclassing API to Build Dynamic Models                                        313 [{'source': './data/ml.pdf', 'start_index': 23474}]\n",
      "* 544 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\f2015  by  Sébastien  Jean  et  al..11  In  TensorFlow  you  can  use  the  tf.nn.sam\n",
      "pled_softmax_loss() function for this during training and use the normal soft‐\n",
      "max  function  at  inference  time  (sampled  softmax  cannot  be  used  at  inference\n",
      "time because it requires knowing the target).\n",
      "\n",
      "The TensorFlow Addons project includes many sequence-to-sequence tools to let you\n",
      "easily  build  production-ready  Encoder–Decoders.  For  example,  the  following  code\n",
      "creates  a  basic  Encoder–Decoder  model,  similar  to  the  one  represented  in\n",
      "Figure 16-3:\n",
      "\n",
      "import tensorflow_addons as tfa\n",
      "\n",
      "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
      "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
      "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32) [{'source': './data/ml.pdf', 'start_index': 1181411}]\n",
      "* Yann LeCun’s website features great demos of LeNet-5 classifying digits.\n",
      "\n",
      "AlexNet\n",
      "The  AlexNet  CNN  architecture11  won  the  2012  ImageNet  ILSVRC  challenge  by  a\n",
      "large margin: it achieved a top-five error rate of 17%, while the second best achieved\n",
      "only 26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\n",
      "Geoffrey Hinton. It is similar to LeNet-5, only much larger and deeper, and it was the\n",
      "first to stack convolutional layers directly on top of one another, instead of stacking a\n",
      "pooling layer on top of each convolutional layer. Table 14-2 presents this architecture.\n",
      "\n",
      "Table 14-2. AlexNet architecture\n",
      "\n",
      "Layer\n",
      "Out\n",
      "\n",
      "Type\n",
      "Fully connected –\n",
      "\n",
      "Maps\n",
      "\n",
      "F10\n",
      "\n",
      "Fully connected –\n",
      "\n",
      "Fully connected –\n",
      "\n",
      "Max pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Convolution\n",
      "\n",
      "Convolution\n",
      "\n",
      "Max pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Max pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "256\n",
      "\n",
      "256\n",
      "\n",
      "384\n",
      "\n",
      "384\n",
      "\n",
      "256\n",
      "\n",
      "256\n",
      "\n",
      "96\n",
      "\n",
      "96\n",
      "\n",
      "F9\n",
      "\n",
      "S8\n",
      "\n",
      "C7\n",
      "\n",
      "C6\n",
      "\n",
      "C5\n",
      "\n",
      "S4\n",
      "\n",
      "C3\n",
      "\n",
      "S2\n",
      "\n",
      "C1\n",
      "\n",
      "In\n",
      "\n",
      "Size\n",
      "1,000\n",
      "\n",
      "4,096\n",
      "\n",
      "4,096\n",
      "\n",
      "6 × 6\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "27 × 27\n",
      "\n",
      "27 × 27 [{'source': './data/ml.pdf', 'start_index': 1002070}]\n",
      "* Finally, all the scores go through a softmax layer to get a final weight for each encoder\n",
      "output (e.g., α(3,2)). All the weights for a given decoder time step add up to 1 (since the\n",
      "softmax layer is not time-distributed). This particular attention mechanism is called\n",
      "Bahdanau  attention  (named  after  the  paper’s  first  author).  Since  it  concatenates  the\n",
      "encoder output with the decoder’s previous hidden state, it is sometimes called con‐\n",
      "catenative attention (or additive attention). [{'source': './data/ml.pdf', 'start_index': 1195056}]\n",
      "* Figure 19-15. Splitting a fully connected neural network\n",
      "\n",
      "Some  neural  network  architectures,  such  as  convolutional  neural  networks  (see\n",
      "Chapter 14), contain layers that are only partially connected to the lower layers, so it\n",
      "is much easier to distribute chunks across devices in an efficient way (Figure 19-16).\n",
      "\n",
      "702 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
      "\n",
      "\fFigure 19-16. Splitting a partially connected neural network [{'source': './data/ml.pdf', 'start_index': 1522200}]\n",
      "* It’s surprising at first that this destructive technique works at all. Would a company\n",
      "perform  better  if  its  employees  were  told  to  toss  a  coin  every  morning  to  decide\n",
      "whether  or  not  to  go  to  work?  Well,  who  knows;  perhaps  it  would!  The  company\n",
      "would  be  forced  to  adapt  its  organization;  it  could  not  rely  on  any  single  person  to\n",
      "work the coffee machine or perform any other critical tasks, so this expertise would\n",
      "have to be spread across several people. Employees would have to learn to cooperate\n",
      "with  many  of  their  coworkers,  not  just  a  handful  of  them.  The  company  would\n",
      "become much more resilient. If one person quit, it wouldn’t make much of a differ‐\n",
      "ence. It’s unclear whether this idea would actually work for companies, but it certainly\n",
      "does for neural networks. Neurons trained with dropout cannot co-adapt with their\n",
      "neighboring  neurons;  they  have  to  be  as  useful  as  possible  on  their  own.  They  also [{'source': './data/ml.pdf', 'start_index': 786628}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "735\n",
      "\n",
      "\ferror-prone, and it will be harder to reuse the custom code you write. However,\n",
      "in  some  cases  writing  a  custom  training  loop  is  necessary—for  example,  if  you\n",
      "want to use different optimizers for different parts of your neural network, like in\n",
      "the Wide & Deep paper. A custom training loop can also be useful when debug‐\n",
      "ging, or when trying to understand exactly how training works.\n",
      "\n",
      "9. Custom Keras components should be convertible to TF Functions, which means\n",
      "they should stick to TF operations as much as possible and respect all the rules\n",
      "listed in “TF Function Rules” on page 409. If you absolutely need to include arbi‐\n",
      "trary  Python  code  in  a  custom  component,  you  can  either  wrap  it  in  a\n",
      "tf.py_function()  operation  (but  this  will  reduce  performance  and  limit  your\n",
      "model’s  portability)  or  set  dynamic=True  when  creating  the  custom  layer  or\n",
      "model (or set run_eagerly=True when calling the model’s compile() method). [{'source': './data/ml.pdf', 'start_index': 1603310}]\n",
      "* Figure 7-6. MNIST pixel importance (according to a Random Forest classifier)\n",
      "\n",
      "Random  Forests  are  very  handy  to  get  a  quick  understanding  of  what  features\n",
      "actually matter, in particular if you need to perform feature selection.\n",
      "\n",
      "Boosting\n",
      "Boosting  (originally  called  hypothesis  boosting)  refers  to  any  Ensemble  method  that\n",
      "can  combine  several  weak  learners  into  a  strong  learner.  The  general  idea  of  most\n",
      "boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
      "cessor. There are many boosting methods available, but by far the most popular are\n",
      "\n",
      "Boosting \n",
      "\n",
      "| \n",
      "\n",
      "199\n",
      "\n",
      "\fAdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let’s start with Ada‐\n",
      "Boost. [{'source': './data/ml.pdf', 'start_index': 441137}]\n",
      "* 394 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fFigure 12-3. Custom model example: an arbitrary model with a custom ResidualBlock\n",
      "layer containing a skip connection\n",
      "\n",
      "The inputs go through a first dense layer, then through a residual block composed of\n",
      "two dense layers and an addition operation (as we will see in Chapter 14, a residual\n",
      "block adds its inputs to its outputs), then through this same residual block three more\n",
      "times, then through a second residual block, and the final result goes through a dense\n",
      "output layer. Note that this model does not make much sense; it’s just an example to\n",
      "illustrate the fact that you can easily build any kind of model you want, even one that\n",
      "contains loops and skip connections. To implement this model, it is best to first create\n",
      "a ResidualBlock layer, since we are going to create a couple of identical blocks (and\n",
      "we might want to reuse it in another model): [{'source': './data/ml.pdf', 'start_index': 851045}]\n",
      "* Index \n",
      "\n",
      "| \n",
      "\n",
      "805\n",
      "\n",
      "\fLLE (Locally Linear Embedding), 230\n",
      "overview of, 213\n",
      "PCA (Principal Component Analysis),\n",
      "\n",
      "219-230\n",
      "discount factors, 619\n",
      "discriminators, 568\n",
      "Distribution Strategies API, 668, 709\n",
      "dot product, 551\n",
      "Double DQN, 640\n",
      "Double Dueling DQN, 642\n",
      "DQN agents, 652\n",
      "dropout, 365\n",
      "dual numbers, 768\n",
      "dual problem, 168, 761\n",
      "duck typing, 68\n",
      "Dueling DQN algorithm, 641\n",
      "dummy attributes, 67\n",
      "dying ReLUs problem, 335\n",
      "dynamic models, 313\n",
      "dynamic placer algorithm, 697\n",
      "Dynamic Programming, 628\n",
      "\n",
      "E\n",
      "eager execution/eager mode, 408\n",
      "early stopping, 141\n",
      "Elastic Net, 140\n",
      "ELU (exponential linear unit), 336-338\n",
      "embedded devices, 685\n",
      "Embedded Reber grammars, 566\n",
      "embedding, 68, 413, 433\n",
      "embedding matrix, 435\n",
      "encoders, 501, 569\n",
      "Encoder–Decoder model, 501, 542-548\n",
      "end-of-sequence (EoS) token, 542, 556\n",
      "energy function, 774\n",
      "Ensemble Learning [{'source': './data/ml.pdf', 'start_index': 1738978}]\n",
      "* • The scaling factor scales down the similarity scores to avoid saturating the soft‐\n",
      "\n",
      "max function, which would lead to tiny gradients.\n",
      "\n",
      "• It  is  possible  to  mask  out  some  key/value  pairs  by  adding  a  very  large  negative\n",
      "value to the corresponding similarity scores, just before computing the softmax.\n",
      "This is useful in the Masked Multi-Head Attention layer. [{'source': './data/ml.pdf', 'start_index': 1214511}]\n",
      "* An Encoder–Decoder Network for Neural Machine Translation \n",
      "\n",
      "| \n",
      "\n",
      "543\n",
      "\n",
      "\fFigure 16-4. Feeding the previous output word as input at inference time\n",
      "\n",
      "OK, now you have the big picture. Still, there are a few more details to handle if you\n",
      "implement this model: [{'source': './data/ml.pdf', 'start_index': 1179286}]\n",
      "* 500 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\fFigure 15-3. A cell’s hidden state and its output may be different\n",
      "\n",
      "Input and Output Sequences\n",
      "An  RNN  can  simultaneously  take  a  sequence  of  inputs  and  produce  a  sequence  of\n",
      "outputs  (see  the  top-left  network  in  Figure  15-4).  This  type  of  sequence-to-sequence\n",
      "network is useful for predicting time series such as stock prices: you feed it the prices\n",
      "over the last N days, and it must output the prices shifted by one day into the future\n",
      "(i.e., from N – 1 days ago to tomorrow).\n",
      "\n",
      "Alternatively, you could feed the network a sequence of inputs and ignore all outputs\n",
      "except for the last one (see the top-right network in Figure 15-4). In other words, this\n",
      "is a sequence-to-vector network. For example, you could feed the network a sequence\n",
      "of  words  corresponding  to  a  movie  review,  and  the  network  would  output  a  senti‐\n",
      "ment score (e.g., from –1 [hate] to +1 [love]). [{'source': './data/ml.pdf', 'start_index': 1080303}]\n",
      "* Fighting the Unstable Gradients Problem\n",
      "Many of the tricks we used in deep nets to alleviate the unstable gradients problem\n",
      "can also be used for RNNs: good parameter initialization, faster optimizers, dropout,\n",
      "and so on. However, nonsaturating activation functions (e.g., ReLU) may not help as\n",
      "much here; in fact, they may actually lead the RNN to be even more unstable during\n",
      "training.  Why?  Well,  suppose  Gradient  Descent  updates  the  weights  in  a  way  that\n",
      "increases the outputs slightly at the first time step. Because the same weights are used\n",
      "at every time step, the outputs at the second time step may also be slightly increased,\n",
      "and those at the third, and so on until the outputs explode—and a nonsaturating acti‐\n",
      "vation  function  does  not  prevent  that.  You  can  reduce  this  risk  by  using  a  smaller\n",
      "learning  rate,  but  you  can  also  simply  use  a  saturating  activation  function  like  the [{'source': './data/ml.pdf', 'start_index': 1104447}]\n",
      "* Figure 14-8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\n",
      "\n",
      "A pooling layer typically works on every input channel independ‐\n",
      "ently, so the output depth is the same as the input depth.\n",
      "\n",
      "Other than reducing computations, memory usage, and the number of parameters, a\n",
      "max  pooling  layer  also  introduces  some  level  of  invariance  to  small  translations,  as\n",
      "shown in Figure 14-9. Here we assume that the bright pixels have a lower value than\n",
      "dark  pixels,  and  we  consider  three  images  (A,  B,  C)  going  through  a  max  pooling\n",
      "layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but\n",
      "\n",
      "9 Other kernels we’ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding\n",
      "\n",
      "windows.\n",
      "\n",
      "Pooling Layers \n",
      "\n",
      "| \n",
      "\n",
      "457 [{'source': './data/ml.pdf', 'start_index': 988322}]\n",
      "* depth_pool = keras.layers.Lambda(\n",
      "    lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\n",
      "                             padding=\"valid\"))\n",
      "\n",
      "One last type of pooling layer that you will often see in modern architectures is the\n",
      "global average pooling layer. It works very differently: all it does is compute the mean\n",
      "of  each  entire  feature  map  (it’s  like  an  average  pooling  layer  using  a  pooling  kernel\n",
      "with the same spatial dimensions as the inputs). This means that it just outputs a sin‐\n",
      "gle  number  per  feature  map  and  per  instance.  Although  this  is  of  course  extremely\n",
      "destructive (most of the information in the feature map is lost), it can be useful as the\n",
      "output layer, as we will see later in this chapter. To create such a layer, simply use the\n",
      "keras.layers.GlobalAvgPool2D class:\n",
      "\n",
      "global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
      "\n",
      "It’s equivalent to this simple Lambda layer, which computes the mean over the spatial\n",
      "dimensions (height and width): [{'source': './data/ml.pdf', 'start_index': 993605}]\n",
      "* SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
      "\n",
      "5. What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g.,\n",
      "\n",
      "0.99999) when using an SGD optimizer?\n",
      "\n",
      "6. Name three ways you can produce a sparse model.\n",
      "\n",
      "7. Does  dropout  slow  down  training?  Does  it  slow  down  inference  (i.e.,  making\n",
      "\n",
      "predictions on new instances)? What about MC Dropout?\n",
      "\n",
      "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
      "\n",
      "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but\n",
      "it’s  the  point  of  this  exercise).  Use  He  initialization  and  the  ELU  activation\n",
      "function. [{'source': './data/ml.pdf', 'start_index': 803983}]\n",
      "* You might wonder why we didn’t plot the loss. It turns out that loss is a poor indicator\n",
      "of  the  model’s  performance.  The  loss  might  go  down,  yet  the  agent  might  perform\n",
      "worse (e.g., this can happen when the agent gets stuck in one small region of the envi‐\n",
      "ronment, and the DQN starts overfitting this region). Conversely, the loss could go\n",
      "up, yet the agent might perform better (e.g., if the DQN was underestimating the Q-\n",
      "Values, and it starts correctly increasing its predictions, the agent will likely perform\n",
      "better, getting more rewards, but the loss might increase because the DQN also sets\n",
      "the targets, which will be larger too).\n",
      "\n",
      "The basic Deep Q-Learning algorithm we’ve been using so far would be too unstable\n",
      "to  learn  to  play  Atari  games.  So  how  did  DeepMind  do  it?  Well,  they  tweaked  the\n",
      "algorithm!\n",
      "\n",
      "13 A great 2018 post by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.\n",
      "\n",
      "638 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 18: Reinforcement Learning [{'source': './data/ml.pdf', 'start_index': 1380098}]\n",
      "* Encoding Categorical Features Using One-Hot Vectors                                     431\n",
      "Encoding Categorical Features Using Embeddings                                             433\n",
      "Keras Preprocessing Layers                                                                                     437\n",
      "TF Transform                                                                                                                439\n",
      "The TensorFlow Datasets (TFDS) Project                                                                441\n",
      "Exercises                                                                                                                        442 [{'source': './data/ml.pdf', 'start_index': 32173}]\n",
      "* 16. Natural Language Processing with RNNs and Attention. . . . . . . . . . . . . . . . . . . . . . . .   525\n",
      "Generating Shakespearean Text Using a Character RNN                                      526\n",
      "Creating the Training Dataset                                                                                 527\n",
      "How to Split a Sequential Dataset                                                                          527\n",
      "Chopping the Sequential Dataset into Multiple Windows                                 528\n",
      "\n",
      "x \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents [{'source': './data/ml.pdf', 'start_index': 37621}]\n",
      "* For example, you can slightly shift, rotate, and resize every picture in the training set\n",
      "by  various  amounts  and  add  the  resulting  pictures  to  the  training  set  (see\n",
      "Figure 14-12). This forces the model to be more tolerant to variations in the position,\n",
      "orientation, and size of the objects in the pictures. For a model that’s more tolerant of\n",
      "different  lighting  conditions,  you  can  similarly  generate  many  images  with  various\n",
      "contrasts. In general, you can also flip the pictures horizontally (except for text, and\n",
      "other  asymmetrical  objects).  By  combining  these  transformations,  you  can  greatly\n",
      "increase the size of your training set.\n",
      "\n",
      "Figure 14-12. Generating new training instances from existing ones [{'source': './data/ml.pdf', 'start_index': 1004570}]\n",
      "* An Encoder–Decoder Network for Neural Machine Translation \n",
      "\n",
      "| \n",
      "\n",
      "547\n",
      "\n",
      "\fthe  word  “are,”  and  so  on.  Assuming  the  vocabulary  has  10,000  words,  each  model\n",
      "will output 10,000 probabilities. [{'source': './data/ml.pdf', 'start_index': 1188551}]\n",
      "* 19 Reproduced with the kind authorization of the authors.\n",
      "\n",
      "Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "605\n",
      "\n",
      "\fable  to  flow  through  the  network  and  reach  the  final  layers  of  the  generator:  this\n",
      "seems  like  an  unnecessary  constraint  that  probably  slowed  down  training.  And\n",
      "finally, some visual artifacts may appear because the same noise was used at different\n",
      "levels.  If  instead  the  generator  tried  to  produce  its  own  pseudorandom  noise,  this\n",
      "noise might not look very convincing, leading to more visual artifacts. Plus, part of\n",
      "the generator’s weights would be dedicated to generating pseudorandom noise, which\n",
      "again  seems  wasteful.  By  adding  extra  noise  inputs,  all  these  issues  are  avoided;  the\n",
      "GAN is able to use the provided noise to add the right amount of stochasticity to each\n",
      "part of the image. [{'source': './data/ml.pdf', 'start_index': 1309962}]\n",
      "* from tensorflow.keras.layers import Dense\n",
      "output_layer = Dense(10)\n",
      "\n",
      "Or simply use full paths, if you prefer:\n",
      "\n",
      "from tensorflow import keras\n",
      "output_layer = keras.layers.Dense(10)\n",
      "\n",
      "This approach is more verbose, but I use it in this book so you can easily see which\n",
      "packages to use, and to avoid confusion between standard classes and custom classes.\n",
      "In production code, I prefer the previous approach. Many people also use from ten\n",
      "sorflow.keras import layers followed by layers.Dense(10).\n",
      "\n",
      "The model’s summary() method displays all the model’s layers,14 including each layer’s\n",
      "name (which is automatically generated unless you set it when creating the layer), its\n",
      "output shape (None means the batch size can be anything), and its number of parame‐\n",
      "ters. The summary ends with the total number of parameters, including trainable and\n",
      "non-trainable parameters. Here we only have trainable parameters (we will see exam‐\n",
      "ples of non-trainable parameters in Chapter 11): [{'source': './data/ml.pdf', 'start_index': 629928}]\n",
      "* class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
      "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
      "\n",
      "For example, the first image in the training set represents a coat:\n",
      "\n",
      ">>> class_names[y_train[0]]\n",
      "'Coat'\n",
      "\n",
      "Figure 10-11 shows some samples from the Fashion MNIST dataset.\n",
      "\n",
      "Figure 10-11. Samples from Fashion MNIST\n",
      "\n",
      "298 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fCreating the model using the Sequential API\n",
      "\n",
      "Now  let’s  build  the  neural  network!  Here  is  a  classification  MLP  with  two  hidden\n",
      "layers:\n",
      "\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
      "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
      "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
      "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
      "\n",
      "Let’s go through this code line by line: [{'source': './data/ml.pdf', 'start_index': 626707}]\n",
      "* The Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "341\n",
      "\n",
      "\fmodel = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(10, activation=\"softmax\")\n",
      "])\n",
      "\n",
      "That’s  all!  In  this  tiny  example  with  just  two  hidden  layers,  it’s  unlikely  that  Batch\n",
      "Normalization will have a very positive impact; but for deeper networks it can make a\n",
      "tremendous difference.\n",
      "\n",
      "Let’s display the model summary: [{'source': './data/ml.pdf', 'start_index': 728826}]\n",
      "* Sentiment Analysis \n",
      "\n",
      "| \n",
      "\n",
      "541\n",
      "\n",
      "\fwill get the documentation for this module. By default, TF Hub will cache the down‐\n",
      "loaded files into the local system’s temporary directory. You may prefer to download\n",
      "them into a more permanent directory to avoid having to download them again after\n",
      "every system cleanup. To do that, set the TFHUB_CACHE_DIR environment variable to\n",
      "the  directory  of  your  choice  (e.g.,  os.environ[\"TFHUB_CACHE_DIR\"]  =  \"./\n",
      "my_tfhub_cache\").\n",
      "\n",
      "So far, we have looked at time series, text generation using Char-RNN, and sentiment\n",
      "analysis using word-level RNN models, training our own word embeddings or reus‐\n",
      "ing  pretrained  embeddings.  Let’s  now  look  at  another  important  NLP  task:  neural\n",
      "machine translation (NMT), first using a pure Encoder–Decoder model, then improv‐\n",
      "ing it with attention mechanisms, and finally looking the extraordinary Transformer\n",
      "architecture. [{'source': './data/ml.pdf', 'start_index': 1176008}]\n",
      "* The output layer of the original model should usually be replaced because it is most\n",
      "likely not useful at all for the new task, and it may not even have the right number of\n",
      "outputs for the new task.\n",
      "\n",
      "Similarly, the upper hidden layers of the original model are less likely to be as useful\n",
      "as the lower layers, since the high-level features that are most useful for the new task\n",
      "may differ significantly from the ones that were most useful for the original task. You\n",
      "want to find the right number of layers to reuse.\n",
      "\n",
      "346 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fThe more similar the tasks are, the more layers you want to reuse\n",
      "(starting with the lower layers). For very similar tasks, try keeping\n",
      "all the hidden layers and just replacing the output layer. [{'source': './data/ml.pdf', 'start_index': 740822}]\n",
      "* For example, let’s use the nnlm-en-dim50 sentence embedding module, version 1, in\n",
      "our sentiment analysis model:\n",
      "\n",
      "import tensorflow_hub as hub\n",
      "\n",
      "model = keras.Sequential([\n",
      "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
      "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
      "    keras.layers.Dense(128, activation=\"relu\"),\n",
      "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
      "])\n",
      "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
      "              metrics=[\"accuracy\"]) [{'source': './data/ml.pdf', 'start_index': 1173749}]\n",
      "* 458 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fmax_pool = keras.layers.MaxPool2D(pool_size=2)\n",
      "\n",
      "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you\n",
      "might expect, it works exactly like a max pooling layer, except it computes the mean\n",
      "rather  than  the  max.  Average  pooling  layers  used  to  be  very  popular,  but  people\n",
      "mostly use max pooling layers now, as they generally perform better. This may seem\n",
      "surprising, since computing the mean generally loses less information than comput‐\n",
      "ing  the  max.  But  on  the  other  hand,  max  pooling  preserves  only  the  strongest  fea‐\n",
      "tures, getting rid of all the meaningless ones, so the next layers get a cleaner signal to\n",
      "work with. Moreover, max pooling offers stronger translation invariance than average\n",
      "pooling, and it requires slightly less compute. [{'source': './data/ml.pdf', 'start_index': 991075}]\n",
      "* 14 The complete WaveNet uses a few more tricks, such as skip connections like in a ResNet, and Gated Activation\n",
      "\n",
      "Units similar to those found in a GRU cell. Please see the notebook for more details.\n",
      "\n",
      "522 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "\fsimilar  pairs  of  layers  using  growing  dilation  rates:  1,  2,  4,  8,  and  again  1,  2,  4,  8.\n",
      "Finally,  we  add  the  output  layer:  a  convolutional  layer  with  10  filters  of  size  1  and\n",
      "without  any  activation  function.  Thanks  to  the  padding  layers,  every  convolutional\n",
      "layer outputs a sequence of the same length as the input sequences, so the targets we\n",
      "use during training can be the full sequences: no need to crop them or downsample\n",
      "them. [{'source': './data/ml.pdf', 'start_index': 1128467}]\n",
      "* d. What  happens  when  neural  nets  are  too  small.  Remove  one  neuron  to  keep\n",
      "just  two.  Notice  that  the  neural  network  is  now  incapable  of  finding  a  good\n",
      "solution,  even  if  you  try  multiple  times.  The  model  has  too  few  parameters\n",
      "and systematically underfits the training set.\n",
      "\n",
      "e. What happens when neural nets are large enough. Set the number of neurons\n",
      "to eight, and train the network several times. Notice that it is now consistently\n",
      "fast and never gets stuck. This highlights an important finding in neural net‐\n",
      "work  theory:  large  neural  networks  almost  never  get  stuck  in  local  minima,\n",
      "and  even  when  they  do  these  local  optima  are  almost  as  good  as  the  global\n",
      "optimum. However, they can still get stuck on long plateaus for a long time. [{'source': './data/ml.pdf', 'start_index': 697861}]\n",
      "* Equation 16-2. Sine/cosine positional embeddings\n",
      "\n",
      "Pp, 2i = sin p/100002i/d\n",
      "Pp, 2i + 1 = cos p/100002i/d\n",
      "\n",
      "Figure 16-9. Sine/cosine positional embedding matrix (transposed, top) with a focus on\n",
      "two values of i (bottom)\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "557 [{'source': './data/ml.pdf', 'start_index': 1208008}]\n",
      "* Where  does  Machine  Learning  start  and  where  does  it  end?  What  exactly  does  it\n",
      "mean for a machine to learn something? If I download a copy of Wikipedia, has my\n",
      "computer  really  learned  something?  Is  it  suddenly  smarter?  In  this  chapter  we  will\n",
      "start by clarifying what Machine Learning is and why you may want to use it.\n",
      "\n",
      "Then,  before  we  set  out  to  explore  the  Machine  Learning  continent,  we  will  take  a\n",
      "look at the map and learn about the main regions and the most notable landmarks:\n",
      "supervised  versus  unsupervised  learning,  online  versus  batch  learning,  instance-\n",
      "based versus model-based learning. Then we will look at the workflow of a typical ML\n",
      "project,  discuss  the  main  challenges  you  may  face,  and  cover  how  to  evaluate  and\n",
      "fine-tune a Machine Learning system. [{'source': './data/ml.pdf', 'start_index': 74938}]\n",
      "* Classification,” Proceedings of the 2015 IEEE International Conference on Computer Vision (2015): 1026–1034.\n",
      "\n",
      "334 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fhe_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
      "                                                 distribution='uniform')\n",
      "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init) [{'source': './data/ml.pdf', 'start_index': 711488}]\n",
      "* • Finally, we plot one of the resulting feature maps (similar to the top-right image\n",
      "\n",
      "in Figure 14-5).\n",
      "\n",
      "The tf.nn.conv2d() line deserves a bit more explanation:\n",
      "\n",
      "• images is the input mini-batch (a 4D tensor, as explained earlier).\n",
      "• filters is the set of filters to apply (also a 4D tensor, as explained earlier).\n",
      "• strides is equal to 1, but it could also be a 1D array with four elements, where\n",
      "the  two  central  elements  are  the  vertical  and  horizontal  strides  (sh  and  sw).  The\n",
      "first and last elements must currently be equal to 1. They may one day be used to\n",
      "specify a batch stride (to skip some instances) and a channel stride (to skip some\n",
      "of the previous layer’s feature maps or channels).\n",
      "\n",
      "• padding must be either \"SAME\" or \"VALID\": [{'source': './data/ml.pdf', 'start_index': 981474}]\n",
      "* Now let’s use tf.keras! We’ll start by building a simple image classifier.\n",
      "\n",
      "Building an Image Classifier Using the Sequential API\n",
      "First, we need to load a dataset. In this chapter we will tackle Fashion MNIST, which\n",
      "is a drop-in replacement of MNIST (introduced in Chapter 3). It has the exact same\n",
      "format  as  MNIST  (70,000  grayscale  images  of  28  ×  28  pixels  each,  with  10  classes),\n",
      "but the images represent fashion items rather than handwritten digits, so each class is\n",
      "more  diverse,  and  the  problem  turns  out  to  be  significantly  more  challenging  than\n",
      "MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST,\n",
      "but only about 83% on Fashion MNIST.\n",
      "\n",
      "Using Keras to load the dataset\n",
      "\n",
      "Keras provides some utility functions to fetch and load common datasets, including\n",
      "MNIST,  Fashion  MNIST,  and  the  California  housing  dataset  we  used  in  Chapter  2.\n",
      "Let’s load Fashion MNIST:\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "297 [{'source': './data/ml.pdf', 'start_index': 624382}]\n",
      "* useful information from the inputs (e.g., a layer with two neurons can only output 2D\n",
      "data, so if it processes 3D data, some information will be lost). No matter how big and\n",
      "powerful the rest of the network is, that information will never be recovered. [{'source': './data/ml.pdf', 'start_index': 690449}]\n",
      "* 486 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fover that same rose overlaps a lot with the max bounding box, so we will get rid\n",
      "of it.\n",
      "\n",
      "3. Repeat step two until there are no more bounding boxes to get rid of.\n",
      "\n",
      "This  simple  approach  to  object  detection  works  pretty  well,  but  it  requires  running\n",
      "the  CNN  many  times,  so  it  is  quite  slow.  Fortunately,  there  is  a  much  faster  way  to\n",
      "slide a CNN across an image: using a fully convolutional network (FCN). [{'source': './data/ml.pdf', 'start_index': 1047901}]\n",
      "* Solutions to these exercises are available in Appendix A.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "329\n",
      "\n",
      "\f\fCHAPTER 11\n",
      "Training Deep Neural Networks\n",
      "\n",
      "In  Chapter  10  we  introduced  artificial  neural  networks  and  trained  our  first  deep\n",
      "neural networks. But they were shallow nets, with just a few hidden layers. What if\n",
      "you need to tackle a complex problem, such as detecting hundreds of types of objects\n",
      "in high-resolution images? You may need to train a much deeper DNN, perhaps with\n",
      "10 layers or many more, each containing hundreds of neurons, linked by hundreds of\n",
      "thousands  of  connections.  Training  a  deep  DNN  isn’t  a  walk  in  the  park.  Here  are\n",
      "some of the problems you could run into: [{'source': './data/ml.pdf', 'start_index': 702136}]\n",
      "* 15. Processing Sequences Using RNNs and CNNs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [{'source': './data/ml.pdf', 'start_index': 35985}]\n",
      "* fact that it is in the past tense. In short, the word representation encodes many differ‐\n",
      "ent characteristics of the word. If we just used a single Scaled Dot-Product Attention\n",
      "layer, we would only be able to query all of these characteristics in one shot. This is\n",
      "why the Multi-Head Attention layer applies multiple different linear transformations\n",
      "of the values, keys, and queries: this allows the model to apply many different projec‐\n",
      "tions of the word representation into different subspaces, each focusing on a subset of\n",
      "the word’s characteristics. Perhaps one of the linear layers will project the word repre‐\n",
      "sentation into a subspace where all that remains is the information that the word is a [{'source': './data/ml.pdf', 'start_index': 1218825}]\n",
      "* Activation function\n",
      "\n",
      "We  discussed  how  to  choose  the  activation  function  earlier  in  this  chapter:  in\n",
      "general, the ReLU activation function will be a good default for all hidden layers.\n",
      "For the output layer, it really depends on your task.\n",
      "\n",
      "24 Dominic Masters and Carlo Luschi, “Revisiting Small Batch Training for Deep Neural Networks,” arXiv pre‐\n",
      "\n",
      "print arXiv:1804.07612 (2018).\n",
      "\n",
      "25 Elad Hoffer et al., “Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training\n",
      "\n",
      "of Neural Networks,” Proceedings of the 31st International Conference on Neural Information Processing Systems\n",
      "(2017): 1729–1739.\n",
      "\n",
      "26 Priya Goyal et al., “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour,” arXiv preprint arXiv:\n",
      "\n",
      "1706.02677 (2017).\n",
      "\n",
      "326 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fNumber of iterations\n",
      "\n",
      "In  most  cases,  the  number  of  training  iterations  does  not  actually  need  to  be\n",
      "tweaked: just use early stopping instead. [{'source': './data/ml.pdf', 'start_index': 694134}]\n",
      "* MNIST\n",
      "In  this  chapter  we  will  be  using  the  MNIST  dataset,  which  is  a  set  of  70,000  small\n",
      "images of digits handwritten by high school students and employees of the US Cen‐\n",
      "sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\n",
      "ied so much that it is often called the “hello world” of Machine Learning: whenever\n",
      "people come up with a new classification algorithm they are curious to see how it will\n",
      "perform  on  MNIST,  and  anyone  who  learns  Machine  Learning  tackles  this  dataset\n",
      "sooner or later.\n",
      "\n",
      "Scikit-Learn provides many helper functions to download popular datasets. MNIST is\n",
      "one of them. The following code fetches the MNIST dataset:1\n",
      "\n",
      ">>> from sklearn.datasets import fetch_openml\n",
      ">>> mnist = fetch_openml('mnist_784', version=1)\n",
      ">>> mnist.keys()\n",
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',\n",
      "           'categories', 'url']) [{'source': './data/ml.pdf', 'start_index': 241972}]\n",
      "* In  contrast,  faces  generated  by  generative  adversarial  networks  (GANs)  are  now  so\n",
      "convincing that it is hard to believe that the people they represent do not exist. You\n",
      "can judge so for yourself by visiting https://thispersondoesnotexist.com/, a website that\n",
      "shows  faces  generated  by  a  recent  GAN  architecture  called  StyleGAN  (you  can  also\n",
      "check  out  https://thisrentaldoesnotexist.com/  to  see  some  generated  Airbnb  bed‐\n",
      "rooms). GANs are now widely used for super resolution (increasing the resolution of\n",
      "an image), colorization, powerful image editing (e.g., replacing photo bombers with\n",
      "realistic background), turning a simple sketch into a photorealistic image, predicting\n",
      "the next frames in a video, augmenting a dataset (to train other models), generating\n",
      "other types of data (such as text, audio, and time series), identifying the weaknesses in\n",
      "other models and strengthening them, and more.\n",
      "\n",
      "567 [{'source': './data/ml.pdf', 'start_index': 1230574}]\n",
      "* With these techniques, you can alleviate the unstable gradients problem and train an\n",
      "RNN much more efficiently. Now let’s look at how to deal with the short-term mem‐\n",
      "ory problem.\n",
      "\n",
      "Tackling the Short-Term Memory Problem\n",
      "Due  to  the  transformations  that  the  data  goes  through  when  traversing  an  RNN,\n",
      "some information is lost at each time step. After a while, the RNN’s state contains vir‐\n",
      "tually no trace of the first inputs. This can be a showstopper. Imagine Dory the fish6\n",
      "trying  to  translate  a  long  sentence;  by  the  time  she’s  finished  reading  it,  she  has  no\n",
      "clue  how  it  started.  To  tackle  this  problem,  various  types  of  cells  with  long-term\n",
      "memory have been introduced. They have proven so successful that the basic cells are\n",
      "not used much anymore. Let’s first look at the most popular of these long-term mem‐\n",
      "ory cells: the LSTM cell.\n",
      "\n",
      "LSTM cells [{'source': './data/ml.pdf', 'start_index': 1111998}]\n",
      "* we pass it the output of the first hidden layer.\n",
      "\n",
      "18 The name input_ is used to avoid overshadowing Python’s built-in input() function.\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "309\n",
      "\n",
      "\f• Next, we create a Concatenate layer, and once again we immediately use it like a\n",
      "function, to concatenate the input and the output of the second hidden layer. You\n",
      "may  prefer  the  keras.layers.concatenate()  function,  which  creates  a\n",
      "Concatenate layer and immediately calls it with the given inputs.\n",
      "\n",
      "• Then we create the output layer, with a single neuron and no activation function,\n",
      "\n",
      "and we call it like a function, passing it the result of the concatenation.\n",
      "• Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
      "\n",
      "Once you have built the Keras model, everything is exactly like earlier, so there’s no\n",
      "need to repeat it here: you must compile the model, train it, evaluate it, and use it to\n",
      "make predictions. [{'source': './data/ml.pdf', 'start_index': 651793}]\n",
      "* layer and returns its output.\n",
      "\n",
      "Similarly, you can add a custom metric based on model internals by computing it in\n",
      "any way you want, as long as the result is the output of a metric object. For example,\n",
      "you  can  create  a  keras.metrics.Mean  object  in  the  constructor,  then  call  it  in  the\n",
      "call() method, passing it the recon_loss, and finally add it to the model by calling\n",
      "the  model’s  add_metric()  method.  This  way,  when  you  train  the  model,  Keras  will\n",
      "display both the mean loss over each epoch (the loss is the sum of the main loss plus\n",
      "0.05  times  the  reconstruction  loss)  and  the  mean  reconstruction  error  over  each\n",
      "epoch. Both will go down during training:\n",
      "\n",
      "Epoch 1/5\n",
      "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
      "Epoch 2/5\n",
      "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
      "[...] [{'source': './data/ml.pdf', 'start_index': 858790}]\n",
      "* The signal flows only in one direction (from the inputs to the out‐\n",
      "puts), so this architecture is an example of a feedforward neural net‐\n",
      "work (FNN).\n",
      "\n",
      "When an ANN contains a deep stack of hidden layers,9 it is called a deep neural net‐\n",
      "work (DNN). The field of Deep Learning studies DNNs, and more generally models\n",
      "containing  deep  stacks  of  computations.  Even  so,  many  people  talk  about  Deep\n",
      "Learning whenever neural networks are involved (even shallow ones).\n",
      "\n",
      "For  many  years  researchers  struggled  to  find  a  way  to  train  MLPs,  without  success.\n",
      "But  in  1986,  David  Rumelhart,  Geoffrey  Hinton,  and  Ronald  Williams  published  a\n",
      "\n",
      "9 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\n",
      "\n",
      "ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "289 [{'source': './data/ml.pdf', 'start_index': 605204}]\n",
      "* 530 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\flayer.  We  can  then  compile  this  model,  using  the  \"sparse_categorical_crossen\n",
      "tropy\" loss and an Adam optimizer. Finally, we are ready to train the model for sev‐\n",
      "eral epochs (this may take many hours, depending on your hardware):\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
      "                     dropout=0.2, recurrent_dropout=0.2),\n",
      "    keras.layers.GRU(128, return_sequences=True,\n",
      "                     dropout=0.2, recurrent_dropout=0.2),\n",
      "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
      "                                                    activation=\"softmax\"))\n",
      "])\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
      "history = model.fit(dataset, epochs=20) [{'source': './data/ml.pdf', 'start_index': 1147257}]\n",
      "* The only slightly tricky part in this code is the loop that adds the ResidualUnit layers\n",
      "to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\n",
      "have 128 filters, and so on. We then set the stride to 1 when the number of filters is\n",
      "the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,\n",
      "and finally we update prev_filters.\n",
      "\n",
      "It is amazing that in fewer than 40 lines of code, we can build the model that won the\n",
      "ILSVRC  2015  challenge!  This  demonstrates  both  the  elegance  of  the  ResNet  model\n",
      "and the expressiveness of the Keras API. Implementing the other CNN architectures\n",
      "is not much harder. However, Keras comes with several of these architectures built in,\n",
      "so why not use them instead? [{'source': './data/ml.pdf', 'start_index': 1030223}]\n",
      "* Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "593\n",
      "\n",
      "\fdiscriminator = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.Dense(150, activation=\"selu\"),\n",
      "    keras.layers.Dense(100, activation=\"selu\"),\n",
      "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
      "])\n",
      "gan = keras.models.Sequential([generator, discriminator])\n",
      "\n",
      "Next, we need to compile these models. As the discriminator is a binary classifier, we\n",
      "can  naturally  use  the  binary  cross-entropy  loss.  The  generator  will  only  be  trained\n",
      "through the gan model, so we do not need to compile it at all. The gan model is also a\n",
      "binary classifier, so it can use the binary cross-entropy loss. Importantly, the discrimi‐\n",
      "nator  should  not  be  trained  during  the  second  phase,  so  we  make  it  non-trainable\n",
      "before compiling the gan model:\n",
      "\n",
      "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
      "discriminator.trainable = False\n",
      "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\") [{'source': './data/ml.pdf', 'start_index': 1284401}]\n",
      "* about a sequence-to-vector RNN, and a vector-to-sequence RNN?\n",
      "\n",
      "2. How many dimensions must the inputs of an RNN layer have? What does each\n",
      "\n",
      "dimension represent? What about its outputs?\n",
      "\n",
      "3. If  you  want  to  build  a  deep  sequence-to-sequence  RNN,  which  RNN  layers\n",
      "should have return_sequences=True? What about a sequence-to-vector RNN?\n",
      "\n",
      "4. Suppose you have a daily univariate time series, and you want to forecast the next\n",
      "\n",
      "seven days. Which RNN architecture should you use?\n",
      "\n",
      "5. What are the main difficulties when training RNNs? How can you handle them?\n",
      "\n",
      "6. Can you sketch the LSTM cell’s architecture?\n",
      "\n",
      "7. Why would you want to use 1D convolutional layers in an RNN?\n",
      "\n",
      "8. Which neural network architecture could you use to classify videos?\n",
      "\n",
      "9. Train a classification model for the SketchRNN dataset, available in TensorFlow\n",
      "\n",
      "Datasets. [{'source': './data/ml.pdf', 'start_index': 1129998}]\n",
      "* Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a mini-\n",
      "batch\n",
      "\n",
      "Y t = ϕ X t Wx + Y t − 1 Wy + b\n",
      "\n",
      "= ϕ X t Y t − 1 W + b with W =\n",
      "\n",
      "Wx\n",
      "Wy\n",
      "\n",
      "1 Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather\n",
      "\n",
      "than the ReLU activation function. For example, take a look at Vu Pham et al.’s 2013 paper “Dropout Improves\n",
      "Recurrent Neural Networks for Handwriting Recognition”. ReLU-based RNNs are also possible, as shown in\n",
      "Quoc V. Le et al.’s 2015 paper “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”.\n",
      "\n",
      "Recurrent Neurons and Layers \n",
      "\n",
      "| \n",
      "\n",
      "499\n",
      "\n",
      "\fIn this equation:\n",
      "\n",
      "• Y(t) is an m × nneurons matrix containing the layer’s outputs at time step t for each\n",
      "instance in the mini-batch (m is the number of instances in the mini-batch and\n",
      "nneurons is the number of neurons).\n",
      "\n",
      "• X(t)  is  an  m  ×  ninputs  matrix  containing  the  inputs  for  all  instances  (ninputs  is  the\n",
      "\n",
      "number of input features). [{'source': './data/ml.pdf', 'start_index': 1077193}]\n",
      "* Note that the last layer is not ideal: it must have a single unit because we want to fore‐\n",
      "cast a univariate time series, and this means we must have a single output value per\n",
      "time step. However, having a single unit means that the hidden state is just a single\n",
      "number.  That’s  really  not  much,  and  it’s  probably  not  that  useful;  presumably,  the\n",
      "RNN will mostly use the hidden states of the other recurrent layers to carry over all\n",
      "the information it needs from time step to time step, and it will not use the final lay‐\n",
      "er’s hidden state very much. Moreover, since a SimpleRNN layer uses the tanh activa‐\n",
      "tion function by default, the predicted values must lie within the range –1 to 1. But\n",
      "what if you want to use another activation function? For both these reasons, it might\n",
      "be  preferable  to  replace  the  output  layer  with  a  Dense  layer:  it  would  run  slightly\n",
      "\n",
      "Forecasting a Time Series \n",
      "\n",
      "| \n",
      "\n",
      "507 [{'source': './data/ml.pdf', 'start_index': 1093741}]\n",
      "* 13 Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” arXiv pre‐\n",
      "\n",
      "print arXiv:1409.0473 (2014).\n",
      "\n",
      "14 The most common metric used in NMT is the BiLingual Evaluation Understudy (BLEU) score, which com‐\n",
      "pares each translation produced by the model with several good translations produced by humans: it counts\n",
      "the number of n-grams (sequences of n words) that appear in any of the target translations and adjusts the\n",
      "score to take into account the frequency of the produced n-grams in the target translations.\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "549\n",
      "\n",
      "\fFigure 16-6. Neural machine translation using an Encoder–Decoder network with an\n",
      "attention model [{'source': './data/ml.pdf', 'start_index': 1193614}]\n",
      "* Unfortunately,  such  model  parallelism  turns  out  to  be  pretty  tricky,  and  it  really\n",
      "depends  on  the  architecture  of  your  neural  network.  For  fully  connected  networks,\n",
      "there is generally not much to be gained from this approach (see Figure 19-15). Intui‐\n",
      "tively, it may seem that an easy way to split the model is to place each layer on a dif‐\n",
      "ferent device, but this does not work because each layer needs to wait for the output\n",
      "of the previous layer before it can do anything. So perhaps you can slice it vertically—\n",
      "for  example,  with  the  left  half  of  each  layer  on  one  device,  and  the  right  part  on\n",
      "another device? This is slightly better, since both halves of each layer can indeed work\n",
      "in parallel, but the problem is that each half of the next layer requires the output of\n",
      "both halves, so there will be a lot of cross-device communication (represented by the\n",
      "dashed arrows). This is likely to completely cancel out the benefit of the parallel com‐ [{'source': './data/ml.pdf', 'start_index': 1521093}]\n",
      "* However, it’s not always possible to determine the model’s performance without any\n",
      "human analysis. For example, suppose you trained an image classification model (see\n",
      "Chapter 3) to detect several product defects on a production line. How can you get an\n",
      "alert  if  the  model’s  performance  drops,  before  thousands  of  defective  products  get\n",
      "shipped to your clients? One solution is to send to human raters a sample of all the\n",
      "pictures  that  the  model  classified  (especially  pictures  that  the  model  wasn’t  so  sure\n",
      "about).  Depending  on  the  task,  the  raters  may  need  to  be  experts,  or  they  could  be\n",
      "nonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechani‐\n",
      "cal Turk). In some applications they could even be the users themselves, responding\n",
      "for example via surveys or repurposed captchas.24 [{'source': './data/ml.pdf', 'start_index': 235336}]\n",
      "* 9. Build your own CNN from scratch and try to achieve the highest possible accu‐\n",
      "\n",
      "racy on MNIST.\n",
      "\n",
      "10. Use transfer learning for large image classification, going through these steps:\n",
      "\n",
      "a. Create a training set containing at least 100 images per class. For example, you\n",
      "could classify your own pictures based on the location (beach, mountain, city,\n",
      "etc.),  or  alternatively  you  can  use  an  existing  dataset  (e.g.,  from  TensorFlow\n",
      "Datasets).\n",
      "\n",
      "b. Split it into a training set, a validation set, and a test set.\n",
      "\n",
      "c. Build the input pipeline, including the appropriate preprocessing operations,\n",
      "\n",
      "and optionally add data augmentation.\n",
      "\n",
      "d. Fine-tune a pretrained model on this dataset.\n",
      "\n",
      "11. Go  through  TensorFlow’s  Style  Transfer  tutorial.  It  is  a  fun  way  to  generate  art\n",
      "\n",
      "using Deep Learning.\n",
      "\n",
      "Solutions to these exercises are available in Appendix A.\n",
      "\n",
      "496 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks [{'source': './data/ml.pdf', 'start_index': 1071488}]\n",
      "* using Deep Learning.\n",
      "\n",
      "Solutions to these exercises are available in Appendix A.\n",
      "\n",
      "496 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fCHAPTER 15\n",
      "Processing Sequences Using\n",
      "RNNs and CNNs [{'source': './data/ml.pdf', 'start_index': 1072275}]\n",
      "* In this chapter we will look at how to deploy models, first to TF Serving, then to Goo‐\n",
      "gle Cloud AI Platform. We will also take a quick look at deploying models to mobile\n",
      "apps, embedded devices, and web apps. Lastly, we will discuss how to speed up com‐\n",
      "putations  using  GPUs  and  how  to  train  models  across  multiple  devices  and  servers\n",
      "using  the  Distribution  Strategies  API.  That’s  a  lot  of  topics  to  discuss,  so  let’s  get\n",
      "started! [{'source': './data/ml.pdf', 'start_index': 1444409}]\n",
      "* 9 Other kernels we’ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding\n",
      "\n",
      "windows.\n",
      "\n",
      "Pooling Layers \n",
      "\n",
      "| \n",
      "\n",
      "457\n",
      "\n",
      "\fshifted  by  one  and  two  pixels  to  the  right.  As  you  can  see,  the  outputs  of  the  max\n",
      "pooling  layer  for  images  A  and  B  are  identical.  This  is  what  translation  invariance\n",
      "means.  For  image  C,  the  output  is  different:  it  is  shifted  one  pixel  to  the  right  (but\n",
      "there is still 75% invariance). By inserting a max pooling layer every few layers in a\n",
      "CNN, it is possible to get some level of translation invariance at a larger scale. More‐\n",
      "over,  max  pooling  offers  a  small  amount  of  rotational  invariance  and  a  slight  scale\n",
      "invariance. Such invariance (even if it is limited) can be useful in cases where the pre‐\n",
      "diction should not depend on these details, such as in classification tasks.\n",
      "\n",
      "Figure 14-9. Invariance to small translations [{'source': './data/ml.pdf', 'start_index': 988964}]\n",
      "* F\n",
      "F1 score, 92\n",
      "fake quantization, 687\n",
      "false positive rate (FPR), 97\n",
      "fan-in/fan-out numbers, 333\n",
      "Fashion MNIST dataset, 297, 574, 590\n",
      "Fast-MCD (minimum covariance determi‐\n",
      "\n",
      "nant), 274\n",
      "\n",
      "\ffeature engineering, 27\n",
      "feature extraction, 12, 27\n",
      "feature maps, 228, 450\n",
      "feature scaling, 69\n",
      "feature selection, 27\n",
      "feature space, 226\n",
      "feature vector, 113\n",
      "features, 8\n",
      "feedforward neural networks (FNNs), 289\n",
      "filters, 450\n",
      "final trained models, 20\n",
      "finite difference approximation, 766\n",
      "First In, First Out (FIFO) queues, 383\n",
      "first-order partial derivatives (Jacobians), 358\n",
      "fitness functions, 20\n",
      "fixed Q-Value targets, 639\n",
      "flat datasets, 529\n",
      "folds, 73, 89\n",
      "forecasting, 503\n",
      "forget gate, 516\n",
      "forward pass, 290\n",
      "forward-mode autodiff, 767\n",
      "fraud detection, 237\n",
      "Full Gradient Descent, 122\n",
      "fully connected layer, 285\n",
      "fully convolutional networks (FCNs), 487\n",
      "fully-specified model architecture, 20\n",
      "function definitions, 792\n",
      "function graphs, 792\n",
      "Functional API, 308-313 [{'source': './data/ml.pdf', 'start_index': 1741146}]\n",
      "* 752 \n",
      "\n",
      "|  Appendix A: Exercise Solutions\n",
      "\n",
      "\fand a bit harder to deploy, since it requires managing parameter servers. How‐\n",
      "ever, it is useful to train huge models that don’t fit in GPU RAM.\n",
      "\n",
      "For the solutions to exercises 9, 10, and 11, please see the Jupyter notebooks available\n",
      "at https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "753\n",
      "\n",
      "\f\fAPPENDIX B\n",
      "Machine Learning Project Checklist\n",
      "\n",
      "This  checklist  can  guide  you  through  your  Machine  Learning  projects.  There  are\n",
      "eight main steps:\n",
      "\n",
      "1. Frame the problem and look at the big picture.\n",
      "\n",
      "2. Get the data.\n",
      "\n",
      "3. Explore the data to gain insights.\n",
      "\n",
      "4. Prepare the data to better expose the underlying data patterns to Machine Learn‐\n",
      "\n",
      "ing algorithms.\n",
      "\n",
      "5. Explore many different models and shortlist the best ones.\n",
      "\n",
      "6. Fine-tune your models and combine them into a great solution.\n",
      "\n",
      "7. Present your solution.\n",
      "\n",
      "8. Launch, monitor, and maintain your system.\n",
      "\n",
      "Obviously, you should feel free to adapt this checklist to your needs. [{'source': './data/ml.pdf', 'start_index': 1654049}]\n",
      "* Reinforcement Learning (RL) is one of the most exciting fields of Machine Learning\n",
      "today, and also one of the oldest. It has been around since the 1950s, producing many\n",
      "interesting  applications  over  the  years,1  particularly  in  games  (e.g.,  TD-Gammon,  a\n",
      "Backgammon-playing  program)  and  in  machine  control,  but  seldom  making  the\n",
      "headline news. But a revolution took place in 2013, when researchers from a British\n",
      "startup  called  DeepMind  demonstrated  a  system  that  could  learn  to  play  just  about\n",
      "any  Atari  game  from  scratch,2  eventually  outperforming  humans3  in  most  of  them,\n",
      "using only raw pixels as inputs and without any prior knowledge of the rules of the\n",
      "games.4  This  was  the  first  of  a  series  of  amazing  feats,  culminating  in  March  2016\n",
      "with the victory of their system AlphaGo against Lee Sedol, a legendary professional\n",
      "player of the game of Go, and in May 2017 against Ke Jie, the world champion. No [{'source': './data/ml.pdf', 'start_index': 1315112}]\n",
      "* Epoch 1/5\n",
      "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
      "Epoch 2/5\n",
      "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
      "[...]\n",
      "\n",
      "In over 99% of cases, everything we have discussed so far will be sufficient to imple‐\n",
      "ment whatever model you want to build, even with complex architectures, losses, and\n",
      "metrics.  However,  in  some  rare  cases  you  may  need  to  customize  the  training  loop\n",
      "\n",
      "11 You can also call add_loss() on any layer inside the model, as the model recursively gathers losses from all of\n",
      "\n",
      "its layers.\n",
      "\n",
      "398 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fitself. Before we get there, we need to look at how to compute gradients automatically\n",
      "in TensorFlow.\n",
      "\n",
      "Computing Gradients Using Autodiff\n",
      "To understand how to use autodiff (see Chapter 10 and Appendix D) to compute gra‐\n",
      "dients automatically, let’s consider a simple toy function:\n",
      "\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + 2 * w1 * w2 [{'source': './data/ml.pdf', 'start_index': 859484}]\n",
      "* 2. Let’s  compute  how  many  parameters  the  CNN  has.  Since  its  first  convolutional\n",
      "layer has 3 × 3 kernels, and the input has three channels (red, green, and blue),\n",
      "each feature map has 3 × 3 × 3 weights, plus a bias term. That’s 28 parameters per\n",
      "\n",
      "Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "739 [{'source': './data/ml.pdf', 'start_index': 1614725}]\n",
      "* From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "291\n",
      "\n",
      "\fThe Rectified Linear Unit function: ReLU(z) = max(0, z)\n",
      "\n",
      "The  ReLU  function  is  continuous  but  unfortunately  not  differentiable  at  z  =  0\n",
      "(the slope changes abruptly, which can make Gradient Descent bounce around),\n",
      "and its derivative is 0 for z < 0. In practice, however, it works very well and has\n",
      "the  advantage  of  being  fast  to  compute,  so  it  has  become  the  default.12  Most\n",
      "importantly, the fact that it does not have a maximum output value helps reduce\n",
      "some issues during Gradient Descent (we will come back to this in Chapter 11). [{'source': './data/ml.pdf', 'start_index': 611731}]\n",
      "* StyleGANs\n",
      "The state of the art in high-resolution image generation was advanced once again by\n",
      "the same Nvidia team in a 2018 paper18 that introduced the popular StyleGAN archi‐\n",
      "tecture. The authors used style transfer techniques in the generator to ensure that the\n",
      "generated images have the same local structure as the training images, at every scale,\n",
      "greatly improving the quality of the generated images. The discriminator and the loss\n",
      "function were not modified, only the generator. Let’s take a look at the StyleGAN. It is\n",
      "composed of two networks (see Figure 17-20):\n",
      "\n",
      "Mapping network [{'source': './data/ml.pdf', 'start_index': 1307098}]\n",
      "* 4. An  undercomplete  autoencoder  is  one  whose  codings  layer  is  smaller  than  the\n",
      "input  and  output  layers.  If  it  is  larger,  then  it  is  an  overcomplete  autoencoder.\n",
      "The main risk of an excessively undercomplete autoencoder is that it may fail to\n",
      "reconstruct the inputs. The main risk of an overcomplete autoencoder is that it\n",
      "may just copy the inputs to the outputs, without learning any useful features.\n",
      "\n",
      "5. To tie the weights of an encoder layer and its corresponding decoder layer, you\n",
      "simply make the decoder weights equal to the transpose of the encoder weights.\n",
      "This reduces the number of parameters in the model by half, often making train‐\n",
      "ing converge faster with less training data and reducing the risk of overfitting the\n",
      "training set. [{'source': './data/ml.pdf', 'start_index': 1637056}]\n",
      "* Now  let’s  look  at  categorical  features.  We  will  start  by  encoding  them  as  one-hot\n",
      "vectors.\n",
      "\n",
      "Encoding Categorical Features Using One-Hot Vectors\n",
      "Consider the ocean_proximity feature in the California housing dataset we explored\n",
      "in  Chapter  2:  it  is  a  categorical  feature  with  five  possible  values:  \"<1H  OCEAN\",\n",
      "\"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", and \"ISLAND\". We need to encode this feature\n",
      "before we feed it to a neural network. Since there are very few categories, we can use\n",
      "one-hot encoding. For this, we first need to map each category to its index (0 to 4),\n",
      "which can be done using a lookup table:\n",
      "\n",
      "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
      "indices = tf.range(len(vocab), dtype=tf.int64)\n",
      "\n",
      "Preprocessing the Input Features \n",
      "\n",
      "| \n",
      "\n",
      "431\n",
      "\n",
      "\ftable_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
      "num_oov_buckets = 2\n",
      "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
      "\n",
      "Let’s go through this code: [{'source': './data/ml.pdf', 'start_index': 932811}]\n",
      "* We  appreciate,  but  do  not  require,  attribution.  An  attribution  usually  includes  the\n",
      "title,  author,  publisher,  and  ISBN.  For  example:  “Hands-On  Machine  Learning  with\n",
      "Scikit-Learn,  Keras,  and  TensorFlow,  2nd  Edition,  by  Aurélien  Géron  (O’Reilly).\n",
      "Copyright  2019  Kiwisoft  S.A.S.,  978-1-492-03264-9.”  If  you  feel  your  use  of  code\n",
      "examples falls outside fair use or the permission given above, feel free to contact us at\n",
      "permissions@oreilly.com.\n",
      "\n",
      "O’Reilly Online Learning\n",
      "\n",
      "For  almost  40  years,  O’Reilly  Media  has  provided  technology\n",
      "and business training, knowledge, and insight to help compa‐\n",
      "nies succeed. [{'source': './data/ml.pdf', 'start_index': 64805}]\n",
      "* During inference (i.e., when making a prediction for a new instance) the RAM occu‐\n",
      "pied by one layer can be released as soon as the next layer has been computed, so you\n",
      "only need as much RAM as required by two consecutive layers. But during training\n",
      "everything computed during the forward pass needs to be preserved for the reverse\n",
      "pass, so the amount of RAM needed is (at least) the total amount of RAM required by\n",
      "all layers.\n",
      "\n",
      "If training crashes because of an out-of-memory error, you can try\n",
      "reducing  the  mini-batch  size.  Alternatively,  you  can  try  reducing\n",
      "dimensionality using a stride, or removing a few layers. Or you can\n",
      "try using 16-bit floats instead of 32-bit floats. Or you could distrib‐\n",
      "ute the CNN across multiple devices.\n",
      "\n",
      "Now let’s look at the second common building block of CNNs: the pooling layer. [{'source': './data/ml.pdf', 'start_index': 985790}]\n",
      "* test_logdir = get_run_logdir()\n",
      "writer = tf.summary.create_file_writer(test_logdir)\n",
      "with writer.as_default():\n",
      "    for step in range(1, 1000 + 1):\n",
      "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
      "        data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
      "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
      "        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\n",
      "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
      "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
      "        tf.summary.text(\"my_text\", texts, step=step)\n",
      "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
      "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
      "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "319\n",
      "\n",
      "\fThis is actually a useful visualization tool to have, even beyond TensorFlow or Deep\n",
      "Learning. [{'source': './data/ml.pdf', 'start_index': 674309}]\n",
      "* Simply subclass the Model class, create the layers you need in the constructor, and use\n",
      "them to perform the computations you want in the call() method. For example, cre‐\n",
      "ating  an  instance  of  the  following  WideAndDeepModel  class  gives  us  an  equivalent\n",
      "model to the one we just built with the Functional API. You can then compile it, eval‐\n",
      "uate it, and use it to make predictions, exactly like we just did:\n",
      "\n",
      "class WideAndDeepModel(keras.Model):\n",
      "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
      "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
      "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
      "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
      "        self.main_output = keras.layers.Dense(1)\n",
      "        self.aux_output = keras.layers.Dense(1) [{'source': './data/ml.pdf', 'start_index': 659856}]\n",
      "* nets we have considered so far. For example, they can take sentences, documents, or\n",
      "audio samples as input, making them extremely useful for natural language process‐\n",
      "ing applications such as automatic translation or speech-to-text. [{'source': './data/ml.pdf', 'start_index': 1073301}]\n",
      "* Progressive Growing of GANs\n",
      "An important technique was proposed in a 2018 paper16 by Nvidia researchers Tero\n",
      "Karras  et  al.:  they  suggested  generating  small  images  at  the  beginning  of  training,\n",
      "then gradually adding convolutional layers to both the generator and the discrimina‐\n",
      "tor to produce larger and larger images (4 × 4, 8 × 8, 16 × 16, …, 512 × 512, 1,024 ×\n",
      "1,024). This approach resembles greedy layer-wise training of stacked autoencoders.\n",
      "\n",
      "14 Reproduced with the kind authorization of the authors.\n",
      "\n",
      "15 Mehdi Mirza and Simon Osindero, “Conditional Generative Adversarial Nets,” arXiv preprint arXiv:\n",
      "\n",
      "1411.1784 (2014).\n",
      "\n",
      "16 Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability, and Variation,” Proceedings\n",
      "\n",
      "of the International Conference on Learning Representations (2018).\n",
      "\n",
      "Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "601 [{'source': './data/ml.pdf', 'start_index': 1300737}]\n",
      "* To solve this problem, you may want to use a variant of the ReLU function, such as\n",
      "the  leaky  ReLU.  This  function  is  defined  as  LeakyReLUα(z)  =  max(αz,  z)  (see\n",
      "Figure 11-2). The hyperparameter α defines how much the function “leaks”: it is the\n",
      "slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that\n",
      "leaky ReLUs never die; they can go into a long coma, but they have a chance to even‐\n",
      "tually wake up. A 2015 paper5 compared several variants of the ReLU activation func‐\n",
      "tion, and one of its conclusions was that the leaky variants always outperformed the\n",
      "strict ReLU activation function. In fact, setting α = 0.2 (a huge leak) seemed to result\n",
      "in  better  performance  than  α  =  0.01  (a  small  leak).  The  paper  also  evaluated  the\n",
      "randomized leaky ReLU (RReLU), where α is picked randomly in a given range during\n",
      "training and is fixed to an average value during testing. RReLU also performed fairly [{'source': './data/ml.pdf', 'start_index': 713199}]\n",
      "* semantic interpolation, 590\n",
      "semantic segmentation, 249, 458, 492\n",
      "semi-supervised learning\n",
      "\n",
      "clustering algorithms for, 237, 253\n",
      "defined, 13\n",
      "examples of, 13\n",
      "\n",
      "SENet (Squeeze-and-Excitation Network), 476\n",
      "sensitivity, 91\n",
      "sentence encoders, 541\n",
      "sentiment analysis\n",
      "defined, 526\n",
      "masking, 538\n",
      "overview of, 534\n",
      "reusing pretrained embeddings, 540\n",
      "\n",
      "separable convolution, 474\n",
      "sequence-to-sequence models, 510\n",
      "sequence-to-vector networks, 501\n",
      "SequenceExample protobuf (TensorFlow), 429\n",
      "\n",
      "sequences\n",
      "\n",
      "forecasting time series, 503-511\n",
      "handling long, 511-523\n",
      "input and output, 501\n",
      "RNNS for, 497\n",
      "\n",
      "Sequential API\n",
      "\n",
      "image classifiers using, 297-307\n",
      "regression MLP using, 307\n",
      "\n",
      "service account, 682\n",
      "sets, 383, 787\n",
      "Shannon's information theory, 180\n",
      "short-term memory problems, 514-523\n",
      "shortcut connections, 471\n",
      "shrinkage, 205\n",
      "shuffling-buffer approach, 417\n",
      "sigmoid (Logistic) activation function, 143,\n",
      "\n",
      "293-294, 302, 332 [{'source': './data/ml.pdf', 'start_index': 1762026}]\n",
      "* Placing Operations and Variables on Devices                                                      697\n",
      "Parallel Execution Across Multiple Devices                                                         699\n",
      "Training Models Across Multiple Devices                                                                701\n",
      "Model Parallelism                                                                                                     701\n",
      "Data Parallelism                                                                                                        704\n",
      "Training at Scale Using the Distribution Strategies API                                     709\n",
      "Training a Model on a TensorFlow Cluster                                                          711\n",
      "Running Large Training Jobs on Google Cloud AI Platform                             714\n",
      "Black Box Hyperparameter Tuning on AI Platform                                            716 [{'source': './data/ml.pdf', 'start_index': 47423}]\n",
      "* randomized leaky ReLU (RReLU), where α is picked randomly in a given range during\n",
      "training and is fixed to an average value during testing. RReLU also performed fairly\n",
      "well and seemed to act as a regularizer (reducing the risk of overfitting the training\n",
      "set).  Finally,  the  paper  evaluated  the  parametric  leaky  ReLU  (PReLU),  where  α  is\n",
      "authorized  to  be  learned  during  training  (instead  of  being  a  hyperparameter,  it\n",
      "becomes a parameter that can be modified by backpropagation like any other param‐ [{'source': './data/ml.pdf', 'start_index': 713985}]\n",
      "* Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the\n",
      "network by setting include_top=False: this excludes the global average pooling layer\n",
      "and the dense output layer. We then add our own global average pooling layer, based\n",
      "on the output of the base model, followed by a dense output layer with one unit per\n",
      "class, using the softmax activation function. Finally, we create the Keras Model:\n",
      "\n",
      "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
      "                                                  include_top=False)\n",
      "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
      "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
      "model = keras.Model(inputs=base_model.input, outputs=output)\n",
      "\n",
      "482 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fAs explained in Chapter 11, it’s usually a good idea to freeze the weights of the pre‐\n",
      "trained layers, at least at the beginning of training: [{'source': './data/ml.pdf', 'start_index': 1038285}]\n",
      "* These problems have kept researchers very busy since 2014: many papers were pub‐\n",
      "lished on this topic, some proposing new cost functions11 (though a 2018 paper12 by\n",
      "Google researchers questions their efficiency) or techniques to stabilize training or to\n",
      "avoid  the  mode  collapse  issue.  For  example,  a  popular  technique  called  experience\n",
      "replay consists in storing the images produced by the generator at each iteration in a\n",
      "replay buffer (gradually dropping older generated images) and training the discrimi‐\n",
      "nator using real images plus fake images drawn from this buffer (rather than just fake\n",
      "images  produced  by  the  current  generator).  This  reduces  the  chances  that  the  dis‐\n",
      "criminator  will  overfit  the  latest  generator’s  outputs.  Another  common  technique  is\n",
      "called mini-batch discrimination: it measures how similar images are across the batch\n",
      "and provides this statistic to the discriminator, so it can easily reject a whole batch of [{'source': './data/ml.pdf', 'start_index': 1291842}]\n",
      "* Insufficient Quantity of Training Data\n",
      "For a toddler to learn what an apple is, all it takes is for you to point to an apple and\n",
      "say “apple” (possibly repeating this procedure a few times). Now the child is able to\n",
      "recognize apples in all sorts of colors and shapes. Genius.\n",
      "\n",
      "Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\n",
      "ing  algorithms  to  work  properly.  Even  for  very  simple  problems  you  typically  need\n",
      "thousands of examples, and for complex problems such as image or speech recogni‐\n",
      "tion  you  may  need  millions  of  examples  (unless  you  can  reuse  parts  of  an  existing\n",
      "model).\n",
      "\n",
      "Main Challenges of Machine Learning \n",
      "\n",
      "| \n",
      "\n",
      "23 [{'source': './data/ml.pdf', 'start_index': 112336}]\n",
      "==================================================\n",
      "* What Is a Good Explanation?\n",
      "\n",
      "This section further condenses Miller’s summary on “good” explanations and adds concrete\n",
      "implications for interpretable machine learning.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "30 [{'source': './data/xai.pdf', 'start_index': 71027}]\n",
      "* ¹⁷Lipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-266.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "31\n",
      "\n",
      "explanations) usually perform well because averaging over those “stories” makes the predictions\n",
      "more robust and accurate. But it also means that there is more than one selective explanation why\n",
      "a certain prediction was made. What it means for interpretable machine learning:\n",
      "Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex. The\n",
      "LIME method does a good job with this. [{'source': './data/xai.pdf', 'start_index': 74997}]\n",
      "* ¹⁰Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).\n",
      "¹¹Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.”\n",
      "\n",
      "Advances in Neural Information Processing Systems (2016).\n",
      "\n",
      "¹²Doshi-Velez, Finale, and Been Kim.\n",
      "\n",
      "“Towards a rigorous\n",
      "\n",
      "science of\n",
      "\n",
      "interpretable machine\n",
      "\n",
      "learning,” no. Ml: 1–13.\n",
      "\n",
      "http://arxiv.org/abs/1702.08608 ( 2017).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "16 [{'source': './data/xai.pdf', 'start_index': 34924}]\n",
      "* Interpretability\n",
      "\n",
      "29\n",
      "\n",
      "Human-friendly Explanations\n",
      "\n",
      "Let us dig deeper and discover what we humans see as “good” explanations and what the\n",
      "implications are for interpretable machine learning. Humanities research can help us find out. Miller\n",
      "(2017) has conducted a huge survey of publications on explanations, and this chapter builds on his\n",
      "summary.\n",
      "\n",
      "In this chapter, I want to convince you of the following: As an explanation for an event, humans\n",
      "prefer short explanations (only 1 or 2 causes) that contrast the current situation with a situation in\n",
      "which the event would not have occurred. Especially abnormal causes provide good explanations.\n",
      "Explanations are social interactions between the explainer and the explainee (recipient of the\n",
      "explanation) and therefore the social context has a great influence on the actual content of the\n",
      "explanation. [{'source': './data/xai.pdf', 'start_index': 68811}]\n",
      "* An explanation is the answer to a why-question (Miller 2017).\n",
      "\n",
      "• Why did not the treatment work on the patient?\n",
      "• Why was my loan rejected?\n",
      "• Why have we not been contacted by alien life yet?\n",
      "\n",
      "The first two questions can be answered with an “everyday”-explanation, while the third one comes\n",
      "from the category “More general scientific phenomena and philosophical questions”. We focus on\n",
      "the “everyday”-type explanations, because those are relevant to interpretable machine learning.\n",
      "Questions that start with “how” can usually be rephrased as “why” questions: “How was my loan\n",
      "rejected?” can be turned into “Why was my loan rejected?”.\n",
      "\n",
      "In the following, the term “explanation” refers to the social and cognitive process of explaining, but\n",
      "also to the product of these processes. The explainer can be a human being or a machine.\n",
      "\n",
      "What Is a Good Explanation?\n",
      "\n",
      "This section further condenses Miller’s summary on “good” explanations and adds concrete\n",
      "implications for interpretable machine learning. [{'source': './data/xai.pdf', 'start_index': 70198}]\n",
      "* Explanations are contrastive (Lipton 1990¹⁷): Humans usually do not ask why a certain prediction\n",
      "was made, but why this prediction was made instead of another prediction. We tend to think in\n",
      "counterfactual cases, i.e. “How would the prediction have been if input X had been different?”.\n",
      "For a house price prediction, the house owner might be interested in why the predicted price was\n",
      "high compared to the lower price they had expected. If my loan application is rejected, I do not\n",
      "care to hear all the factors that generally speak for or against a rejection. I am interested in the\n",
      "factors in my application that would need to change to get the loan. I want to know the contrast\n",
      "between my application and the would-be-accepted version of my application. The recognition that\n",
      "contrasting explanations matter is an important finding for explainable machine learning. From\n",
      "most interpretable models, you can extract an explanation that implicitly contrasts a prediction of [{'source': './data/xai.pdf', 'start_index': 71218}]\n",
      "* Explanations are social: They are part of a conversation or interaction between the explainer\n",
      "and the receiver of the explanation. The social context determines the content and nature of the\n",
      "explanations. If I wanted to explain to a technical person why digital cryptocurrencies are worth\n",
      "so much, I would say things like: “The decentralized, distributed, blockchain-based ledger, which\n",
      "cannot be controlled by a central entity, resonates with people who want to secure their wealth,\n",
      "which explains the high demand and price.” But to my grandmother I would say: “Look, Grandma:\n",
      "Cryptocurrencies are a bit like computer gold. People like and pay a lot for gold, and young people\n",
      "like and pay a lot for computer gold.” What it means for interpretable machine learning:\n",
      "Pay attention to the social environment of your machine learning application and the target\n",
      "audience. Getting the social part of the machine learning model right depends entirely on your [{'source': './data/xai.pdf', 'start_index': 75540}]\n",
      "* contrasting explanations matter is an important finding for explainable machine learning. From\n",
      "most interpretable models, you can extract an explanation that implicitly contrasts a prediction of\n",
      "an instance with the prediction of an artificial data instance or an average of instances. Physicians\n",
      "might ask: “Why did the drug not work for my patient?”. And they might want an explanation that\n",
      "contrasts their patient with a patient for whom the drug worked and who is similar to the non-\n",
      "responding patient. Contrastive explanations are easier to understand than complete explanations.\n",
      "A complete explanation of the physician’s question why the drug does not work might include: The\n",
      "patient has had the disease for 10 years, 11 genes are over-expressed, the patients body is very quick\n",
      "in breaking the drug down into ineffective chemicals, … A contrastive explanation might be much\n",
      "simpler: In contrast to the responding patient, the non-responding patient has a certain combination [{'source': './data/xai.pdf', 'start_index': 71994}]\n",
      "* ⁶¹Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of machine learning.” ICML Workshop on\n",
      "\n",
      "Human Interpretability in Machine Learning. (2016).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "111\n",
      "\n",
      "The big picture of explainable machine learning. The real world goes through many layers before it reaches the human\n",
      "in the form of explanations.\n",
      "\n",
      "The lowest layer is the World. This could literally be nature itself, like the biology of the human\n",
      "body and how it reacts to medication, but also more abstract things like the real estate market. The\n",
      "World layer contains everything that can be observed and is of interest. Ultimately, we want to learn\n",
      "something about the World and interact with it.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "112\n",
      "\n",
      "The second layer is the Data layer. We have to digitize the World in order to make it processable\n",
      "for computers and also to store information. The Data layer contains anything from images, texts,\n",
      "tabular data and so on. [{'source': './data/xai.pdf', 'start_index': 240882}]\n",
      "* • Comprehensibility: How well do humans understand the explanations? This looks just like\n",
      "one more property among many, but it is the elephant in the room. Difficult to define and\n",
      "measure, but extremely important to get right. Many people agree that comprehensibility\n",
      "depends on the audience. Ideas for measuring comprehensibility include measuring the size\n",
      "of the explanation (number of features with a non-zero weight in a linear model, number of\n",
      "decision rules, …) or testing how well people can predict the behavior of the machine learning\n",
      "model from the explanations. The comprehensibility of the features used in the explanation\n",
      "should also be considered. A complex transformation of features might be less comprehensible\n",
      "than the original features. [{'source': './data/xai.pdf', 'start_index': 66598}]\n",
      "* When you need explanations with ALL factors for a particular prediction or behavior, you do not\n",
      "want a human-friendly explanation, but a complete causal attribution. You probably want a causal\n",
      "attribution if you are legally required to specify all influencing features or if you debug the machine\n",
      "learning model. In this case, ignore the following points. In all other cases, where lay people or people\n",
      "with little time are the recipients of the explanation, the following sections should be interesting to\n",
      "you.\n",
      "\n",
      "What Is an Explanation?\n",
      "\n",
      "An explanation is the answer to a why-question (Miller 2017).\n",
      "\n",
      "• Why did not the treatment work on the patient?\n",
      "• Why was my loan rejected?\n",
      "• Why have we not been contacted by alien life yet? [{'source': './data/xai.pdf', 'start_index': 69660}]\n",
      "* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of\n",
      "machine learning.” ICML Workshop on Human Interpretability in Machine Learning. (2016).\n",
      "\n",
      "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should I trust you?: Explaining the\n",
      "predictions of any classifier.” Proceedings of the 22nd ACM SIGKDD international conference on\n",
      "knowledge discovery and data mining. ACM (2016).\n",
      "\n",
      "Shapley, Lloyd S. “A value for n-person games.” Contributions to the Theory of Games 2.28 (1953):\n",
      "307-317.\n",
      "\n",
      "Staniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and\n",
      "breakDown packages.” arXiv preprint arXiv:1804.01955 (2018).\n",
      "\n",
      "Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep\n",
      "neural networks.” IEEE Transactions on Evolutionary Computation (2019).\n",
      "\n",
      "Szegedy, Christian, et al. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199\n",
      "(2013). [{'source': './data/xai.pdf', 'start_index': 499406}]\n",
      "* ⁹³Staniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and breakDown packages.” arXiv preprint\n",
      "\n",
      "arXiv:1804.01955 (2018).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "Example-based explanation methods select particular instances of the dataset to explain the behavior\n",
      "of machine learning models or to explain the underlying data distribution. [{'source': './data/xai.pdf', 'start_index': 370991}]\n",
      "* Interpretability\n",
      "\n",
      "26\n",
      "\n",
      "Properties of Explanations\n",
      "\n",
      "We want to explain the predictions of a machine learning model. To achieve this, we rely on some\n",
      "explanation method, which is an algorithm that generates explanations. An explanation usually\n",
      "relates the feature values of an instance to its model prediction in a humanly understandable\n",
      "way. Other types of explanations consist of a set of data instances (e.g in the case of the k-nearest\n",
      "neighbor model). For example, we could predict cancer risk using a support vector machine and\n",
      "explain predictions using the local surrogate method, which generates decision trees as explanations.\n",
      "Or we could use a linear regression model instead of a support vector machine. The linear regression\n",
      "model is already equipped with an explanation method (interpretation of the weights). [{'source': './data/xai.pdf', 'start_index': 61052}]\n",
      "* The next chapter focuses on the evaluation of explanations for individual predictions on the function\n",
      "level. What are the relevant properties of explanations that we would consider for their evaluation?\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "26\n",
      "\n",
      "Properties of Explanations [{'source': './data/xai.pdf', 'start_index': 60847}]\n",
      "* Doshi-Velez and Kim (2017) propose three main levels for the evaluation of interpretability:\n",
      "\n",
      "Application level evaluation (real task): Put the explanation into the product and have it tested\n",
      "by the end user. Imagine fracture detection software with a machine learning component that\n",
      "locates and marks fractures in X-rays. At the application level, radiologists would test the fracture\n",
      "detection software directly to evaluate the model. This requires a good experimental setup and an\n",
      "understanding of how to assess quality. A good baseline for this is always how good a human would\n",
      "be at explaining the same decision. [{'source': './data/xai.pdf', 'start_index': 59270}]\n",
      "* In interpretable machine learning, counterfactual explanations can be used to explain predictions\n",
      "of individual instances. The “event” is the predicted outcome of an instance, the “causes” are the\n",
      "particular feature values of this instance that were input to the model and “caused” a certain\n",
      "prediction. Displayed as a graph, the relationship between the inputs and the prediction is very\n",
      "simple: The feature values cause the prediction.\n",
      "\n",
      "The causal relationships between inputs of a machine learning model and the predictions, when the model is merely\n",
      "seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data).\n",
      "\n",
      "Even if in reality the relationship between the inputs and the outcome to be predicted might not be\n",
      "causal, we can see the inputs of a model as the cause of the prediction. [{'source': './data/xai.pdf', 'start_index': 377061}]\n",
      "* Explanations are truthful. Good explanations prove to be true in reality (i.e. in other situations).\n",
      "But disturbingly, this is not the most important factor for a “good” explanation. For example,\n",
      "selectiveness seems to be more important than truthfulness. An explanation that selects only one\n",
      "or two possible causes rarely covers the entire list of relevant causes. Selectivity omits part of the\n",
      "truth. It is not true that only one or two factors, for example, have caused a stock market crash, but\n",
      "the truth is that there are millions of causes that influence millions of people to act in such a way\n",
      "that in the end a crash was caused. What it means for interpretable machine learning:\n",
      "The explanation should predict the event as truthfully as possible, which in machine learning is\n",
      "sometimes called fidelity. So if we say that a second balcony increases the price of a house, then\n",
      "that also should apply to other houses (or at least to similar houses). For humans, fidelity of an [{'source': './data/xai.pdf', 'start_index': 79999}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "168\n",
      "\n",
      "Local Surrogate (LIME)\n",
      "\n",
      "Local surrogate models are interpretable models that are used to explain individual predictions of\n",
      "black box machine learning models. Local interpretable model-agnostic explanations (LIME)⁷⁹ is a\n",
      "paper in which the authors propose a concrete implementation of local surrogate models. Surrogate\n",
      "models are trained to approximate the predictions of the underlying black box model. Instead of\n",
      "training a global surrogate model, LIME focuses on training local surrogate models to explain\n",
      "individual predictions. [{'source': './data/xai.pdf', 'start_index': 332397}]\n",
      "* Importance of Interpretability\n",
      "\n",
      "If a machine learning model performs well, why do not we just trust the model and ignore why\n",
      "it made a certain decision? “The problem is that a single metric, such as classification accuracy, is\n",
      "an incomplete description of most real-world tasks.” (Doshi-Velez and Kim 2017 ¹²) [{'source': './data/xai.pdf', 'start_index': 33166}]\n",
      "* 214\n",
      "\n",
      "For example, you can create an interpretable prediction model: a so-called “nearest prototype model”.\n",
      "The prediction function is defined as:\n",
      "\n",
      "^f (x) = argmaxi2Sk(x; xi)\n",
      "\n",
      "which means that we select the prototype i from the set of prototypes S that is closest to the new\n",
      "data point, in the sense that it yields the highest value of the kernel function. The prototype itself\n",
      "is returned as an explanation for the prediction. This procedure has three tuning parameters: The\n",
      "type of kernel, the kernel scaling parameter and the number of prototypes. All parameters can be\n",
      "optimized within a cross validation loop. The criticisms are not used in this approach.\n",
      "\n",
      "As a third option, we can use MMD-critic to make any machine learning model globally explainable\n",
      "by examining prototypes and criticisms along with their model predictions. The procedure is as\n",
      "follows: [{'source': './data/xai.pdf', 'start_index': 429370}]\n",
      "* The complexity of the explanation model has to be defined in advance. This is just a small complaint,\n",
      "because in the end the user always has to define the compromise between fidelity and sparsity.\n",
      "\n",
      "Another really big problem is the instability of the explanations. In an article ⁸⁸ the authors showed\n",
      "that the explanations of two very close points varied greatly in a simulated setting. Also, in my\n",
      "experience, if you repeat the sampling process, then the explantions that come out can be different.\n",
      "Instability means that it is difficult to trust the explanations, and you should be very critical.\n",
      "\n",
      "Conclusion: Local surrogate models, with LIME as a concrete implementation, are very promising.\n",
      "But the method is still in development phase and many problems need to be solved before it can be\n",
      "safely applied.\n",
      "\n",
      "⁸⁸Alvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv preprint arXiv:1806.08049 (2018).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "Shapley Values\n",
      "\n",
      "177 [{'source': './data/xai.pdf', 'start_index': 348817}]\n",
      "* Let us dive deeper into the reasons why interpretability is so important. When it comes to predictive\n",
      "modeling, you have to make a trade-off: Do you just want to know what is predicted? For example,\n",
      "the probability that a customer will churn or how effective some drug will be for a patient. Or do you\n",
      "want to know why the prediction was made and possibly pay for the interpretability with a drop\n",
      "in predictive performance? In some cases, you do not care why a decision was made, it is enough\n",
      "to know that the predictive performance on a test dataset was good. But in other cases, knowing\n",
      "the ‘why’ can help you learn more about the problem, the data and the reason why a model might\n",
      "fail. Some models may not require explanations because they are used in a low-risk environment,\n",
      "meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the\n",
      "method has already been extensively studied and evaluated (e.g. optical character recognition). The [{'source': './data/xai.pdf', 'start_index': 33477}]\n",
      "* Martens, David, and Foster Provost. “Explaining data-driven document classifications.” (2014).\n",
      "\n",
      "Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint\n",
      "arXiv:1706.07269. (2017).\n",
      "\n",
      "Nickerson, Raymond S. “Confirmation Bias: A ubiquitous phenomenon in many guises.” Review of\n",
      "General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).\n",
      "\n",
      "Papernot, Nicolas, et al. “Practical black-box attacks against machine learning.” Proceedings of the\n",
      "2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).\n",
      "\n",
      "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-precision model-agnostic\n",
      "explanations.” AAAI Conference on Artificial Intelligence (2018).\n",
      "\n",
      "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of\n",
      "machine learning.” ICML Workshop on Human Interpretability in Machine Learning. (2016). [{'source': './data/xai.pdf', 'start_index': 498663}]\n",
      "* Interpretable Machine Learning\n",
      "\n",
      "A Guide for Making Black Box Models Explainable\n",
      "\n",
      "Christoph Molnar\n",
      "\n",
      "This book is for sale at http://leanpub.com/interpretable-machine-learning\n",
      "\n",
      "This version was published on 2019-02-21\n",
      "\n",
      "This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing\n",
      "process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and\n",
      "many iterations to get reader feedback, pivot until you have the right book and build traction once\n",
      "you do.\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0\n",
      "International License\n",
      "\n",
      "\fContents\n",
      "\n",
      "Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "1 [{'source': './data/xai.pdf', 'start_index': 1}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "Separating the explanations from the machine learning model (= model-agnostic interpretation\n",
      "methods) has some advantages (Ribeiro, Singh, and Guestrin 2016⁶¹). The great advantage of model-\n",
      "agnostic interpretation methods over model-specific ones is their flexibility. Machine learning\n",
      "developers are free to use any machine learning model they like when the interpretation methods\n",
      "can be applied to any model. Anything that builds on an interpretation of a machine learning model,\n",
      "such as a graphic or user interface, also becomes independent of the underlying machine learning\n",
      "model. Typically, not just one, but many types of machine learning models are evaluated to solve\n",
      "a task, and when comparing models in terms of interpretability, it is easier to work with model-\n",
      "agnostic explanations, because the same method can be used for any type of model. [{'source': './data/xai.pdf', 'start_index': 238526}]\n",
      "* Interpretability\n",
      "\n",
      "There is no mathematical definition of interpretability. A (non-mathematical) definition I like by\n",
      "Miller (2017)¹⁰ is: Interpretability is the degree to which a human can understand the cause of\n",
      "a decision. Another one is: Interpretability is the degree to which a human can consistently\n",
      "predict the model’s result ¹¹. The higher the interpretability of a machine learning model, the\n",
      "easier it is for someone to comprehend why certain decisions or predictions have been made.\n",
      "A model is better interpretable than another model if its decisions are easier for a human to\n",
      "comprehend than decisions from the other model. I will use both the terms interpretable and\n",
      "explainable interchangeably. Like Miller (2017), I think it makes sense to distinguish between the\n",
      "terms interpretability/explainability and explanation. I will use “explanation” for explanations of\n",
      "individual predictions. See the section about explanations to learn what we humans see as a good\n",
      "explanation. [{'source': './data/xai.pdf', 'start_index': 32176}]\n",
      "* We take a closer look at the properties of explanation methods and explanations (Robnik-Sikonja\n",
      "and Bohanec, 2018¹⁵). These properties can be used to judge how good an explanation method or\n",
      "explanation is. It is not clear for all these properties how to measure them correctly, so one of the\n",
      "challenges is to formalize how they could be calculated.\n",
      "\n",
      "Properties of Explanation Methods\n",
      "\n",
      "• Expressive Power is the “language” or structure of the explanations the method is able to\n",
      "generate. An explanation method could generate IF-THEN rules, decision trees, a weighted\n",
      "sum, natural language or something else. [{'source': './data/xai.pdf', 'start_index': 61873}]\n",
      "* The Future of Interpretability\n",
      "\n",
      "Let us take a look at the possible future of machine learning interpretability.\n",
      "\n",
      "The focus will be on model-agnostic interpretability tools.\n",
      "\n",
      "It is much easier to automate interpretability when it is decoupled from the underlying machine\n",
      "learning model. The advantage of model-agnostic interpretability lies in its modularity. We can eas-\n",
      "ily replace the underlying machine learning model. We can just as easily replace the interpretation\n",
      "method. For these reasons, model-agnostic methods will scale much better. That is why I believe that\n",
      "model-agnostic methods will become more dominant in the long term. But intrinsically interpretable\n",
      "methods will also have a place.\n",
      "\n",
      "Machine learning will be automated and, with it, interpretability. [{'source': './data/xai.pdf', 'start_index': 482487}]\n",
      "* meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the\n",
      "method has already been extensively studied and evaluated (e.g. optical character recognition). The\n",
      "need for interpretability arises from an incompleteness in problem formalization (Doshi-Velez and\n",
      "Kim 2017), which means that for certain problems or tasks it is not enough to get the prediction\n",
      "(the what). The model must also explain how it came to the prediction (the why), because a correct\n",
      "prediction only partially solves your original problem. The following reasons drive the demand for\n",
      "interpretability and explanations (Doshi-Velez and Kim 2017 and Miller 2017). [{'source': './data/xai.pdf', 'start_index': 34257}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "space.\n",
      "\n",
      "230\n",
      "\n",
      "Dog or fish? For the SVM prediction (middle row) images that had similar colors as the test image were the most\n",
      "influential. For the neural network prediction (bottom row) fish in different setting were most influential, but also a\n",
      "dog image (top right). Work by Koh and Liang (2017).\n",
      "\n",
      "Handling domain mismatches / Debugging model errors [{'source': './data/xai.pdf', 'start_index': 464066}]\n",
      "* Interpretability\n",
      "\n",
      "20\n",
      "\n",
      "• Trust: It is easier for humans to trust a system that explains its decisions compared to a black\n",
      "\n",
      "box.\n",
      "\n",
      "When we do not need interpretability.\n",
      "\n",
      "The following scenarios illustrate when we do not need or even do not want interpretability of\n",
      "machine learning models. [{'source': './data/xai.pdf', 'start_index': 44911}]\n",
      "* model. The Model-Agnostic Methods chapter deals with methods such as partial dependence plots\n",
      "and permutation feature importance. Model-agnostic methods work by changing the input of the\n",
      "machine learning model and measuring changes in the prediction output. Model-agnostic methods\n",
      "that return data instances as explanations are discussed in the chapter Example Based Explanations.\n",
      "All model-agnostic methods can be further differentiated based on whether they explain global\n",
      "model behavior across all data instances or individual predictions. The following methods explain\n",
      "the overall behavior of the model: Partial Dependence Plots, Accumulated Local Effects, Feature\n",
      "Interaction, Feature Importance, Global Surrogate Models and Prototypes and Criticisms. To explain\n",
      "individual predictions we have Local Surrogate Models, Shapley Value Explanations, Counterfactual\n",
      "Explanations (and closely related: Adversarial Examples). Some methods can be used to explain both [{'source': './data/xai.pdf', 'start_index': 9707}]\n",
      "* Human curiosity and learning: Humans have a mental model of their environment that is updated\n",
      "when something unexpected happens. This update is performed by finding an explanation for the\n",
      "unexpected event. For example, a human feels unexpectedly sick and asks, “Why do I feel so sick?”.\n",
      "He learns that he gets sick every time he eats those red berries. He updates his mental model and\n",
      "decides that the berries caused the sickness and should therefore be avoided. When opaque machine\n",
      "learning models are used in research, scientific findings remain completely hidden if the model only\n",
      "gives predictions without explanations. To facilitate learning and satisfy curiosity as to why certain\n",
      "predictions or behaviors are created by machines, interpretability and explanations are crucial. Of\n",
      "course, humans do not need explanations for everything that happens. For most people it is okay that\n",
      "they do not understand how a computer works. Unexpected events makes us curious. For example: [{'source': './data/xai.pdf', 'start_index': 35434}]\n",
      "* Example-Based Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n",
      "Counterfactual Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n",
      "Adversarial Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n",
      "Prototypes and Criticisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n",
      "Influential Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n",
      "\n",
      "A Look into the Crystal Ball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n",
      "The Future of Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n",
      "The Future of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n",
      "\n",
      "Contribute to the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 [{'source': './data/xai.pdf', 'start_index': 4276}]\n",
      "* Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.”\n",
      "arXiv preprint arXiv:1703.04730 (2017).\n",
      "\n",
      "Laugel, Thibault, et al. “Inverse classification for comparison-based interpretability in machine\n",
      "learning.” arXiv preprint arXiv:1712.08443 (2017).\n",
      "\n",
      "Letham, Benjamin, et al. “Interpretable classifiers using rules and Bayesian analysis: Building a better\n",
      "stroke prediction model.” The Annals of Applied Statistics 9.3 (2015): 1350-1371.\n",
      "\n",
      "Lipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-\n",
      "266.\n",
      "\n",
      "Lipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).\n",
      "\n",
      "\fReferences\n",
      "\n",
      "245\n",
      "\n",
      "Lundberg, Scott, and Su-In Lee. “An unexpected unity among methods for interpreting model\n",
      "predictions.” arXiv preprint arXiv:1611.07478 (2016).\n",
      "\n",
      "Martens, David, and Foster Provost. “Explaining data-driven document classifications.” (2014). [{'source': './data/xai.pdf', 'start_index': 497819}]\n",
      "* Global, Holistic Model Interpretability\n",
      "\n",
      "How does the trained model make predictions? [{'source': './data/xai.pdf', 'start_index': 54053}]\n",
      "* In this section, I will present the approach suggested by Wachter et. al (2017)⁹⁶. They suggest\n",
      "minimizing the following loss.\n",
      "\n",
      "L(x; x′; y′; (cid:21)) = (cid:21) (cid:1) ( ^f (x′) (cid:0) y′)2 + d(x; x′)\n",
      "\n",
      "The first term is the quadratic distance between the model prediction for the counterfactual x’ and\n",
      "the desired outcome y’, which the user must define in advance. The second term is the distance d\n",
      "between the instance x to be explained and the counterfactual x’, but more about this later. The\n",
      "parameter (cid:21) balances the distance in prediction (first term) against the distance in feature values\n",
      "(second term). The loss is solved for a given (cid:21) and returns a counterfactual x’. A higher value of (cid:21)\n",
      "\n",
      "⁹⁶Wachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without opening the black box: Automated decisions\n",
      "\n",
      "and the GDPR.” (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "194 [{'source': './data/xai.pdf', 'start_index': 385234}]\n",
      "* in breaking the drug down into ineffective chemicals, … A contrastive explanation might be much\n",
      "simpler: In contrast to the responding patient, the non-responding patient has a certain combination\n",
      "of genes that make the drug less effective. The best explanation is the one that highlights the greatest\n",
      "difference between the object of interest and the reference object. What it means for interpretable\n",
      "machine learning:\n",
      "Humans do not want a complete explanation for a prediction, but want to compare what the\n",
      "differences were to another instance’s prediction (can be an artificial one). Creating contrastive\n",
      "explanations is application-dependent because it requires a point of reference for comparison. And\n",
      "this may depend on the data point to be explained, but also on the user receiving the explanation.\n",
      "A user of a house price prediction website might want to have an explanation of a house price\n",
      "prediction contrastive to their own house or maybe to another house on the website or maybe [{'source': './data/xai.pdf', 'start_index': 72780}]\n",
      "* • Portability describes the range of machine learning models with which the explanation\n",
      "method can be used. Methods with a low translucency have a higher portability because they\n",
      "treat the machine learning model as a black box. Surrogate models might be the explanation\n",
      "method with the highest portability. Methods that only work for e.g. recurrent neural networks\n",
      "have low portability.\n",
      "\n",
      "• Algorithmic Complexity describes the computational complexity of the method that generates\n",
      "the explanation. This property is important to consider when computation time is a bottleneck\n",
      "in generating explanations.\n",
      "\n",
      "Properties of Individual Explanations\n",
      "\n",
      "• Accuracy: How well does an explanation predict unseen data? High accuracy is especially\n",
      "important if the explanation is used for predictions in place of the machine learning model.\n",
      "\n",
      "¹⁵Robnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning.\n",
      "\n",
      "Springer, Cham. 159-175. (2018). [{'source': './data/xai.pdf', 'start_index': 63134}]\n",
      "* Interpretable Models\n",
      "\n",
      "109\n",
      "\n",
      "local level? To explain a prediction, you can always retrieve the k neighbors that were used for\n",
      "the prediction. Whether the model is interpretable depends solely on the question whether you\n",
      "can ‘interpret’ a single instance in the dataset. If an instance consists of hundreds or thousands of\n",
      "features, then it is not interpretable, I would argue. But if you have few features or a way to reduce\n",
      "your instance to the most important features, presenting the k-nearest neighbors can give you good\n",
      "explanations.\n",
      "\n",
      "\fModel-Agnostic Methods [{'source': './data/xai.pdf', 'start_index': 237988}]\n",
      "* This multi-layered abstraction also helps to understand the differences in approaches between\n",
      "statisticians and machine learning practitioners. Statisticians deal with the Data layer, such as\n",
      "planning clinical trials or designing surveys. They skip the Black Box Model layer and go right to the\n",
      "Interpretability Methods layer. Machine learning specialists also deal with the Data layer, such as\n",
      "collecting labeled samples of skin cancer images or crawling Wikipedia. Then they train a black box\n",
      "machine learning model. The Interpretability Methods layer is skipped and humans directly deal\n",
      "with the black box model predictions. It’s great that interpretable machine learning fuses the work\n",
      "of statisticians and machine learning specialists. [{'source': './data/xai.pdf', 'start_index': 242526}]\n",
      "* Interpretability is not required when the problem is well studied. Some applications have been\n",
      "sufficiently well studied so that there is enough practical experience with the model and problems\n",
      "with the model have been solved over time. A good example is a machine learning model for optical\n",
      "character recognition that processes images from envelopes and extracts addresses. There is years of\n",
      "experience with these systems and it is clear that they work. In addition, we are not really interested\n",
      "in gaining additional insights about the task at hand. [{'source': './data/xai.pdf', 'start_index': 46107}]\n",
      "* • Model flexibility: The interpretation method can work with any machine learning model,\n",
      "\n",
      "such as random forests and deep neural networks.\n",
      "\n",
      "• Explanation flexibility: You are not limited to a certain form of explanation. In some cases it\n",
      "might be useful to have a linear formula, in other cases a graphic with feature importances.\n",
      "• Representation flexibility: The explanation system should be able to use a different feature\n",
      "representation as the model being explained. For a text classifier that uses abstract word\n",
      "embedding vectors, it might be preferable to use the presence of individual words for the\n",
      "explanation.\n",
      "\n",
      "The bigger picture\n",
      "\n",
      "Let us take a high level look at model-agnostic interpretability. We capture the world by collecting\n",
      "data, and abstract it further by learning to predict the data (for the task) with a machine learning\n",
      "model. Interpretability is just another layer on top that helps humans understand. [{'source': './data/xai.pdf', 'start_index': 239955}]\n",
      "* Introduction\n",
      "\n",
      "13\n",
      "\n",
      "A learner learns a model from labeled training data. The model is used to make predictions.\n",
      "\n",
      "A Black Box Model is a system that does not reveal its internal mechanisms. In machine learning,\n",
      "“black box” describes models that cannot be understood by looking at their parameters (e.g. a neural\n",
      "network). The opposite of a black box is sometimes referred to as White Box, and is referred to in\n",
      "this book as interpretable model. Model-agnostic methods for interpretability treat machine learning\n",
      "models as black boxes, even if they are not.\n",
      "\n",
      "Interpretable Machine Learning refers to methods and models that make the behavior and\n",
      "predictions of machine learning systems understandable to humans.\n",
      "\n",
      "A Dataset is a table with the data from which the machine learns. The dataset contains the features\n",
      "and the target to predict. When used to induce a model, the dataset is called training data.\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "14 [{'source': './data/xai.pdf', 'start_index': 29863}]\n",
      "* 2\n",
      "4\n",
      "10\n",
      "12\n",
      "\n",
      "Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "15\n",
      "Importance of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "21\n",
      "Taxonomy of Interpretability Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "23\n",
      "Scope of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "25\n",
      "Evaluation of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "26\n",
      "Properties of Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "29\n",
      "Human-friendly Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [{'source': './data/xai.pdf', 'start_index': 1190}]\n",
      "* • Translucency describes how much the explanation method relies on looking into the machine\n",
      "learning model, like its parameters. For example, explanation methods relying on intrinsically\n",
      "interpretable models like the linear regression model (model-specific) are highly translucent.\n",
      "Methods only relying on manipulating inputs and observing the predictions have zero\n",
      "translucency. Depending on the scenario, different levels of translucency might be desirable.\n",
      "The advantage of high translucency is that the method can rely on more information to\n",
      "generate explanations. The advantage of low translucency is that the explanation method is\n",
      "more portable. [{'source': './data/xai.pdf', 'start_index': 62481}]\n",
      "* (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "198\n",
      "\n",
      "An illustration of Growing Spheres and selecting sparse counterfactuals by Laugel et. al (2017).\n",
      "\n",
      "Anchors by Ribeiro et. al (2018)⁹⁹ are the opposite of counterfactuals. Anchors answer the question:\n",
      "Which features are sufficient to anchor a prediction, i.e. changing the other features cannot change\n",
      "the prediction? Once we have found features that serve as anchors for a prediction, we will no longer\n",
      "find counterfactual instances by changing the features not used in the anchor.\n",
      "\n",
      "Examples for anchors by Ribeiro et. al (2018).\n",
      "\n",
      "⁹⁹Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-precision model-agnostic explanations.” AAAI Conference on\n",
      "\n",
      "Artificial Intelligence (2018).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "199\n",
      "\n",
      "Adversarial Examples [{'source': './data/xai.pdf', 'start_index': 396951}]\n",
      "* Interpretability is not required if the model has no significant impact. Imagine someone named\n",
      "Mike working on a machine learning side project to predict where his friends will go for their next\n",
      "holidays based on Facebook data. Mike just likes to surprise his friends with educated guesses where\n",
      "they will be going on holidays. There is no real problem if the model is wrong (at worst just a little\n",
      "embarrassment for Mike), nor is there a problem if Mike cannot explain the output of his model. It\n",
      "is perfectly fine not to have interpretability in this case. The situation would change if Mike started\n",
      "building a business around these holiday destination predictions. If the model is wrong, the business\n",
      "could lose money, or the model may work worse for some people because of learned racial bias. As\n",
      "soon as the model has a significant impact, be it financial or social, interpretability becomes relevant. [{'source': './data/xai.pdf', 'start_index': 45199}]\n",
      "* Local Interpretability for a Group of Predictions\n",
      "\n",
      "Why did the model make specific predictions for a group of instances?\n",
      "\n",
      "Model predictions for multiple instances can be explained either with global model interpretation\n",
      "methods (on a modular level) or with explanations of individual instances. The global methods can\n",
      "be applied by taking the group of instances, treating them as if the group were the complete dataset,\n",
      "and using the global methods with this subset. The individual explanation methods can be used on\n",
      "each instance and then listed or aggregated for the entire group.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "25\n",
      "\n",
      "Evaluation of Interpretability\n",
      "\n",
      "There is no real consensus about what interpretability is in machine learning. Nor is it clear how to\n",
      "measure it. But there is some initial research on this and an attempt to formulate some approaches\n",
      "for evaluation, as described in the following section.\n",
      "\n",
      "Doshi-Velez and Kim (2017) propose three main levels for the evaluation of interpretability: [{'source': './data/xai.pdf', 'start_index': 58374}]\n",
      "* ¹⁵Robnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning.\n",
      "\n",
      "Springer, Cham. 159-175. (2018).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "27\n",
      "\n",
      "Low accuracy can be fine if the accuracy of the machine learning model is also low, and if the\n",
      "goal is to explain what the black box model does. In this case, only fidelity is important. [{'source': './data/xai.pdf', 'start_index': 63961}]\n",
      "* Based on the premise that the goal of a machine learning model can never be perfectly specified, it\n",
      "follows that interpretable machine learning is necessary to close the gap between the misspecified\n",
      "and the actual goal. In many areas and sectors, interpretability will be the catalyst for the adoption\n",
      "of machine learning. Some anecdotal evidence: Many people I have spoken to do not use machine\n",
      "learning because they cannot explain the models to others. I believe that interpretability will\n",
      "address this issue and make machine learning attractive to organisations and people who demand\n",
      "some transparency. In addition to the misspecification of the problem, many industries require\n",
      "interpretability, be it for legal reasons, due to risk aversion or to gain insight into the underlying task.\n",
      "Machine learning automates the modeling process and moves the human a bit further away from the\n",
      "data and the underlying task: This increases the risk of problems with experimental design, choice of [{'source': './data/xai.pdf', 'start_index': 481353}]\n",
      "* Good explanations are consistent with prior beliefs of the explainee. Humans tend to ignore\n",
      "information that is inconsistent with their prior beliefs. This effect is called confirmation bias\n",
      "(Nickerson 1998²⁰). Explanations are not spared by this kind of bias. People will tend to devalue\n",
      "or ignore explanations that do not agree with their beliefs. The set of beliefs varies from person to\n",
      "person, but there are also group-based prior beliefs such as political worldviews. What it means for\n",
      "interpretable machine learning:\n",
      "Good explanations are consistent with prior beliefs. This is difficult to integrate into machine\n",
      "learning and would probably drastically compromise predictive performance. Our prior belief for\n",
      "the effect of house size on predicted price is that the larger the house, the higher the price. Let\n",
      "us assume that a model also shows a negative effect of house size on the predicted price for a few [{'source': './data/xai.pdf', 'start_index': 81070}]\n",
      "* ⁸⁸Alvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv preprint arXiv:1806.08049 (2018).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "Shapley Values\n",
      "\n",
      "177\n",
      "\n",
      "A prediction can be explained by assuming that each feature value of the instance is a “player” in\n",
      "a game where the prediction is the payout. The Shapley value – a method from coalitional game\n",
      "theory – tells us how to fairly distribute the “payout” among the features.\n",
      "\n",
      "General Idea\n",
      "\n",
      "Assume the following scenario:\n",
      "\n",
      "You have trained a machine learning model to predict apartment prices. For a certain apartment\n",
      "it predicts €300,000 and you need to explain this prediction. The apartment has a size of 50 m², is\n",
      "located on the 2nd floor, has a park nearby and cats are banned:\n",
      "\n",
      "The predicted price for a 50 m² 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain\n",
      "how each of these feature values contributed to the prediction. [{'source': './data/xai.pdf', 'start_index': 349628}]\n",
      "* New methods for the interpretation of machine learning models are published at breakneck speed.\n",
      "To keep up with everything that is published would be madness and simply impossible. That is why\n",
      "you will not find the most novel and fancy methods in this book, but established methods and basic\n",
      "concepts of machine learning interpretability. These basics prepare you for making machine learning\n",
      "models interpretable. Internalizing the basic concepts also empowers you to better understand and\n",
      "evaluate any new paper on interpretability published on arxiv.org⁶ in the last 5 minutes since you\n",
      "began reading this book (I might be exaggerating the publication rate). [{'source': './data/xai.pdf', 'start_index': 8283}]\n",
      "* Explanations are selected: People do not expect explanations that cover the actual and complete\n",
      "list of causes of an event. We are used to selecting one or two causes from a variety of possible\n",
      "causes as THE explanation. As proof, turn on the TV news: “The decline in stock prices is blamed on\n",
      "a growing backlash against the company’s product due to problems with the latest software update.”\n",
      "“Tsubasa and his team lost the match because of a weak defense: they gave their opponents too much\n",
      "room to play out their strategy.”\n",
      "“The increasing distrust of established institutions and our government are the main factors that\n",
      "have reduced voter turnout.”\n",
      "The fact that an event can be explained by various causes is called the Rashomon Effect. Rashomon\n",
      "is a Japanese movie that tells alternative, contradictory stories (explanations) about the death of a\n",
      "samurai. For machine learning models, it is advantageous if a good prediction can be made from [{'source': './data/xai.pdf', 'start_index': 73947}]\n",
      "* = {\\url{https://christophm.github.io/interpretable-ml-book/}},\n",
      "= {2019},\n",
      "= {A Guide for Making Black Box Models Explainable}\n",
      "\n",
      "}\n",
      "\n",
      "I am always curious about where and how interpretation methods are used in industry and research.\n",
      "If you use the book as a reference, it would be great if you wrote me a line and told me what for.\n",
      "This is of course optional and only serves to satisfy my own curiosity and to stimulate interesting\n",
      "exchanges. My mail is christoph.molnar.ai@gmail.com .\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "\fAcknowledgements\n",
      "\n",
      "Writing this book was (and still is) a lot of fun. But it is also a lot of work and I am very happy about\n",
      "the support I received.\n",
      "\n",
      "My biggest thank-you goes to Katrin who had the hardest job in terms of hours and effort: she proof-\n",
      "read the book from beginning to end and discovered many spelling mistakes and inconsistencies that\n",
      "I would never have found. I am very grateful for her support. [{'source': './data/xai.pdf', 'start_index': 490620}]\n",
      "* Interpretability\n",
      "\n",
      "21\n",
      "\n",
      "Taxonomy of Interpretability Methods\n",
      "\n",
      "Methods for machine learning interpretability can be classified according to various criteria. [{'source': './data/xai.pdf', 'start_index': 48198}]\n",
      "* Introduction\n",
      "\n",
      "11\n",
      "\n",
      "describe a deep neural network, and there is no way to understand the model in its entirety. Other\n",
      "models, such as the random forest, consist of hundreds of decision trees that “vote” for predictions.\n",
      "To understand how the decision was made, you would have to look into the votes and structures of\n",
      "each of the hundreds of trees. That just does not work no matter how clever you are or how good\n",
      "your working memory is. The best performing models are often blends of several models (also called\n",
      "ensembles) that cannot be interpreted, even if each single model could be interpreted. If you focus\n",
      "only on performance, you will automatically get more and more opaque models. Just take a look at\n",
      "interviews with winners on the kaggle.com machine learning competition platform⁸: The winning\n",
      "models were mostly ensembles of models or very complex models such as boosted trees or deep\n",
      "neural networks.\n",
      "\n",
      "⁸http://blog.kaggle.com/\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "Terminology\n",
      "\n",
      "12 [{'source': './data/xai.pdf', 'start_index': 27441}]\n",
      "* Example\n",
      "\n",
      "Since the computation of image explanations is rather slow, the lime R package⁸³ contains a\n",
      "precomputed example which we will also use to show the output of the method. The explanations\n",
      "can be displayed directly on the image samples. Since we can have several predicted labels per\n",
      "image (sorted by probability), we can explain the top n_labels. For the following image the top 3\n",
      "predictions were electric guitar; acoustic guitar; and Labrador.\n",
      "\n",
      "LIME explanations for the top 3 classes for image classification made by Google’s Inception neural network. The\n",
      "example is taken from the LIME paper (Ribeiro et. al., 2016).\n",
      "\n",
      "⁸³https://github.com/thomasp85/lime\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "175\n",
      "\n",
      "The prediction and explanation in the first case are very reasonable. The first prediction of electric\n",
      "guitar is of course wrong, but the explanation shows us that the neural network still behaved\n",
      "reasonably because the image part identified suggests that this could be an electric guitar.\n",
      "\n",
      "Advantages [{'source': './data/xai.pdf', 'start_index': 344724}]\n",
      "* ¹⁰⁶Biggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine learning.” Pattern Recognition 84 (2018):\n",
      "\n",
      "317-331.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "207\n",
      "\n",
      "them. Using interpretation methods to understand which features are important and how features\n",
      "affect the prediction is also a proactive step in understanding the weaknesses of a machine learning\n",
      "model. As the data scientist, do you trust your model in this dangerous world without ever having\n",
      "looked beyond the predictive power on a test dataset? Have you analyzed how the model behaves\n",
      "in different scenarios, identified the most important inputs, checked the prediction explanations for\n",
      "some examples? Have you tried to find adversarial inputs? The interpretability of machine learning\n",
      "models plays a major role in cybersecurity. Being reactive, the opposite of proactive, means waiting\n",
      "until the system has been attacked and only then understanding the problem and installing some\n",
      "defensive measures. [{'source': './data/xai.pdf', 'start_index': 415442}]\n",
      "* ¹¹⁴https://github.com/kohpangwei/influence-release\n",
      "¹¹⁵http://mlexplained.com/2018/06/01/paper-dissected-understanding-black-box-predictions-via-influence-functions/#more-641\n",
      "\n",
      "\fA Look into the Crystal Ball\n",
      "\n",
      "What is the future of interpretable machine learning? This chapter is a speculative mental exercise\n",
      "and a subjective guess how interpretable machine learning will develop. I opened the book with\n",
      "rather pessimistic short stories and would like to conclude with a more optimistic outlook.\n",
      "\n",
      "I have based my “predictions” on three premises: [{'source': './data/xai.pdf', 'start_index': 473608}]\n",
      "* ⁴Friedman,\n",
      "\n",
      "Jerome,\n",
      "\n",
      "Trevor\n",
      "\n",
      "Hastie,\n",
      "\n",
      "and\n",
      "\n",
      "Robert\n",
      "\n",
      "Tibshirani.\n",
      "\n",
      "“The\n",
      "\n",
      "elements\n",
      "\n",
      "of\n",
      "\n",
      "statistical\n",
      "\n",
      "learning”.\n",
      "\n",
      "www.web.stanford.edu/(cid:24)hastie/ElemStatLearn/ (2009).\n",
      "⁵https://www.coursera.org/learn/machine-learning\n",
      "⁶https://arxiv.org/\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "3\n",
      "\n",
      "aspects of global model behavior and individual predictions: Individual Conditional Expectation and\n",
      "Influential Instances.\n",
      "\n",
      "The book ends with an optimistic outlook on what the future of interpretable machine learning\n",
      "might look like.\n",
      "\n",
      "You can either read the book from beginning to end or jump directly to the methods that interest\n",
      "you.\n",
      "\n",
      "I hope you will enjoy the read!\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "Story Time\n",
      "\n",
      "4\n",
      "\n",
      "We will start with some short stories. Each story is an admittedly exaggerated call for interpretable\n",
      "machine learning. If you are in a hurry, you can skip the stories. If you want to be entertained and\n",
      "(de-)motivated, read on! [{'source': './data/xai.pdf', 'start_index': 10673}]\n",
      "* Algorithm transparency is about how the algorithm learns a model from the data and what kind\n",
      "of relationships it can learn. If you use convolutional neural networks to classify images, you\n",
      "can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an\n",
      "understanding of how the algorithm works, but not for the specific model that is learned in the end,\n",
      "and not for how individual predictions are made. Algorithm transparency only requires knowledge\n",
      "of the algorithm and not of the data or learned model. This book focuses on model interpretability\n",
      "and not algorithm transparency. Algorithms such as the least squares method for linear models\n",
      "are well studied and understood. They are characterized by a high transparency. Deep learning\n",
      "approaches (pushing a gradient through a network with millions of weights) are less well understood\n",
      "and the inner workings are the focus of ongoing research. They are considered less transparent. [{'source': './data/xai.pdf', 'start_index': 53085}]\n",
      "* Interpretability\n",
      "\n",
      "32\n",
      "\n",
      "argue that now, the better explanation is “because the teacher tested the student”. It was unlikely\n",
      "that the teacher would test, so the teacher behaved abnormally. What it means for interpretable\n",
      "machine learning:\n",
      "If one of the input features for a prediction was abnormal in any sense (like a rare category of a\n",
      "categorical feature) and the feature influenced the prediction, it should be included in an explanation,\n",
      "even if other ‘normal’ features have the same influence on the prediction as the abnormal one. An\n",
      "abnormal feature in our house price prediction example might be that a rather expensive house has\n",
      "two balconies. Even if some attribution method finds that the two balconies contribute as much to the\n",
      "price difference as the above average house size, the good neighborhood or the recent renovation, the\n",
      "abnormal feature “two balconies” might be the best explanation for why the house is so expensive. [{'source': './data/xai.pdf', 'start_index': 79060}]\n",
      "* By fitting machine learning models based on the Data layer, we get the Black Box Model layer.\n",
      "Machine learning algorithms learn with data from the real world to make predictions or find\n",
      "structures.\n",
      "\n",
      "Above the Black Box Model layer is the Interpretability Methods layer, which helps us deal with\n",
      "the opacity of machine learning models. What were the most important features for a particular\n",
      "diagnosis? Why was a financial transaction classified as fraud?\n",
      "\n",
      "The last layer is occupied by a Human. Look! This one waves to you because you are reading this\n",
      "book and helping to provide better explanations for black box models! Humans are ultimately the\n",
      "consumers of the explanations. [{'source': './data/xai.pdf', 'start_index': 241847}]\n",
      "* Interpretable Models\n",
      "\n",
      "77\n",
      "\n",
      "Statistical software usually has really good interfaces to fit GLMs, GAMs and more special linear\n",
      "models.\n",
      "\n",
      "The opacity of many machine learning models comes from 1) a lack of sparseness, which means\n",
      "that many features are used, 2) features that are treated in a nonlinear fashion, which means you\n",
      "need more than a single weight to describe the effect, and 3) the modeling of interactions between\n",
      "the features. Assuming that linear models are highly interpretable but often underfit reality, the\n",
      "extensions described in this chapter offer a good way to achieve a smooth transition to more\n",
      "flexible models, while preserving some of the interpretability.\n",
      "\n",
      "Disadvantages [{'source': './data/xai.pdf', 'start_index': 166248}]\n",
      "* Aamodt, Agnar, and Enric Plaza. “Case-based reasoning: Foundational issues, methodological\n",
      "variations, and system approaches.” AI communications 7.1 (1994): 39-59.\n",
      "\n",
      "Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. “Tubespam: comment spam filtering\n",
      "on YouTube.” In Machine Learning and Applications (Icmla), Ieee 14th International Conference on,\n",
      "138–43. IEEE. (2015).\n",
      "\n",
      "Alvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv\n",
      "preprint arXiv:1806.08049 (2018).\n",
      "\n",
      "Apley, Daniel W. “Visualizing the effects of predictor variables in black box supervised learning\n",
      "models.” arXiv preprint arXiv:1612.08468 (2016).\n",
      "\n",
      "Athalye, Anish, and Ilya Sutskever. “Synthesizing robust adversarial examples.” arXiv preprint\n",
      "arXiv:1707.07397 (2017).\n",
      "\n",
      "Biggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine\n",
      "learning.” Pattern Recognition 84 (2018): 317-331. [{'source': './data/xai.pdf', 'start_index': 493741}]\n",
      "* An alternative to model-agnostic interpretation methods is to use only interpretable models, which\n",
      "often has the big disadvantage that predictive performance is lost compared to other machine\n",
      "learning models and you limit yourself to one type of model. The other alternative is to use model-\n",
      "specific interpretation methods. The disadvantage of this is that it also binds you to one model type\n",
      "and it will be difficult to switch to something else.\n",
      "\n",
      "Desirable aspects of a model-agnostic explanation system are (Ribeiro, Singh, and Guestrin 2016):\n",
      "\n",
      "• Model flexibility: The interpretation method can work with any machine learning model,\n",
      "\n",
      "such as random forests and deep neural networks. [{'source': './data/xai.pdf', 'start_index': 239407}]\n",
      "* Interpretability\n",
      "\n",
      "22\n",
      "\n",
      "• Intrinsically interpretable model: One solution to interpreting black box models is to\n",
      "approximate them (either globally or locally) with an interpretable model. The interpretable\n",
      "model itself is interpreted by looking at internal model parameters or feature summary\n",
      "statistics. [{'source': './data/xai.pdf', 'start_index': 51609}]\n",
      "* This book starts with some (dystopian) short stories that are not needed to understand the book,\n",
      "but hopefully will entertain and make you think. Then the book explores the concepts of machine\n",
      "learning interpretability. We will discuss when interpretability is important and what different types\n",
      "of explanations there are. Terms used throughout the book can be looked up in the Terminology\n",
      "chapter. Most of the models and methods explained are presented using real data examples\n",
      "which are described in the Data chapter. One way to make machine learning interpretable is\n",
      "to use interpretable models, such as linear models or decision trees. The other option is the use\n",
      "of model-agnostic interpretation tools that can be applied to any supervised machine learning\n",
      "model. The Model-Agnostic Methods chapter deals with methods such as partial dependence plots\n",
      "and permutation feature importance. Model-agnostic methods work by changing the input of the [{'source': './data/xai.pdf', 'start_index': 8945}]\n",
      "* Interpretability\n",
      "\n",
      "17\n",
      "\n",
      "Recommended products when buying some paint from [Amazon](https://www.amazon.com/Colore-Acrylic-Paint-\n",
      "Set-12/dp/B014UMGA5W/). Visited on December 5th 2012.\n",
      "\n",
      "In many scientific disciplines there is a change from qualitative to quantitative methods (e.g.\n",
      "sociology, psychology), and also towards machine learning (biology, genomics). The goal of\n",
      "science is to gain knowledge, but many problems are solved with big datasets and black box\n",
      "machine learning models. The model itself becomes the source of knowledge instead of the data.\n",
      "Interpretability makes it possible to extract this additional knowledge captured by the model. [{'source': './data/xai.pdf', 'start_index': 38169}]\n",
      "* • Fidelity: How well does the explanation approximate the prediction of the black box model?\n",
      "High fidelity is one of the most important properties of an explanation, because an explanation\n",
      "with low fidelity is useless to explain the machine learning model. Accuracy and fidelity are\n",
      "closely related. If the black box model has high accuracy and the explanation has high fidelity,\n",
      "the explanation also has high accuracy. Some explanations offer only local fidelity, meaning\n",
      "the explanation only approximates well to the model prediction for a subset of the data (e.g.\n",
      "local surrogate models) or even for only an individual data instance (e.g. Shapley Values).\n",
      "• Consistency: How much does an explanation differ between models that have been trained\n",
      "on the same task and that produce similar predictions? For example, I train a support vector\n",
      "machine and a linear regression model on the same task and both produce very similar [{'source': './data/xai.pdf', 'start_index': 64335}]\n",
      "* The counterfactual explanation method is model-agnostic, since it only works with the model\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "192\n",
      "\n",
      "inputs and output. This method would also feel at home in the model-agnostic chapter, since the\n",
      "interpretation can be expressed as a summary of the differences in feature values (“change features\n",
      "A and B to change the prediction”). But a counterfactual explanation is itself a new instance, so it\n",
      "lives in this chapter (“starting from instance X, change A and B to get a counterfactual instance”).\n",
      "Unlike prototypes, counterfactuals do not have to be actual instances from the training data, but can\n",
      "be a new combination of feature values.\n",
      "\n",
      "Before discussing how to create counterfactuals, I would like to discuss some use cases for\n",
      "counterfactuals and how a good counterfactual explanation looks like. [{'source': './data/xai.pdf', 'start_index': 378568}]\n",
      "* • Model internals (e.g. learned weights): The interpretation of intrinsically interpretable\n",
      "models falls into this category. Examples are the weights in linear models or the learned tree\n",
      "structure (the features and thresholds used for the splits) of decision trees. The lines are blurred\n",
      "between model internals and feature summary statistic in, for example, linear models, because\n",
      "the weights are both model internals and summary statistics for the features at the same time.\n",
      "Another method that outputs model internals is the visualization of feature detectors learned\n",
      "in convolutional neural networks. Interpretability methods that output model internals are by\n",
      "definition model-specific (see next criterion). [{'source': './data/xai.pdf', 'start_index': 50188}]\n",
      "* Intrinsic or post hoc? This criteria distinguishes whether interpretability is achieved by restricting\n",
      "the complexity of the machine learning model (intrinsic) or by applying methods that analyze the\n",
      "model after training (post hoc). Intrinsic interpretability refers to machine learning models that are\n",
      "considered interpretable due to their simple structure, such as short decision trees or sparse linear\n",
      "models. Post hoc interpretability refers to the application of interpretation methods after model\n",
      "training. Permutation feature importance is, for example, a post hoc interpretation method. Post\n",
      "hoc methods can also be applied to intrinsically interpretable models. For example, permutation\n",
      "feature importance can be computed for decision trees. The organization of the chapters in this\n",
      "book is determined by the distinction between intrinsically interpretable models and post hoc (and\n",
      "model-agnostic) interpretation methods. [{'source': './data/xai.pdf', 'start_index': 48354}]\n",
      "* You could describe a model as interpretable if you can comprehend the entire model at once (Lipton\n",
      "2016¹⁴). To explain the global model output, you need the trained model, knowledge of the algorithm\n",
      "and the data. This level of interpretability is about understanding how the model makes decisions,\n",
      "based on a holistic view of its features and each of the learned components such as weights, other\n",
      "parameters, and structures. Which features are important and what kind of interactions between\n",
      "them take place? Global model interpretability helps to understand the distribution of your target\n",
      "outcome based on the features. Global model interpretability is very difficult to achieve in practice.\n",
      "Any model that exceeds a handful of parameters or weights is unlikely to fit into the short-term\n",
      "memory of the average human. I argue that you cannot really imagine a linear model with 5 features,\n",
      "because it would mean drawing the estimated hyperplane mentally in a 5-dimensional space. Any [{'source': './data/xai.pdf', 'start_index': 54140}]\n",
      "* Even if in reality the relationship between the inputs and the outcome to be predicted might not be\n",
      "causal, we can see the inputs of a model as the cause of the prediction.\n",
      "\n",
      "Given this simple graph, it is easy to see how we can simulate counterfactuals for predictions of\n",
      "machine learning models: We simply change the feature values of an instance before making the\n",
      "predictions and we analyze how the prediction changes. We are interested in scenarios in which the\n",
      "prediction changes in a relevant way, like a flip in predicted class (e.g. credit application accepted\n",
      "or rejected) or in which the prediction reaches a certain threshold (e.g. the probability for cancer\n",
      "reaches 10%). A counterfactual explanation of a prediction describes the smallest change to the\n",
      "feature values that changes the prediction to a predefined output.\n",
      "\n",
      "The counterfactual explanation method is model-agnostic, since it only works with the model\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "192 [{'source': './data/xai.pdf', 'start_index': 377735}]\n",
      "* Mathematically, local surrogate models with interpretability constraint can be expressed as follows:\n",
      "\n",
      "explanation(x) = arg min\n",
      "g2G\n",
      "\n",
      "L(f; g; (cid:25)x) + Ω(g)\n",
      "\n",
      "The explanation model for instance x is the model g (e.g. linear regression model) that minimizes\n",
      "loss L (e.g. mean squared error), which measures how close the explanation is to the prediction of\n",
      "the original model f (e.g. an xgboost model), while the model complexity Ω(g) is kept low (e.g. prefer\n",
      "fewer features). G is the family of possible explanations, for example all possible linear regression\n",
      "models. The proximity measure (cid:25)x defines how large the neighborhood around instance x is that we\n",
      "consider for the explanation. In practice, LIME only optimizes the loss part. The user has to determine\n",
      "the complexity, e.g. by selecting the maximum number of features that the linear regression model\n",
      "may use.\n",
      "\n",
      "The recipe for training local surrogate models: [{'source': './data/xai.pdf', 'start_index': 333980}]\n",
      "* ²⁰Nickerson, Raymond S. “Confirmation Bias: A ubiquitous phenomenon in many guises.” Review of General Psychology 2 (2). Educational\n",
      "\n",
      "Publishing Foundation: 175. (1998).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "33\n",
      "\n",
      "probabilities of joint events. (Joe is a librarian. Is he more likely to be a shy person or to be a shy\n",
      "person who likes to read books?) A good example is “The house is expensive because it is big”,\n",
      "which is a very general, good explanation of why houses are expensive or cheap. What it means\n",
      "for interpretable machine learning:\n",
      "Generality can easily be measured by the feature’s support, which is the number of instances to\n",
      "which the explanation applies divided by the total number of instances.\n",
      "\n",
      "\fDatasets\n",
      "\n",
      "Throughout the book, all models and techniques are applied to real datasets that are freely\n",
      "available online. We will use different datasets for different tasks: Classification, regression and\n",
      "text classification.\n",
      "\n",
      "Bike Rentals (Regression) [{'source': './data/xai.pdf', 'start_index': 82793}]\n",
      "* Explanations are used to manage social interactions. By creating a shared meaning of something,\n",
      "the explainer influences the actions, emotions and beliefs of the recipient of the explanation. For\n",
      "a machine to interact with us, it may need to shape our emotions and beliefs. Machines have to\n",
      "“persuade” us, so that they can achieve their intended goal. I would not fully accept my robot\n",
      "vacuum cleaner if it did not explain its behavior to some degree. The vacuum cleaner creates a\n",
      "shared meaning of, for example, an “accident” (like getting stuck on the bathroom carpet … again)\n",
      "by explaining that it got stuck instead of simply stopping to work without comment. Interestingly,\n",
      "there may be a misalignment between the goal of the explaining machine (create trust) and the goal\n",
      "of the recipient (understand the prediction or behavior). Perhaps the full explanation for why Doge\n",
      "got stuck could be that the battery was very low, that one of the wheels is not working properly and [{'source': './data/xai.pdf', 'start_index': 41706}]\n",
      "* These adversarial examples were generated by minimizing the following function with respect to r:\n",
      "\n",
      "loss( ^f (x + r); l) + c (cid:1) jrj\n",
      "\n",
      "In this formula, x is an image (represented as a vector of pixels), r is the changes to the pixels to create\n",
      "an adversarial image (x+r produces a new image), l is the desired outcome class, and the parameter c\n",
      "is used to balance the distance between images and the distance between predictions. The first term\n",
      "is the distance between the predicted outcome of the adversarial example and the desired class l, the\n",
      "second term measures the distance between the adversarial example and the original image. This\n",
      "formulation is almost identical to the loss function to generate counterfactual explanations. There\n",
      "are additional constraints for r so that the pixel values remain between 0 and 1. The authors suggest\n",
      "to solve this optimization problem with a box-constrained L-BFGS, an optimization algorithm that\n",
      "works with gradients. [{'source': './data/xai.pdf', 'start_index': 400883}]\n",
      "* Szegedy, Christian, et al. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199\n",
      "(2013).\n",
      "\n",
      "Wachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without open-\n",
      "ing the black box: Automated decisions and the GDPR.” (2017).\n",
      "\n",
      "Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. “Scalable Bayesian rule lists.” Proceedings of the\n",
      "34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n",
      "\n",
      "Zhao, Qingyuan, and Trevor Hastie. “Causal interpretations of black-box models.” Journal of\n",
      "Business & Economic Statistics, to appear. (2017).\n",
      "\n",
      "Štrumbelj, Erik, and Igor Kononenko. “A general method for visualizing and explaining black-box\n",
      "regression models.” In International Conference on Adaptive and Natural Computing Algorithms,\n",
      "21–30. Springer. (2011).\n",
      "\n",
      "Štrumbelj, Erik, and Igor Kononenko. “Explaining prediction models and individual predictions with\n",
      "feature contributions.” Knowledge and information systems 41.3 (2014): 647-665. [{'source': './data/xai.pdf', 'start_index': 500260}]\n",
      "* LIME is one of the few methods that works for tabular data, text and images.\n",
      "\n",
      "The fidelity measure (how well the interpretable model approximates the black box predictions)\n",
      "gives us a good idea of how reliable the interpretable model is in explaining the black box predictions\n",
      "in the neighborhood of the data instance of interest.\n",
      "\n",
      "LIME is implemented in Python (lime⁸⁴ and Skater⁸⁵) and R (lime package⁸⁶ and iml package⁸⁷) and\n",
      "is very easy to use.\n",
      "\n",
      "The explanations created with local surrogate models can use other features than the original\n",
      "model. This can be a big advantage over other methods, especially if the original features cannot bet\n",
      "interpreted. A text classifier can rely on abstract word embeddings as features, but the explanation\n",
      "can be based on the presence or absence of words in a sentence. A regression model can rely on a\n",
      "non-interpretable transformation of some attributes, but the explanations can be created with the\n",
      "original attributes.\n",
      "\n",
      "Disadvantages [{'source': './data/xai.pdf', 'start_index': 346926}]\n",
      "* The counterfactual explanation method is relatively easy to implement, since it is essentially\n",
      "a loss function that can be optimized with standard optimizer libraries. Some additional details\n",
      "must be taken into account, such as limiting feature values to meaningful ranges (e.g. only positive\n",
      "apartment sizes).\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "For each instance you will usually find multiple counterfactual explanations (Rashomon\n",
      "effect). This is inconvenient – most people prefer simple explanations over the complexity of the real\n",
      "world. It is also a practical challenge. Let us say we generated 23 counterfactual explanations for one\n",
      "instance. Are we reporting them all? Only the best? What if they are all relatively “good”, but very\n",
      "different? These questions must be answered anew for each project. It can also be advantageous to\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "197\n",
      "\n",
      "have multiple counterfactual explanations, because then humans can select the ones that correspond\n",
      "to their previous knowledge. [{'source': './data/xai.pdf', 'start_index': 393566}]\n",
      "* That is it with the important parts of MMD-critic theory. One question remains: How can MMD-\n",
      "critic be used for interpretable machine learning?\n",
      "\n",
      "MMD-critic can add interpretability in three ways: By helping to better understand the data\n",
      "distribution; by building an interpretable model; by making a black box model interpretable.\n",
      "\n",
      "If you apply MMD-critic to your data to find prototypes and criticisms, it will improve your\n",
      "understanding of the data, especially if you have a complex data distribution with edge cases. But\n",
      "with MMD-critic you can achieve more!\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "214\n",
      "\n",
      "For example, you can create an interpretable prediction model: a so-called “nearest prototype model”.\n",
      "The prediction function is defined as:\n",
      "\n",
      "^f (x) = argmaxi2Sk(x; xi) [{'source': './data/xai.pdf', 'start_index': 428779}]\n",
      "* After exploring the concepts of interpretability, you will learn about simple, interpretable models\n",
      "such as decision trees, decision rules and linear regression. Later chapters focus on general model-\n",
      "agnostic methods for interpreting black box models like feature importance and accumulated local\n",
      "effects and explaining individual predictions with Shapley values and LIME.\n",
      "\n",
      "All interpretation methods are explained in depth and discussed critically. How do they work under\n",
      "the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This\n",
      "book will enable you to select and correctly apply the interpretation method that is most suitable\n",
      "for your machine learning project. [{'source': './data/xai.pdf', 'start_index': 6015}]\n",
      "* Interpretable Models\n",
      "\n",
      "Feature transformation\n",
      "\n",
      "73 [{'source': './data/xai.pdf', 'start_index': 159315}]\n",
      "* ¹¹⁸https://github.com/christophM/interpretable-ml-book\n",
      "¹¹⁹https://github.com/christophM/interpretable-ml-book/issues\n",
      "¹²⁰mailto:christoph.molnar.ai@gmail.com\n",
      "\n",
      "\fCiting this Book\n",
      "\n",
      "If you found this book useful for your blog post, research article or product, I would be grateful if\n",
      "you would cite this book. You can cite the book like this:\n",
      "\n",
      "Molnar, Christoph. \"Interpretable machine learning. A Guide for Making Black Box Mod\\\n",
      "els Explainable\", 2019. https://christophm.github.io/interpretable-ml-book/.\n",
      "\n",
      "Or use the following bibtex entry:\n",
      "\n",
      "@book{molnar,\n",
      "\n",
      "= {Interpretable Machine Learning},\n",
      "= {Christoph Molnar},\n",
      "\n",
      "title\n",
      "author\n",
      "publisher = {https://christophm.github.io/interpretable-ml-book/},\n",
      "note\n",
      "year\n",
      "subtitle\n",
      "\n",
      "= {\\url{https://christophm.github.io/interpretable-ml-book/}},\n",
      "= {2019},\n",
      "= {A Guide for Making Black Box Models Explainable}\n",
      "\n",
      "} [{'source': './data/xai.pdf', 'start_index': 489907}]\n",
      "* Generating Counterfactual Explanations\n",
      "\n",
      "A simple and naive approach to generating counterfactual explanations is searching by trial and\n",
      "error. This approach involves randomly changing feature values of the instance of interest and\n",
      "stopping when the desired output is predicted. Like the example where Anna tried to find a version\n",
      "of her apartment for which she could charge more rent. But there are better approaches than trial\n",
      "and error. First, we define a loss function that takes as input the instance of interest, a counterfactual\n",
      "and the desired (counterfactual) outcome. The loss measures how far the predicted outcome of the\n",
      "counterfactual is from the predefined outcome and how far the counterfactual is from the instance\n",
      "of interest. We can either optimize the loss directly with an optimization algorithm or by searching\n",
      "around the instance, as suggested in the “Growing Spheres” method (see Software and Alternatives). [{'source': './data/xai.pdf', 'start_index': 384303}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "224\n",
      "\n",
      "A decision tree that models the relationship between the influence of the instances and their features. The maximum\n",
      "depth of the tree is set to 2. [{'source': './data/xai.pdf', 'start_index': 449788}]\n",
      "* • Certainty: Does the explanation reflect the certainty of the machine learning model? Many\n",
      "machine learning models only give predictions without a statement about the models confi-\n",
      "dence that the prediction is correct. If the model predicts a 4% probability of cancer for one\n",
      "patient, is it as certain as the 4% probability that another patient, with different feature values,\n",
      "received? An explanation that includes the model’s certainty is very useful.\n",
      "\n",
      "¹⁶https://en.wikipedia.org/wiki/Rashomon_effect\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "28 [{'source': './data/xai.pdf', 'start_index': 67355}]\n",
      "* Interpretability\n",
      "\n",
      "23\n",
      "\n",
      "Scope of Interpretability\n",
      "\n",
      "An algorithm trains a model that produces the predictions. Each step can be evaluated in terms of\n",
      "transparency or interpretability.\n",
      "\n",
      "Algorithm Transparency\n",
      "\n",
      "How does the algorithm create the model? [{'source': './data/xai.pdf', 'start_index': 52837}]\n",
      "* Artificial Intelligence (2018).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "199\n",
      "\n",
      "Adversarial Examples\n",
      "\n",
      "An adversarial example is an instance with small, intentional feature perturbations that cause\n",
      "a machine learning model to make a false prediction. I recommend reading the chapter about\n",
      "Counterfactual Explanations first, as the concepts are very similar. Adversarial examples are\n",
      "counterfactual examples with the aim to deceive the model, not interpret it.\n",
      "\n",
      "Why are we interested in adversarial examples? Are they not just curious by-products of machine\n",
      "learning models without practical relevance? The answer is a clear “no”. Adversarial examples make\n",
      "machine learning models vulnerable to attacks, as in the following scenarios. [{'source': './data/xai.pdf', 'start_index': 397658}]\n",
      "* ¹⁰¹Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adversarial examples.” arXiv preprint\n",
      "\n",
      "arXiv:1412.6572 (2014).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "202\n",
      "\n",
      "Goodfellow et. al (2014) suggested adding adversarial examples to the training data to learn robust\n",
      "models.\n",
      "\n",
      "A jellyfish … No, wait. A bathtub: 1-pixel attacks\n",
      "\n",
      "The approach presented by Goodfellow and colleagues (2014) requires many pixels to be changed,\n",
      "if only by a little. But what if you can only change a single pixel? Would you be able to deceive\n",
      "a machine learning model? Su et. al (2019) ¹⁰² showed that it is actually possible to deceive image\n",
      "classifiers by changing a single pixel.\n",
      "\n",
      "By intentionally changing a single pixel (marked with red circles) a neural network trained on ImageNet is deceived\n",
      "to predict the wrong class (in blue letters) instead of the original class (in black letters). Work by Su et. al (2019). [{'source': './data/xai.pdf', 'start_index': 404211}]\n",
      "* Global Model Interpretability on a Modular Level\n",
      "\n",
      "How do parts of the model affect predictions?\n",
      "\n",
      "¹⁴Lipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "24 [{'source': './data/xai.pdf', 'start_index': 55322}]\n",
      "* The Shapley value is the only explanation method with a solid theory. The axioms – efficiency,\n",
      "symmetry, dummy, additivity – give the explanation a reasonable foundation. Methods like LIME\n",
      "assume linear behavior of the machine learning model locally, but there is no theory as to why this\n",
      "should work.\n",
      "\n",
      "It is mind-blowing to explain a prediction as a game played by the feature values.\n",
      "\n",
      "Disadvantages [{'source': './data/xai.pdf', 'start_index': 365900}]\n",
      "* Contribute to the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\n",
      "\n",
      "Citing this Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "\n",
      "Acknowledgements\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\n",
      "\n",
      "References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n",
      "R Packages Used for Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n",
      "\n",
      "\fPreface\n",
      "\n",
      "Machine learning has great potential for improving products, processes and research. But computers\n",
      "usually do not explain their predictions which is a barrier to the adoption of machine learning.\n",
      "This book is about making machine learning models and their decisions interpretable. [{'source': './data/xai.pdf', 'start_index': 5153}]\n",
      "* In at least three aspects, the way I published this book is unconventional. First, it is available both as\n",
      "website and as ebook/pdf. The software I used to create this book is called bookdown, written by Yihui\n",
      "Xie¹²⁹, who created many R packages that make it easy to combine R code and text. Thanks a lot!\n",
      "Secondly, I self-publish the book on the platform Leanpub¹³⁰, instead of working with a traditional\n",
      "publisher. And third, I published the book as in-progress book, which has helped me enormously to\n",
      "get feedback and to monetize it along the way. Many thanks to leanpub for making this possible and\n",
      "handling the royalties fairly.\n",
      "I would also like to thank you, dear reader, for reading this book without a big publisher name\n",
      "behind it.\n",
      "\n",
      "I am grateful for the funding of my research on interpretable machine learning by the Bavarian State\n",
      "Ministry of Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B). [{'source': './data/xai.pdf', 'start_index': 492347}]\n",
      "* Robots and programs will explain themselves.\n",
      "\n",
      "We need more intuitive interfaces to machines and programs that make heavy use of machine\n",
      "learning. Some examples: A self-driving car that reports why it stopped abruptly (“70% probability\n",
      "that a kid will cross the road”); A credit default program that explains to a bank employee why a\n",
      "credit application was rejected (“Applicant has too many credit cards and is employed in an unstable\n",
      "job.”); A robot arm that explains why it moved the item from the conveyor belt into the trash bin\n",
      "(“The item has a craze at the bottom.”).\n",
      "\n",
      "Interpretability could boost machine intelligence research.\n",
      "\n",
      "I can imagine that by doing more research on how programs and machines can explain themselves,\n",
      "we can improve improve our understanding of intelligence and become better at creating intelligent\n",
      "machines.\n",
      "\n",
      "In the end, all these predictions are speculations and we have to see what the future really brings.\n",
      "Form your own opinion and continue learning! [{'source': './data/xai.pdf', 'start_index': 488216}]\n",
      "* Interpretable Models\n",
      "\n",
      "83\n",
      "\n",
      "Importance of the features measured by how much the node purity is improved on average.\n",
      "\n",
      "Advantages\n",
      "\n",
      "The tree structure is ideal for capturing interactions between features in the data.\n",
      "\n",
      "The data ends up in distinct groups that are often easier to understand than points on a multi-\n",
      "dimensional hyperplane as in linear regression. The interpretation is arguably pretty simple.\n",
      "\n",
      "The tree structure also has a natural visualization, with its nodes and edges. [{'source': './data/xai.pdf', 'start_index': 177618}]\n",
      "* 243–59. (1944).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "18\n",
      "\n",
      "circle opened a “door” to enter a “room” (which was simply a rectangle). The participants described\n",
      "the actions of the shapes as they would describe the actions of a human agent, assigning intentions\n",
      "and even emotions and personality traits to the shapes. Robots are a good example, like my vacuum\n",
      "cleaner, which I named “Doge”. If Doge gets stuck, I think: “Doge wants to keep cleaning, but\n",
      "asks me for help because it got stuck.” Later, when Doge finishes cleaning and searches the home\n",
      "base to recharge, I think: “Doge has a desire to recharge and intends to find the home base.” I\n",
      "also attribute personality traits: “Doge is a bit dumb, but in a cute way.” These are my thoughts,\n",
      "especially when I find out that Doge has knocked over a plant while dutifully vacuuming the house.\n",
      "A machine or algorithm that explains its predictions will find more acceptance. See also the chapter\n",
      "on explanations, which argues that explanations are a social process. [{'source': './data/xai.pdf', 'start_index': 40711}]\n",
      "* Example-based explanations help humans construct mental models of the machine learning model\n",
      "and the data the machine learning model has been trained on. It especially helps to understand\n",
      "complex data distributions. But what do I mean by example-based explanations? We often use them\n",
      "in our jobs and daily lives. Let us start with some examples⁹⁴.\n",
      "\n",
      "A physician sees a patient with an unusual cough and a mild fever. The patient’s symptoms remind\n",
      "her of another patient she had years ago with similar symptoms. She suspects that her current patient\n",
      "could have the same disease and she takes a blood sample to test for this specific disease.\n",
      "\n",
      "A data scientist works on a new project for one of his clients: Analysis of the risk factors that lead\n",
      "to the failure of production machines for keyboards. The data scientist remembers a similar project\n",
      "he worked on and reuses parts of the code from the old project because he thinks the client wants\n",
      "the same analysis. [{'source': './data/xai.pdf', 'start_index': 372370}]\n",
      "* deceive the model during application time. The examples impressively demonstrate how easily deep\n",
      "neural networks for object recognition can be deceived by images that appear harmless to humans.\n",
      "If you have not yet seen these examples, you might be surprised, because the changes in predictions\n",
      "are incomprehensible for a human observer. Adversarial examples are like optical illusions but for\n",
      "machines. [{'source': './data/xai.pdf', 'start_index': 399849}]\n",
      "* ¹⁰⁴Athalye, Anish, and Ilya Sutskever. “Synthesizing robust adversarial examples.” arXiv preprint arXiv:1707.07397 (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "205\n",
      "\n",
      "Et(cid:24)T [d(t(x′); t(x))]\n",
      "\n",
      "where x is the original image, t(x) the transformed image (e.g. rotated), x’ the adversarial example\n",
      "and t(x’) its transformed version. Apart from working with a distribution of transformations, the\n",
      "EOT method follows the familiar pattern of framing the search for adversarial examples as an\n",
      "optimization problem. We try to find an adversarial example x’ that maximizes the probability for\n",
      "the selected class yt (e.g. “rifle”) across the distribution of possible transformations T:\n",
      "\n",
      "arg max\n",
      "\n",
      "x′\n",
      "\n",
      "Et(cid:24)T [logP (ytjt(x′))]\n",
      "\n",
      "With the constraint that the expected distance over all possible transformations between adversarial\n",
      "example x’ and original image x remains below a certain threshold:\n",
      "\n",
      "Et(cid:24)T [d(t(x′); t(x))] < ϵ\n",
      "\n",
      "and x 2 [0; 1]d [{'source': './data/xai.pdf', 'start_index': 409570}]\n",
      "* Good explanations are general and probable. A cause that can explain many events is very\n",
      "general and could be considered a good explanation. Note that this contradicts the claim that\n",
      "abnormal causes make good explanations. As I see it, abnormal causes beat general causes. Abnormal\n",
      "causes are by definition rare in the given scenario. In the absence of an abnormal event, a\n",
      "general explanation is considered a good explanation. Also remember that people tend to misjudge\n",
      "\n",
      "²⁰Nickerson, Raymond S. “Confirmation Bias: A ubiquitous phenomenon in many guises.” Review of General Psychology 2 (2). Educational\n",
      "\n",
      "Publishing Foundation: 175. (1998).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "33 [{'source': './data/xai.pdf', 'start_index': 82321}]\n",
      "* The recipe for training local surrogate models:\n",
      "\n",
      "• Select your instance of interest for which you want to have an explanation of its black box\n",
      "\n",
      "prediction.\n",
      "\n",
      "• Perturb your dataset and get the black box predictions for these new points.\n",
      "• Weight the new samples according to their proximity to the instance of interest.\n",
      "• Train a weighted, interpretable model on the dataset with the variations.\n",
      "\n",
      "⁷⁹Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should I trust you?: Explaining the predictions of any classifier.”\n",
      "\n",
      "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "169\n",
      "\n",
      "• Explain the prediction by interpreting the local model. [{'source': './data/xai.pdf', 'start_index': 334857}]\n",
      "* • …\n",
      "\n",
      "The breakthrough for machine learning is not only achieved through better computers / more data\n",
      "/ better software, but also:\n",
      "\n",
      "Interpretability tools catalyze the adoption of machine learning.\n",
      "\n",
      "¹¹⁷https://deepl.com\n",
      "\n",
      "\fA Look into the Crystal Ball\n",
      "\n",
      "237 [{'source': './data/xai.pdf', 'start_index': 481097}]\n",
      "* All interpretable models explained in this book are interpretable on a modular level, with the\n",
      "exception of the k-nearest neighbors method. The following table gives an overview of the\n",
      "interpretable model types and their properties. A model is linear if the association between features\n",
      "and target is modelled linearly. A model with monotonicity constraints ensures that the relationship\n",
      "between a feature and the target outcome always goes in the same direction over the entire range\n",
      "of the feature: An increase in the feature value either always leads to an increase or always to a\n",
      "decrease in the target outcome. Monotonicity is useful for the interpretation of a model because it\n",
      "makes it easier to understand a relationship. Some models can automatically include interactions\n",
      "between features to predict the target outcome. You can include interactions in any type of model\n",
      "by manually creating interaction features. Interactions can improve predictive performance, but too [{'source': './data/xai.pdf', 'start_index': 91231}]\n",
      "* Explanations focus on the abnormal. People focus more on abnormal causes to explain events\n",
      "(Kahnemann and Tversky, 1981¹⁸). These are causes that had a small probability but nevertheless\n",
      "happened. The elimination of these abnormal causes would have greatly changed the outcome\n",
      "(counterfactual explanation). Humans consider these kinds of “abnormal” causes as good expla-\n",
      "nations. An example from Štrumbelj and Kononenko (2011)¹⁹ is: Assume we have a dataset of test\n",
      "situations between teachers and students. Students attend a course and pass the course directly\n",
      "after successfully giving a presentation. The teacher has the option to additionally ask the student\n",
      "questions to test their knowledge. Students who cannot answer these questions will fail the course.\n",
      "Students can have different levels of preparation, which translates into different probabilities for\n",
      "correctly answering the teacher’s questions (if they decide to test the student). We want to predict [{'source': './data/xai.pdf', 'start_index': 76601}]\n",
      "* I am grateful for the funding of my research on interpretable machine learning by the Bavarian State\n",
      "Ministry of Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B).\n",
      "\n",
      "¹²¹https://twitter.com/ExpectAPatronum\n",
      "¹²²https://github.com/christophM/interpretable-ml-book/graphs/contributors\n",
      "¹²³https://twitter.com/YvonneDoinel\n",
      "¹²⁴https://twitter.com/GoAbiAryan\n",
      "¹²⁵http://www.freepik.com/\n",
      "¹²⁶https://www.flaticon.com/\n",
      "¹²⁷http://www.chojugiga.com/\n",
      "¹²⁸https://twitter.com/topeconheroes\n",
      "¹²⁹https://yihui.name/\n",
      "¹³⁰https://leanpub.com/\n",
      "\n",
      "\fReferences\n",
      "\n",
      "“Definition of Algorithm.” https://www.merriam-webster.com/dictionary/algorithm. (2017).\n",
      "\n",
      "Aamodt, Agnar, and Enric Plaza. “Case-based reasoning: Foundational issues, methodological\n",
      "variations, and system approaches.” AI communications 7.1 (1994): 39-59. [{'source': './data/xai.pdf', 'start_index': 493089}]\n",
      "* The chapters in this part cover the following example-based interpretation methods:\n",
      "\n",
      "• Counterfactual explanations tell us how an instance has to change to significantly change\n",
      "its prediction. By creating counterfactual instances, we learn about how the model makes its\n",
      "predictions and can explain individual predictions.\n",
      "\n",
      "• Adversarial examples are counterfactuals used to fool machine learning models. The emphasis\n",
      "\n",
      "is on flipping the prediction and not explaining it.\n",
      "\n",
      "• Prototypes are a selection of representative instances from the data and criticisms are instances\n",
      "\n",
      "that are not well represented by those prototypes. ⁹⁵\n",
      "\n",
      "• Influential instances are the training data points that were the most influential for the\n",
      "parameters of a prediction model or the predictions themselves. Identifying and analysing\n",
      "influential instances helps to find problems with the data, debug the model and understand\n",
      "the model’s behavior better. [{'source': './data/xai.pdf', 'start_index': 375207}]\n",
      "* We ask how the model parameters or the predictions would change if we removed instances from\n",
      "the training data in the training process. This is in contrast to other interpretability approaches\n",
      "that analyze how the prediction changes when we manipulate the features of the instances to be\n",
      "predicted, such as partial dependence plots or feature importance. With influential instances, we\n",
      "do not treat the model as fixed, but as a function of the training data. Influential instances help\n",
      "us answer questions about global model behavior and about individual predictions. Which were\n",
      "the most influential instances for the model parameters or the predictions overall? Which were the\n",
      "most influential instances for a particular prediction? Influential instances tell us for which instances\n",
      "the model could have problems, which training instances should be checked for errors and give an\n",
      "impression of the robustness of the model. We might not trust a model if a single instance has a [{'source': './data/xai.pdf', 'start_index': 441352}]\n",
      "* Why did the model make a certain prediction for an instance?\n",
      "\n",
      "You can zoom in on a single instance and examine what the model predicts for this input, and\n",
      "explain why. If you look at an individual prediction, the behavior of the otherwise complex model\n",
      "might behave more pleasantly. Locally, the prediction might only depend linearly or monotonously\n",
      "on some features, rather than having a complex dependence on them. For example, the value of a\n",
      "house may depend nonlinearly on its size. But if you are looking at only one particular 100 square\n",
      "meters house, there is a possibility that for that data subset, your model prediction depends linearly\n",
      "on the size. You can find this out by simulating how the predicted price changes when you increase or\n",
      "decrease the size by 10 square meters. Local explanations can therefore be more accurate than global\n",
      "explanations. This book presents methods that can make individual predictions more interpretable\n",
      "in the section on model-agnostic methods. [{'source': './data/xai.pdf', 'start_index': 57384}]\n",
      "* ¹⁶https://en.wikipedia.org/wiki/Rashomon_effect\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "28\n",
      "\n",
      "• Degree of Importance: How well does the explanation reflect the importance of features or\n",
      "parts of the explanation? For example, if a decision rule is generated as an explanation for an\n",
      "individual prediction, is it clear which of the conditions of the rule was the most important?\n",
      "• Novelty: Does the explanation reflect whether a data instance to be explained comes from\n",
      "a “new” region far removed from the distribution of training data? In such cases, the model\n",
      "may be inaccurate and the explanation may be useless. The concept of novelty is related to the\n",
      "concept of certainty. The higher the novelty, the more likely it is that the model will have low\n",
      "certainty due to lack of data.\n",
      "\n",
      "• Representativeness: How many instances does an explanation cover? Explanations can cover\n",
      "the entire model (e.g. interpretation of weights in a linear regression model) or represent only\n",
      "an individual prediction (e.g. Shapley Values). [{'source': './data/xai.pdf', 'start_index': 67811}]\n",
      "* Judging by the attributes that constitute a good explanation, as presented in the Human-Friendly\n",
      "Explanations chapter, linear models do not create the best explanations. They are contrastive, but the\n",
      "reference instance is a data point where all numerical features are zero and the categorical features\n",
      "are at their reference categories. This is usually an artificial, meaningless instance that is unlikely\n",
      "to occur in your data or reality. There is an exception: If all numerical features are mean centered\n",
      "(feature minus mean of feature) and all categorical features are effect coded, the reference instance is\n",
      "the data point where all the features take on the mean feature value. This might also be a non-existent\n",
      "data point, but it might at least be more likely or more meaningful. In this case, the weights times\n",
      "the feature values (feature effects) explain the contribution to the predicted outcome contrastive [{'source': './data/xai.pdf', 'start_index': 114979}]\n",
      "* Closely related to learning is the human desire to find meaning in the world. We want to harmonize\n",
      "contradictions or inconsistencies between elements of our knowledge structures. “Why did my dog\n",
      "bite me even though it has never done so before?” a human might ask. There is a contradiction\n",
      "between the knowledge of the dog’s past behavior and the newly made, unpleasant experience of\n",
      "the bite. The vet’s explanation reconciles the dog owner’s contradiction: “The dog was under stress\n",
      "and bit.” The more a machine’s decision affects a person’s life, the more important it is for the\n",
      "machine to explain its behavior. If a machine learning model rejects a loan application, this may\n",
      "be completely unexpected for the applicants. They can only reconcile this inconsistency between\n",
      "expectation and reality with some kind of explanation. The explanations do not actually have to\n",
      "fully explain the situation, but should address a main cause. Another example is algorithmic product [{'source': './data/xai.pdf', 'start_index': 36464}]\n",
      "* sometimes called fidelity. So if we say that a second balcony increases the price of a house, then\n",
      "that also should apply to other houses (or at least to similar houses). For humans, fidelity of an\n",
      "explanation is not as important as its selectivity, its contrast and its social aspect. [{'source': './data/xai.pdf', 'start_index': 80783}]\n",
      "* If you can ensure that the machine learning model can explain decisions, you can also check the\n",
      "following traits more easily (Doshi-Velez and Kim 2017):\n",
      "\n",
      "• Fairness: Ensuring that predictions are unbiased and do not implicitly or explicitly discriminate\n",
      "against protected groups. An interpretable model can tell you why it has decided that a certain\n",
      "person should not get a loan, and it becomes easier for a human to judge whether the decision\n",
      "is based on a learned demographic (e.g. racial) bias.\n",
      "\n",
      "• Privacy: Ensuring that sensitive information in the data is protected.\n",
      "• Reliability or Robustness: Ensuring that small changes in the input do not lead to large changes\n",
      "\n",
      "in the prediction.\n",
      "\n",
      "• Causality: Check that only causal relationships are picked up.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "20\n",
      "\n",
      "• Trust: It is easier for humans to trust a system that explains its decisions compared to a black\n",
      "\n",
      "box.\n",
      "\n",
      "When we do not need interpretability. [{'source': './data/xai.pdf', 'start_index': 44152}]\n",
      "* Interpretable Models\n",
      "\n",
      "61\n",
      "\n",
      "GLM, GAM and more\n",
      "\n",
      "The biggest strength but also the biggest weakness of the linear regression model is that the\n",
      "prediction is modeled as a weighted sum of the features. In addition, the linear model comes with\n",
      "many other assumptions. The bad news is (well, not really news) that all those assumptions are\n",
      "often violated in reality: The outcome given the features might have a non-Gaussian distribution,\n",
      "the features might interact and the relationship between the features and the outcome might be\n",
      "nonlinear. The good news is that the statistics community has developed a variety of modifications\n",
      "that transform the linear regression model from a simple blade into a Swiss knife. [{'source': './data/xai.pdf', 'start_index': 135696}]\n",
      "* Digitization is driving automation. Imperfect goal specification conflicts with automation. I claim\n",
      "that this conflict is mediated partially by interpretation methods.\n",
      "\n",
      "The stage for our predictions is set, the crystal ball is ready, now we look at where the field could\n",
      "go!\n",
      "\n",
      "The Future of Machine Learning\n",
      "\n",
      "Without machine learning there can be no interpretable machine learning. Therefore we have to\n",
      "guess where machine learning is heading before we can talk about interpretability. [{'source': './data/xai.pdf', 'start_index': 477316}]\n",
      "* The tree structure also has a natural visualization, with its nodes and edges.\n",
      "\n",
      "Trees create good explanations as defined in the chapter on “Human-Friendly Explanations”.\n",
      "The tree structure automatically invites to think about predicted values for individual instances\n",
      "as counterfactuals: “If a feature had been greater / smaller than the split point, the prediction would\n",
      "have been y1 instead of y2. The tree explanations are contrastive, since you can always compare\n",
      "the prediction of an instance with relevant “what if”-scenarios (as defined by the tree) that are\n",
      "simply the other leaf nodes of the tree. If the tree is short, like one to three splits deep, the resulting\n",
      "explanations are selective. A tree with a depth of three requires a maximum of three features and\n",
      "split points to create the explanation for the prediction of an individual instance. The truthfulness\n",
      "of the prediction depends on the predictive performance of the tree. The explanations for short trees [{'source': './data/xai.pdf', 'start_index': 178022}]\n",
      "* expectation and reality with some kind of explanation. The explanations do not actually have to\n",
      "fully explain the situation, but should address a main cause. Another example is algorithmic product\n",
      "recommendation. Personally, I always think about why certain products or movies have been\n",
      "algorithmically recommended to me. Often it is quite clear: Advertising follows me on the Internet\n",
      "because I recently bought a washing machine, and I know that in the next days I will be followed by\n",
      "advertisements for washing machines. Yes, it makes sense to suggest gloves if I already have a winter\n",
      "hat in my shopping cart. The algorithm recommends this movie, because users who liked other\n",
      "movies I liked also enjoyed the recommended movie. Increasingly, Internet companies are adding\n",
      "explanations to their recommendations. A good example is the Amazon product recommendation,\n",
      "which is based on frequently purchased product combinations: [{'source': './data/xai.pdf', 'start_index': 37239}]\n",
      "* Biggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine\n",
      "learning.” Pattern Recognition 84 (2018): 317-331.\n",
      "\n",
      "Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).\n",
      "\n",
      "Brown, Tom B., et al. “Adversarial patch.” arXiv preprint arXiv:1712.09665 (2017).\n",
      "\n",
      "Cohen, William W. “Fast effective rule induction.” Machine Learning Proceedings (1995). 115-123.\n",
      "\n",
      "Cook, R. Dennis. “Detection of influential observation in linear regression.” Technometrics 19.1\n",
      "(1977): 15-18.\n",
      "\n",
      "Doshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,”\n",
      "no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).\n",
      "\n",
      "Fanaee-T, Hadi, and Joao Gama. “Event labeling combining ensemble detectors and background\n",
      "knowledge.” Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. doi:10.1007/s13748-\n",
      "013-0040-3. (2013). [{'source': './data/xai.pdf', 'start_index': 494524}]\n",
      "* Counterfactuals are human-friendly explanations, because they are contrastive to the current\n",
      "instance and because they are selective, meaning they usually focus on a small number of feature\n",
      "changes. But counterfactuals suffer from the ‘Rashomon effect’. Rashomon is a Japanese movie in\n",
      "which the murder of a Samurai is told by different people. Each of the stories explains the outcome\n",
      "equally well, but the stories contradict each other. The same can also happen with counterfactuals,\n",
      "since there are usually multiple different counterfactual explanations. Each counterfactual tells a\n",
      "different “story” of how a certain outcome was reached. One counterfactual might say to change\n",
      "feature A, the other counterfactual might say to leave A the same but change feature B, which is a\n",
      "contradiction. This issue of multiple truths can be addressed either by reporting all counterfactual\n",
      "explanations or by having a criterion to evaluate counterfactuals and select the best one. [{'source': './data/xai.pdf', 'start_index': 381310}]\n",
      "* Local Interpretability for a Single Prediction\n",
      "\n",
      "Why did the model make a certain prediction for an instance? [{'source': './data/xai.pdf', 'start_index': 57336}]\n",
      "* Given the nature of the cat-and-mouse game between attackers and defenders, we will see a lot\n",
      "of development and innovation in this area. Just think of the many different types of spam emails\n",
      "that are constantly evolving. New methods of attacks against machine learning models are invented\n",
      "and new defensive measures are proposed against these new attacks. More powerful attacks are\n",
      "developed to evade the latest defenses and so on, ad infinitum. With this chapter I hope to sensitize\n",
      "you to the problem of adversarial examples and that only by proactively studying the machine\n",
      "learning models are we able to discover and remedy weaknesses.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "208\n",
      "\n",
      "Prototypes and Criticisms [{'source': './data/xai.pdf', 'start_index': 418154}]\n",
      "* on the same task and that produce similar predictions? For example, I train a support vector\n",
      "machine and a linear regression model on the same task and both produce very similar\n",
      "predictions. I compute explanations using a method of my choice and analyze how different the\n",
      "explanations are. If the explanations are very similar, the explanations are highly consistent.\n",
      "I find this property somewhat tricky, since the two models could use different features, but\n",
      "get similar predictions (also called “Rashomon Effect”¹⁶). In this case a high consistency is not\n",
      "desirable because the explanations have to be very different. High consistency is desirable if\n",
      "the models really rely on similar relationships. [{'source': './data/xai.pdf', 'start_index': 65083}]\n",
      "* The book focuses on machine learning models for tabular data (also called relational or structured\n",
      "data) and less on computer vision and natural language processing tasks. Reading the book is\n",
      "recommended for machine learning practitioners, data scientists, statisticians, and anyone else\n",
      "interested in making machine learning models interpretable.\n",
      "\n",
      "About me: My name is Christoph Molnar, I’m a statistician and a machine learner. My goal is to\n",
      "make machine learning interpretable. If you are interested in improving the interpretability of your\n",
      "machine learning models, do not hesitate to contact me!\n",
      "\n",
      "Mail: christoph.molnar.ai@gmail.com\n",
      "\n",
      "Website: https://christophm.github.io/¹\n",
      "\n",
      "Follow me on Twitter! @ChristophMolnar²\n",
      "\n",
      "Cover by @YvonneDoinel³\n",
      "\n",
      "¹https://christophm.github.io/\n",
      "²https://twitter.com/ChristophMolnar\n",
      "³https://twitter.com/YvonneDoinel\n",
      "\n",
      "\fIntroduction [{'source': './data/xai.pdf', 'start_index': 6718}]\n",
      "* The blindfolded adversary: Black box attack\n",
      "\n",
      "Imagine the following scenario: I give you access to my great image classifier via Web API. You\n",
      "can get predictions from the model, but you do not have access to the model parameters. From the\n",
      "convenience of your couch, you can send data and my service answers with the corresponding\n",
      "classifications. Most adversarial attacks are not designed to work in this scenario because they\n",
      "require access to the gradient of the underlying deep neural network to find adversarial examples.\n",
      "Papernot and colleagues (2017)¹⁰⁵ showed that it is possible to create adversarial examples without\n",
      "internal model information and without access to the training data. This type of (almost) zero-\n",
      "knowledge attack is called black box attack.\n",
      "\n",
      "How it works: [{'source': './data/xai.pdf', 'start_index': 410876}]\n",
      "* Machine learning models can only be debugged and audited when they can be interpreted. Even\n",
      "in low risk environments, such as movie recommendations, the ability to interpret is valuable in\n",
      "the research and development phase as well as after deployment. Later, when a model is used in\n",
      "a product, things can go wrong. An interpretation for an erroneous prediction helps to understand\n",
      "the cause of the error. It delivers a direction for how to fix the system. Consider an example of a\n",
      "husky versus wolf classifier that misclassifies some huskies as wolves. Using interpretable machine\n",
      "learning methods, you would find that the misclassification was due to the snow on the image. The\n",
      "classifier learned to use snow as a feature for classifying images as “wolf”, which might make sense\n",
      "in terms of separating wolves from huskies in the training dataset, but not in real-world use. [{'source': './data/xai.pdf', 'start_index': 43275}]\n",
      "* In the end, all these predictions are speculations and we have to see what the future really brings.\n",
      "Form your own opinion and continue learning!\n",
      "\n",
      "\fContribute to the Book\n",
      "\n",
      "Thank you for reading my book about Interpretable Machine Learning. The book is under continuous\n",
      "development. It will be improved over time and more chapters will be added. Very similar to how\n",
      "software is developed.\n",
      "\n",
      "All text and code for the book is open source and available at github.com¹¹⁸. On the Github page\n",
      "you can suggest fixes and open issues¹¹⁹ if you find a mistake or if something is missing.\n",
      "\n",
      "If you want to help out even more, the issues page is also the best place to find problems to fix and\n",
      "a good way to contribute to the book. If you are interested in a larger contribution, please send me\n",
      "a message with your concrete idea: christoph.molnar.ai@gmail.com¹²⁰. [{'source': './data/xai.pdf', 'start_index': 489056}]\n",
      "* Another approach is called breakDown, which is implemented in the breakDown R package⁹³.\n",
      "BreakDown also shows the contributions of each feature to the prediction, but computes them step\n",
      "by step. Let us reuse the game analogy: We start with an empty team, add the feature value that\n",
      "would contribute the most to the prediction and iterate until all feature values are added. How\n",
      "much each feature value contributes depends on the respective feature values that are already in\n",
      "the “team”, which is the big drawback of the breakDown method. It is faster than the Shapley value\n",
      "method, and for models without interactions, the results are the same.\n",
      "\n",
      "⁹³Staniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and breakDown packages.” arXiv preprint\n",
      "\n",
      "arXiv:1804.01955 (2018).\n",
      "\n",
      "\fExample-Based Explanations [{'source': './data/xai.pdf', 'start_index': 370345}]\n",
      "* In our second example we want to explain a model that predicts a continuous outcome with\n",
      "counterfactual explanations. Anna wants to rent out her apartment, but she is not sure how much\n",
      "to charge for it, so she decides to train a machine learning model to predict the rent. Of course,\n",
      "since Anna is a data scientist, that is how she solves her problems. After entering all the details\n",
      "about size, location, whether pets are allowed and so on, the model tells her that she can charge\n",
      "900 Euro. She expected 1000 Euro or more, but she trusts her model and decides to play with the\n",
      "feature values of the apartment to see how she can improve the value of the apartment. She finds\n",
      "out that the apartment could be rented out for over 1000 Euro, if it were 15 m² larger. Interesting, but\n",
      "non-actionable knowledge, because she cannot enlarge her apartment. Finally, by tweaking only the\n",
      "feature values under her control (built-in kitchen yes/no, pets allowed yes/no, type of floor, etc.), [{'source': './data/xai.pdf', 'start_index': 380148}]\n",
      "* An already visible trend is the automation of model training. That includes automated engineering\n",
      "and selection of features, automated hyperparameter optimization, comparison of different models,\n",
      "and ensembling or stacking of the models. The result is the best possible prediction model. When\n",
      "we use model-agnostic interpretation methods, we can automatically apply them to any model that\n",
      "emerges from the automated machine learning process. In a way, we can automate this second step\n",
      "as well: Automatically compute the feature importance, plot the partial dependence, train a surrogate\n",
      "model, and so on. Nobody stops you from automatically computing all these model interpretations.\n",
      "The actual interpretation still requires people. Imagine: You upload a dataset, specify the prediction\n",
      "goal and at the push of a button the best prediction model is trained and the program spits out all\n",
      "interpretations of the model. There are already first products and I argue that for many applications [{'source': './data/xai.pdf', 'start_index': 483259}]\n",
      "* Interpretable Models\n",
      "\n",
      "84\n",
      "\n",
      "are very simple and general, because for each split the instance falls into either one or the other leaf,\n",
      "and binary decisions are easy to understand.\n",
      "\n",
      "There is no need to transform features. In linear models, it is sometimes necessary to take the\n",
      "logarithm of a feature. A decision tree works equally well with any monotonic transformation of a\n",
      "feature.\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "Trees fail to deal with linear relationships. Any linear relationship between an input feature and\n",
      "the outcome has to be approximated by splits, creating a step function. This is not efficient. [{'source': './data/xai.pdf', 'start_index': 179001}]\n",
      "* A Naive Bayes model with many hundreds of features would be too big for me and you to keep in\n",
      "our working memory. And even if we manage to memorize all the weights, we would not be able to\n",
      "quickly make predictions for new data points. In addition, you need to have the joint distribution of\n",
      "all features in your head to estimate the importance of each feature and how the features affect the\n",
      "predictions on average. An impossible task. But you can easily understand a single weight. While\n",
      "global model interpretability is usually out of reach, there is a good chance of understanding at least\n",
      "some models on a modular level. Not all models are interpretable at a parameter level. For linear\n",
      "models, the interpretable parts are the weights, for trees it would be the splits (selected features plus\n",
      "cut-off points) and leaf node predictions. Linear models, for example, look like as if they could be [{'source': './data/xai.pdf', 'start_index': 55545}]\n",
      "* The recipe for producing the counterfactuals is simple:\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "195\n",
      "\n",
      "1. Select an instance x to be explained, the desired outcome y’, a tolerance ϵ and a (low) initial\n",
      "\n",
      "value for (cid:21).\n",
      "\n",
      "2. Sample a random instance as initial counterfactual.\n",
      "3. Optimize the loss with the initially sampled counterfactual as starting point.\n",
      "4. While j ^f (x′) (cid:0) y′j > ϵ:\n",
      "• Increase (cid:21).\n",
      "• Optimize the loss with the current counterfactual as starting point.\n",
      "• Return the counterfactual that minimizes the loss.\n",
      "\n",
      "5. Repeat steps 2-3 and return the list of counterfactuals or the one that minimizes the loss.\n",
      "\n",
      "Examples\n",
      "\n",
      "Both examples are from the work of Wachter et. al (2017). [{'source': './data/xai.pdf', 'start_index': 388812}]\n",
      "* Speaking of criteria, how do we define a good counterfactual explanation? First, the user of\n",
      "a counterfactual explanation defines a relevant change in the prediction of an instance (= the\n",
      "alternative reality), so an obvious first requirement is that a counterfactual instance produces\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "193 [{'source': './data/xai.pdf', 'start_index': 382283}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "197\n",
      "\n",
      "have multiple counterfactual explanations, because then humans can select the ones that correspond\n",
      "to their previous knowledge.\n",
      "\n",
      "There is no guarantee that for a given tolerance ϵ a counterfactual instance is found. That is\n",
      "not necessarily the fault of the method, but rather depends on the data.\n",
      "\n",
      "The proposed method does not handle categorical features with many different levels well. The\n",
      "authors of the method suggested running the method separately for each combination of feature\n",
      "values of the categorical features, but this will lead to a combinatorial explosion if you have multiple\n",
      "categorical features with many values. For example, 6 categorical features with 10 unique levels\n",
      "would mean 1 million runs. A solution for only categorical features was proposed by Martens et. al\n",
      "(2014)⁹⁷. A good solution would be to use an optimizer that solves problems with a mix of continuous\n",
      "and discrete inputs. [{'source': './data/xai.pdf', 'start_index': 394388}]\n",
      "* Interpretable Models\n",
      "\n",
      "Step 1: Rule generation\n",
      "\n",
      "103\n",
      "\n",
      "What does a rule look like? The rules generated by the algorithm have a simple form. For example:\n",
      "IF x2 < 3 AND x5 < 7 THEN 1 ELSE 0. The rules are constructed by decomposing decision trees:\n",
      "Any path to a node in a tree can be converted to a decision rule. The trees used for the rules are\n",
      "fitted to predict the target outcome. Therefore the splits and resulting rules are optimized to predict\n",
      "the outcome you are interested in. You simply chain the binary decisions that lead to a certain node\n",
      "with “AND”, and voilÃ , you have a rule. It is desirable to generate a lot of diverse and meaningful\n",
      "rules. Gradient boosting is used to fit an ensemble of decision trees by regressing or classifying y\n",
      "with your original features X. Each resulting tree is converted into multiple rules. Not only boosted\n",
      "trees, but any tree ensemble algorithm can be used to generate the trees for RuleFit. A tree ensemble\n",
      "can be described with this general formula: [{'source': './data/xai.pdf', 'start_index': 224935}]\n",
      "* Friedman, Jerome H. “Greedy function approximation: A gradient boosting machine.” Annals of\n",
      "statistics (2001): 1189-1232.\n",
      "\n",
      "Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. “The elements of statistical learning”.\n",
      "www.web.stanford.edu/(cid:24)hastie/ElemStatLearn/ (2009).\n",
      "\n",
      "Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. “Foundations of rule learning.” Springer\n",
      "Science & Business Media, (2012).\n",
      "\n",
      "Goldstein, Alex, et al. “Package ‘ICEbox’.” (2017).\n",
      "\n",
      "Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adversarial\n",
      "examples.” arXiv preprint arXiv:1412.6572 (2014).\n",
      "\n",
      "Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. “A simple and effective\n",
      "model-based variable importance measure.” arXiv preprint arXiv:1805.04755 (2018).\n",
      "\n",
      "Heider, Fritz, and Marianne Simmel. “An experimental study of apparent behavior.” The American\n",
      "Journal of Psychology 57 (2). JSTOR: 243–59. (1944). [{'source': './data/xai.pdf', 'start_index': 496170}]\n",
      "* Follow me on Twitter! @ChristophMolnar²\n",
      "\n",
      "Cover by @YvonneDoinel³\n",
      "\n",
      "¹https://christophm.github.io/\n",
      "²https://twitter.com/ChristophMolnar\n",
      "³https://twitter.com/YvonneDoinel\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "This book explains to you how to make (supervised) machine learning models interpretable. The\n",
      "chapters contain some mathematical formulas, but you should be able to understand the ideas behind\n",
      "the methods even without the formulas. This book is not for people trying to learn machine learning\n",
      "from scratch. If you are new to machine learning, there are a lot of books and other resources to\n",
      "learn the basics. I recommend the book “The Elements of Statistical Learning” by Hastie, Tibshirani,\n",
      "and Friedman (2009) ⁴ and Andrew Ng’s “Machine Learning” online course⁵ on the online learning\n",
      "platform coursera.com to start with machine learning. Both the book and the course are available\n",
      "free of charge! [{'source': './data/xai.pdf', 'start_index': 7398}]\n",
      "* • k-nearest neighbors model: An (interpretable) machine learning model based on examples.\n",
      "\n",
      "⁹⁵Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.”\n",
      "\n",
      "Advances in Neural Information Processing Systems (2016).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "191\n",
      "\n",
      "Counterfactual Explanations\n",
      "\n",
      "A counterfactual explanation describes a causal situation in the form: “If X had not occurred, Y\n",
      "would not have occurred”. For example: “If I hadn’t taken a sip of this hot coffee, I wouldn’t have\n",
      "burned my tongue”. Event Y is that I burned my tongue; cause X is that I had a hot coffee. Thinking\n",
      "in counterfactuals requires imagining a hypothetical reality that contradicts the observed facts (e.g.\n",
      "a world in which I have not drunk the hot coffee), hence the name “counterfactual”. The ability to\n",
      "think in counterfactuals makes us humans so smart compared to other animals. [{'source': './data/xai.pdf', 'start_index': 376138}]\n",
      "* The explanations are created with 2 features. The results of the sparse local linear models trained\n",
      "for two instances with different predicted classes:\n",
      "\n",
      "LIME explanations for two instances of the bike rental dataset. Warmer temperature and good weather situation have\n",
      "a positive effect on the prediction. The x-axis shows the feature effect: The weight times the actual feature value.\n",
      "\n",
      "From the figure it becomes clear that it is easier to interpret categorical features than numerical\n",
      "features. One solution is to categorize the numerical features into bins.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "LIME for Text\n",
      "\n",
      "173\n",
      "\n",
      "LIME for text differs from LIME for tabular data. Variations of the data are generated differently:\n",
      "Starting from the original text, new texts are created by randomly removing words from the original\n",
      "text. The dataset is represented with binary features for each word. A feature is 1 if the corresponding\n",
      "word is included and 0 if it has been removed.\n",
      "\n",
      "Example [{'source': './data/xai.pdf', 'start_index': 340909}]\n",
      "* We could also build a surrogate model based on a subset of the original data or reweight the instances.\n",
      "In this way, we change the distribution of the surrogate model’s input, which changes the focus of\n",
      "the interpretation (then it is no longer really global). If we weight the data locally by a specific\n",
      "instance of the data (the closer the instances to the selected instance, the higher their weight), we\n",
      "get a local surrogate model that can explain the individual prediction of the instance. Read more\n",
      "about local models in the following chapter.\n",
      "\n",
      "Example\n",
      "\n",
      "To demonstrate the surrogate models, we consider a regression and a classification example.\n",
      "\n",
      "First, we train a support vector machine to predict the daily number of rented bikes given weather\n",
      "and calendar information. The support vector machine is not very interpretable, so we train a\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "165\n",
      "\n",
      "surrogate with a CART decision tree as interpretable model to approximate the behavior of the\n",
      "support vector machine. [{'source': './data/xai.pdf', 'start_index': 327650}]\n",
      "* Example-based explanations are mostly model-agnostic, because they make any machine learning\n",
      "model more interpretable. The difference to model-agnostic methods is that the example-based\n",
      "methods explain a model by selecting instances of the dataset and not by creating summaries\n",
      "of features (such as feature importance or partial dependence). Example-based explanations only\n",
      "make sense if we can represent an instance of the data in a humanly understandable way. This\n",
      "works well for images, because we can view them directly. In general, example-based methods work\n",
      "well if the feature values of an instance carry more context, meaning the data has a structure, like\n",
      "images or texts do. It is more challenging to represent tabular data in a meaningful way, because an\n",
      "instance can consist of hundreds or thousands of (less structured) features. Listing all feature values\n",
      "to describe an instance is usually not useful. It works well if there are only a handful of features or [{'source': './data/xai.pdf', 'start_index': 371352}]\n",
      "* A big thanks goes to Verena Haunschmid for writing the section about LIME explanations for images.\n",
      "She works in data science and I recommend following her on Twitter: @ExpectAPatronum¹²¹.\n",
      "I also want to thank all the early readers who contributed corrections¹²² on Github!\n",
      "\n",
      "Furthermore, I want to thank everyone who created illustrations: The cover was designed by\n",
      "my friend @YvonneDoinel¹²³. The graphics in the Shapley Value chapter were created by Abi\n",
      "Aryan¹²⁴, using icons made by Freepik¹²⁵ from Flaticon¹²⁶. The awesome frog with the crystal ball¹²⁷\n",
      "in the chapter about the Future of Interpretability was designed by @TopeconHeroes¹²⁸. Verena\n",
      "Haunschmid created the graphic in the RuleFit chapter. I would like to thank all researchers who\n",
      "allowed me to use images from their research articles. [{'source': './data/xai.pdf', 'start_index': 491544}]\n",
      "* Interpretable Models\n",
      "\n",
      "47\n",
      "\n",
      "The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest.\n",
      "\n",
      "If we average the predictions for the training data instances, we get an average of 4504. In\n",
      "comparison, the prediction of the 6-th instance is small, since only 1571 bicycle rents are predicted.\n",
      "The effect plot reveals the reason why. The boxplots show the distributions of the effects for all\n",
      "instances of the dataset, the red crosses show the effects for the 6-th instance. The 6-th instance has\n",
      "a low temperature effect because on this day the temperature was 2 degrees, which is low compared\n",
      "to most other days (and remember that the weight of the temperature feature is positive). Also, the\n",
      "effect of the trend feature “days_since_2011” is small compared to the other data instances because\n",
      "this instance is from early 2011 (5 days) and the trend feature also has a positive weight.\n",
      "\n",
      "Encoding of Categorical Features [{'source': './data/xai.pdf', 'start_index': 111130}]\n",
      "* ¹⁰²Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep neural networks.” IEEE Transactions on\n",
      "\n",
      "Evolutionary Computation (2019).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "203\n",
      "\n",
      "and creates a new generation of candidate solutions (children) from the parent generation using the\n",
      "following formula:\n",
      "\n",
      "xi(g + 1) = xr1(g) + F (cid:1) (xr2(g) + xr3(g))\n",
      "\n",
      "where each xi is an element of a candidate solution (either x-coordinate, y-coordinate, red, green or\n",
      "blue), g is the current generation, F is a scaling parameter (set to 0.5) and r1, r2 and r3 are different\n",
      "random numbers. Each new child candidate solution is in turn a pixel with the five attributes for\n",
      "location and color and each of those attributes is a mixture of three random parent pixels.\n",
      "\n",
      "The creation of children is stopped if one of the candidate solutions is an adversarial example,\n",
      "meaning it is classified as an incorrect class, or if the number of maximum iterations specified by\n",
      "the user is reached. [{'source': './data/xai.pdf', 'start_index': 405946}]\n",
      "* ⁵⁸Fokkema, Marjolein, and Benjamin Christoffersen. “Pre: Prediction rule ensembles”. https://CRAN.R-project.org/package=pre (2017).\n",
      "⁵⁹https://github.com/christophM/rulefit\n",
      "⁶⁰https://github.com/scikit-learn-contrib/skope-rules\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "108\n",
      "\n",
      "Other Interpretable Models\n",
      "\n",
      "The list of interpretable models is constantly growing and of unknown size. It includes simple models\n",
      "such as linear models, decision trees and naive Bayes, but also more complex ones that combine or\n",
      "modify non-interpretable machine learning models to make them more interpretable. Especially\n",
      "publications on the latter type of models are currently being produced at high frequency and it is\n",
      "hard to keep up with developments. The book teases only the Naive Bayes classifier and k-nearest\n",
      "neighbors in this chapter.\n",
      "\n",
      "Naive Bayes Classifier [{'source': './data/xai.pdf', 'start_index': 235063}]\n",
      "* The process of integrating machines and algorithms into our daily lives requires interpretability to\n",
      "increase social acceptance. People attribute beliefs, desires, intentions and so on to objects. In a\n",
      "famous experiment, Heider and Simmel (1944) ¹³ showed participants videos of shapes in which a\n",
      "\n",
      "¹³Heider, Fritz, and Marianne Simmel. “An experimental study of apparent behavior.” The American Journal of Psychology 57 (2). JSTOR:\n",
      "\n",
      "243–59. (1944).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "18 [{'source': './data/xai.pdf', 'start_index': 40278}]\n",
      "* j ^f (x′) (cid:0) y′j (cid:20) ϵ\n",
      "\n",
      "To minimize this loss function, any suitable optimization algorithm can be used, e.g. Nelder-Mead. If\n",
      "you have access to the gradients of the machine learning model, you can use gradient-based methods\n",
      "like ADAM. The instance x to be explained, the desired output y’ and the tolerance parameter ϵ must\n",
      "be set in advance. The loss function is minimized for x’ and the (locally) optimal counterfactual\n",
      "x’ returned while increasing (cid:21) until a sufficiently close solution is found (= within the tolerance\n",
      "parameter).\n",
      "\n",
      "arg min\n",
      "\n",
      "x′ max\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "L(x; x′; y′; (cid:21))\n",
      "\n",
      "The function d for measuring the distance between instance x and counterfactual x’ is the Manhattan\n",
      "distance weighted feature-wise with the inverse median absolute deviation (MAD).\n",
      "\n",
      "d(x; x′) =\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "j\n",
      "\n",
      "jxj (cid:0) x′\n",
      "j\n",
      "M ADj [{'source': './data/xai.pdf', 'start_index': 386899}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "218\n",
      "\n",
      "Influential Instances\n",
      "\n",
      "Machine learning models are ultimately a product of training data and deleting one of the training\n",
      "instances can affect the resulting model. We call a training instance “influential” when its deletion\n",
      "from the training data considerably changes the parameters or predictions of the model. By\n",
      "identifying influential training instances, we can “debug” machine learning models and better\n",
      "explain their behaviors and predictions.\n",
      "\n",
      "This chapter shows you two approaches for identifying influential instances, namely deletion\n",
      "diagnostics and influence functions. Both approaches are based on robust statistics, which provides\n",
      "statistical methods that are less affected by outliers or violations of model assumptions. Robust\n",
      "statistics also provides methods to measure how robust estimates from data are (such as a mean\n",
      "estimate or the weights of a prediction model). [{'source': './data/xai.pdf', 'start_index': 436522}]\n",
      "* The interpretable model you choose as a surrogate comes with all its advantages and disadvan-\n",
      "tages.\n",
      "\n",
      "Some people argue that there are, in general, no intrinsically interpretable models (including\n",
      "even linear models and decision trees) and that it would even be dangerous to have an illusion of\n",
      "interpretability. If you share this opinion, then of course this method is not for you.\n",
      "\n",
      "Software\n",
      "\n",
      "I used the iml R package for the examples. If you can train a machine learning model, then you should\n",
      "be able to implement surrogate models yourself. Simply train an interpretable model to predict the\n",
      "predictions of the black box model.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "168\n",
      "\n",
      "Local Surrogate (LIME) [{'source': './data/xai.pdf', 'start_index': 331764}]\n",
      "* A user of a house price prediction website might want to have an explanation of a house price\n",
      "prediction contrastive to their own house or maybe to another house on the website or maybe\n",
      "to an average house in the neighborhood. The solution for the automated creation of contrastive\n",
      "explanations might also involve finding prototypes or archetypes in the data. [{'source': './data/xai.pdf', 'start_index': 73586}]\n",
      "* ⁹⁴Aamodt, Agnar, and Enric Plaza. “Case-based reasoning: Foundational issues, methodological variations, and system approaches.” AI\n",
      "\n",
      "communications 7.1 (1994): 39-59.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "190 [{'source': './data/xai.pdf', 'start_index': 374147}]\n",
      "* ^f (x) = (cid:22)y +\n",
      "\n",
      "D∑\n",
      "\n",
      "d=1\n",
      "\n",
      "split.contrib(d,x) = (cid:22)y +\n",
      "\n",
      "feat.contrib(j,x)\n",
      "\n",
      "j=1\n",
      "\n",
      "p∑\n",
      "\n",
      "The prediction of an individual instance is the mean of the target outcome plus the sum of all\n",
      "contributions of the D splits that occur between the root node and the terminal node where\n",
      "the instance ends up. We are not interested in the split contributions though, but in the feature\n",
      "contributions. A feature might be used for more than one split or not at all. We can add the\n",
      "contributions for each of the p features and get an interpretation of how much each feature has\n",
      "contributed to a prediction.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "Example\n",
      "\n",
      "82\n",
      "\n",
      "Let us have another look at the bike rental data. We want to predict the number of rented bikes on\n",
      "a certain day with a decision tree. The learned tree looks like this: [{'source': './data/xai.pdf', 'start_index': 176118}]\n",
      "* Advantages\n",
      "\n",
      "Even if you replace the underlying machine learning model, you can still use the same local,\n",
      "interpretable model for explanation. Suppose the people looking at the explanations understand\n",
      "decision trees best. Because you use local surrogate models, you use decision trees as explanations\n",
      "without actually having to use a decision tree to make the predictions. For example, you can use a\n",
      "SVM. And if it turns out that an xgboost model works better, you can replace the SVM and still use\n",
      "as decision tree to explain the predictions.\n",
      "\n",
      "Local surrogate models benefit from the literature and experience of training and interpreting\n",
      "interpretable models. [{'source': './data/xai.pdf', 'start_index': 345711}]\n",
      "* The counterfactual method does not require access to the data or the model. It only requires\n",
      "access to the model’s prediction function, which would also work via a web API, for example. This\n",
      "is attractive for companies which are audited by third parties or which are offering explanations\n",
      "for users without disclosing the model or data. A company has an interest in protecting model and\n",
      "data because of trade secrets or data protection reasons. Counterfactual explanations offer a balance\n",
      "between explaining model predictions and protecting the interests of the model owner.\n",
      "\n",
      "The method works also with systems that do not use machine learning. We can create\n",
      "counterfactuals for any system that receives inputs and returns outputs. The system that predicts\n",
      "apartment rents could also consist of handwritten rules, and counterfactual explanations would still\n",
      "work. [{'source': './data/xai.pdf', 'start_index': 392701}]\n",
      "* A linear model with one feature. Trained once on the full data and once without the influential instance. Removing\n",
      "the influential instance changes the fitted slope (weight/coefficient) drastically.\n",
      "\n",
      "Why do influential instances help to understand the model?\n",
      "\n",
      "The key idea behind influential instances for interpretability is to trace model parameters and\n",
      "predictions back to where it all began: the training data. A learner, that is, the algorithm that\n",
      "generates the machine learning model, is a function that takes training data consisting of features\n",
      "X and target y and generates a machine learning model. For example, the learner of a decision tree\n",
      "is an algorithm that selects the split features and the values at which to split. A learner for a neural\n",
      "network uses backpropagation to find the best weights.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "221\n",
      "\n",
      "A learner learns a model from training data (features plus target). The model makes predictions for new data. [{'source': './data/xai.pdf', 'start_index': 440393}]\n",
      "* Fixing training data\n",
      "\n",
      "If you have a limit on how many training instances you can check for correctness, how do you make\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "231 [{'source': './data/xai.pdf', 'start_index': 465823}]\n",
      "* Computer and Communications Security. ACM (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "206\n",
      "\n",
      "5. Repeat steps 2 to 4 for a predefined number of epochs.\n",
      "6. Create adversarial examples for the surrogate model using the fast gradient method (or similar).\n",
      "7. Attack the original model with adversarial examples.\n",
      "\n",
      "The aim of the surrogate model is to approximate the decision boundaries of the black box model,\n",
      "but not necessarily to achieve the same accuracy. [{'source': './data/xai.pdf', 'start_index': 412389}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "167\n",
      "\n",
      "With the R-squared measure, we can easily measure how good our surrogate models are in\n",
      "approximating the black box predictions.\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "You have to be aware that you draw conclusions about the model and not about the data, since\n",
      "the surrogate model never sees the real outcome.\n",
      "\n",
      "It is not clear what the best cut-off for R-squared is in order to be confident that the surrogate\n",
      "model is close enough to the black box model. 80% of variance explained? 50%? 99%?\n",
      "\n",
      "We can measure how close the surrogate model is to the black box model. Let us assume we are\n",
      "not very close, but close enough. It could happen that the interpretable model is very close for one\n",
      "subset of the dataset, but widely divergent for another subset. In this case the interpretation for\n",
      "the simple model would not be equally good for all data points.\n",
      "\n",
      "The interpretable model you choose as a surrogate comes with all its advantages and disadvan-\n",
      "tages. [{'source': './data/xai.pdf', 'start_index': 330906}]\n",
      "* Being proactive means actively testing and identifying weak points of the system. You are proactive\n",
      "when you actively try to deceive the model with adversarial examples and then defend against\n",
      "\n",
      "¹⁰⁶Biggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine learning.” Pattern Recognition 84 (2018):\n",
      "\n",
      "317-331.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "207 [{'source': './data/xai.pdf', 'start_index': 415248}]\n",
      "* Heider, Fritz, and Marianne Simmel. “An experimental study of apparent behavior.” The American\n",
      "Journal of Psychology 57 (2). JSTOR: 243–59. (1944).\n",
      "\n",
      "Holte, Robert C. “Very simple classification rules perform well on most commonly used datasets.”\n",
      "Machine learning 11.1 (1993): 63-90.\n",
      "\n",
      "Hooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM\n",
      "SIGKDD international conference on Knowledge discovery and data mining. (2004).\n",
      "\n",
      "Kahneman, Daniel, and Amos Tversky. “The Simulation Heuristic.” Stanford Univ CA Dept of\n",
      "Psychology. (1981).\n",
      "\n",
      "Kaufman, Leonard, and Peter Rousseeuw. “Clustering by means of medoids”. North-Holland (1987).\n",
      "\n",
      "Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize!\n",
      "Criticism for interpretability.” Advances in Neural Information Processing Systems (2016).\n",
      "\n",
      "Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.”\n",
      "arXiv preprint arXiv:1703.04730 (2017). [{'source': './data/xai.pdf', 'start_index': 496958}]\n",
      "* Interpretable Models\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "53\n",
      "\n",
      "Linear regression models can only represent linear relationships, i.e. a weighted sum of the input\n",
      "features. Each nonlinearity or interaction has to be hand-crafted and explicitly given to the model\n",
      "as an input feature.\n",
      "\n",
      "Linear models are also often not that good regarding predictive performance, because the\n",
      "relationships that can be learned are so restricted and usually oversimplify how complex reality\n",
      "is. [{'source': './data/xai.pdf', 'start_index': 123157}]\n",
      "* Of course this graphic does not capture everything: Data could come from simulations. Black box\n",
      "models also output predictions that might not even reach humans, but only supply other machines,\n",
      "and so on. But overall it is a useful abstraction to understand how interpretability becomes this new\n",
      "layer on top of machine learning models.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "113\n",
      "\n",
      "Partial Dependence Plot (PDP)\n",
      "\n",
      "The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features\n",
      "have on the predicted outcome of a machine learning model (J. H. Friedman 2001⁶²). A partial\n",
      "dependence plot can show whether the relationship between the target and a feature is linear,\n",
      "monotonous or more complex. For example, when applied to a linear regression model, partial\n",
      "dependence plots always show a linear relationship.\n",
      "\n",
      "The partial dependence function for regression is defined as:\n",
      "\n",
      "^fxS (xS) = ExC\n",
      "\n",
      "[\n",
      "\n",
      "]\n",
      "^f (xS; xC)\n",
      "\n",
      "=\n",
      "\n",
      "∫\n",
      "\n",
      "^f (xS; xC)dP(xC) [{'source': './data/xai.pdf', 'start_index': 243268}]\n",
      "* Interpretable Models\n",
      "\n",
      "81\n",
      "\n",
      "by trying different groupings of categories. After the best cutoff per feature has been determined,\n",
      "the algorithm selects the feature for splitting that would result in the best partition in terms of the\n",
      "variance or Gini index and adds this split to the tree. The algorithm continues this search-and-split\n",
      "recursively in both new nodes until a stop criterion is reached. Possible criteria are: A minimum\n",
      "number of instances that have to be in a node before the split, or the minimum number of instances\n",
      "that have to be in a terminal node.\n",
      "\n",
      "Interpretation\n",
      "\n",
      "The interpretation is simple: Starting from the root node, you go to the next nodes and the edges tell\n",
      "you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted\n",
      "outcome. All the edges are connected by ‘AND’.\n",
      "\n",
      "Template: If feature x is [smaller/bigger] than threshold c AND … then the predicted outcome is the\n",
      "mean value of y of the instances in that node. [{'source': './data/xai.pdf', 'start_index': 174058}]\n",
      "* Different machine learning models have different ways of making predictions. Even if two models\n",
      "have the same performance, the way they make predictions from the features can be very different\n",
      "and therefore fail in different scenarios. Understanding the particular weaknesses of a model by\n",
      "identifying influential instances helps to form a “mental model” of the machine learning model\n",
      "behavior in your mind. The following figure shows an example where a support vector machine\n",
      "(SVM) and a neural network were trained to distinguish images of dogs and fish. The most influential\n",
      "instances of an exemplary image of a fish were very different for both models. For the SVM, instances\n",
      "were influential if they were similar in color. For the neural network, instances were influential if\n",
      "they were conceptually similar. For the neural network, even one image of a dog was among the\n",
      "most influential images, showing that it learned the concepts and not the Euclidean distance in color [{'source': './data/xai.pdf', 'start_index': 463086}]\n",
      "* Interpretable Models\n",
      "\n",
      "86\n",
      "\n",
      "Usually there is a trade-off between accuracy and support: By adding more features to the condition,\n",
      "we can achieve higher accuracy, but lose support.\n",
      "\n",
      "To create a good classifier for predicting the value of a house you might need to learn not only one\n",
      "rule, but maybe 10 or 20. Then things can get more complicated and you can run into one of the\n",
      "following problems:\n",
      "\n",
      "• Rules can overlap: What if I want to predict the value of a house and two or more rules apply\n",
      "\n",
      "and they give me contradictory predictions?\n",
      "\n",
      "• No rule applies: What if I want to predict the value of a house and none of the rules apply?\n",
      "\n",
      "There are two main strategies for combining multiple rules: Decision lists (ordered) and decision\n",
      "sets (unordered). Both strategies imply different solutions to the problem of overlapping rules. [{'source': './data/xai.pdf', 'start_index': 184710}]\n",
      "* Something is Wrong With My Dog\n",
      "\n",
      "Szegedy et. al (2013)¹⁰⁰ used a gradient based optimization approach in their work “Intriguing\n",
      "Properties of Neural Networks” to find adversarial examples for deep neural networks.\n",
      "\n",
      "¹⁰⁰Szegedy, Christian, et al. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199 (2013).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "200\n",
      "\n",
      "Adversarial examples for AlexNet by Szegedy et. al (2013). All images in the left column are correctly classified.\n",
      "The middle column shows the (magnified) error added to the images to produce the images in the right column all\n",
      "categorized (incorrectly) as ‘Ostrich’.\n",
      "\n",
      "These adversarial examples were generated by minimizing the following function with respect to r:\n",
      "\n",
      "loss( ^f (x + r); l) + c (cid:1) jrj [{'source': './data/xai.pdf', 'start_index': 400253}]\n",
      "* Apart from model debugging, can we learn something to better understand the model? Just printing\n",
      "out the top 10 most influential instances is not very useful, because it is just a table of instances with\n",
      "many features. All methods that return instances as output only make sense if we have a good way\n",
      "of representing them. But we can better understand what kind of instances are influential when\n",
      "we ask: What distinguishes an influential instance from a non-influential instance? We can turn\n",
      "this question into a regression problem and model the influence of an instance as a function of its\n",
      "feature values. We are free to choose any model from the chapter on Interpretable Machine Learning\n",
      "Models. For this example I chose a decision tree (following figure) that shows that data from women\n",
      "of age 35 and older were the most influential for the support vector machine. Of all the women in\n",
      "the dataset 153 out of 858 were older than 35. In the chapter on Partial Dependence Plots we have [{'source': './data/xai.pdf', 'start_index': 448389}]\n",
      "* • Data point: This category includes all methods that return data points (already existent or\n",
      "newly created) to make a model interpretable. One method is called counterfactual explana-\n",
      "tions. To explain the prediction of a data instance, the method finds a similar data point by\n",
      "changing some of the features for which the predicted outcome changes in a relevant way (e.g.\n",
      "a flip in the predicted class). Another example is the identification of prototypes of predicted\n",
      "classes. To be useful, interpretation methods that output new data points require that the data\n",
      "points themselves can be interpreted. This works well for images and texts, but is less useful\n",
      "for tabular data with hundreds of features.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "22 [{'source': './data/xai.pdf', 'start_index': 50902}]\n",
      "* But a simple weighted sum is too restrictive for many real world prediction problems. In this chapter\n",
      "we will learn about three problems of the classical linear regression model and how to solve them.\n",
      "There are many more problems with possibly violated assumptions, but we will focus on the three\n",
      "shown in the following figure:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "62\n",
      "\n",
      "Three assumptions of the linear model (left side): Gaussian distribution of the outcome given the features, additivity (=\n",
      "no interactions) and linear relationship. Reality usually does not adhere to those assumptions (right side): Outcomes\n",
      "might have non-Gaussian distributions, features might interact and the relationship might be nonlinear.\n",
      "\n",
      "There is a solution to all these problems: [{'source': './data/xai.pdf', 'start_index': 137639}]\n",
      "* Koh and Liang (2017)¹¹² suggested using influence functions, a method of robust statistics, to measure\n",
      "how an instance influences model parameters or predictions. As with deletion diagnostics, the\n",
      "influence functions trace the model parameters and predictions back to the responsible training\n",
      "instance. However, instead of deleting training instances, the method approximates how much the\n",
      "model changes when the instance is upweighted in the empirical risk (sum of the loss over the\n",
      "training data).\n",
      "\n",
      "The method of influence functions requires access to the loss gradient with respect to the model\n",
      "parameters, which only works for a subset of machine learning models. Logistic regression, neural\n",
      "networks and support vector machines qualify, tree-based methods like random forests do not.\n",
      "Influence functions help to understand the model behavior, debug the model and detect errors in\n",
      "the dataset.\n",
      "\n",
      "The following section explains the intuition and math behind influence functions. [{'source': './data/xai.pdf', 'start_index': 454602}]\n",
      "* Let us remember the formula of a linear regression model:\n",
      "\n",
      "y = (cid:12)0 + (cid:12)1x1 + : : : + (cid:12)pxp + ϵ\n",
      "\n",
      "The linear regression model assumes that the outcome y of an instance can be expressed by a\n",
      "weighted sum of its p features with an individual error ϵ that follows a Gaussian distribution. By\n",
      "forcing the data into this corset of a formula, we obtain a lot of model interpretability. The feature\n",
      "effects are additive, meaning no interactions, and the relationship is linear, meaning an increase of\n",
      "a feature by one unit can be directly translated into an increase/decrease of the predicted outcome.\n",
      "The linear model allows us to compress the relationship between a feature and the expected outcome\n",
      "into a single number, namely the estimated weight. [{'source': './data/xai.pdf', 'start_index': 136877}]\n",
      "* Machine learning automates the modeling process and moves the human a bit further away from the\n",
      "data and the underlying task: This increases the risk of problems with experimental design, choice of\n",
      "training distribution, sampling, data encoding, feature engineering, and so on. Interpretation tools\n",
      "make it easier to identify these problems. [{'source': './data/xai.pdf', 'start_index': 482144}]\n",
      "* course, humans do not need explanations for everything that happens. For most people it is okay that\n",
      "they do not understand how a computer works. Unexpected events makes us curious. For example:\n",
      "Why is my computer shutting down unexpectedly? [{'source': './data/xai.pdf', 'start_index': 36221}]\n",
      "* Everything is a toaster: Adversarial patch\n",
      "\n",
      "One of my favorite methods brings adversarial examples into physical reality. Brown et. al (2017)¹⁰³\n",
      "designed a printable label that can be stuck next to objects to make them look like toasters for an\n",
      "image classifier. Brilliant work!\n",
      "\n",
      "A sticker that makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by\n",
      "Brown et. al (2017).\n",
      "\n",
      "This method differs from the methods presented so far for adversarial examples, since the restriction\n",
      "that the adversarial image must be very close to the original image is removed. Instead, the method\n",
      "completely replaces a part of the image with a patch that can take on any shape. The image of the\n",
      "patch is optimized over different background images, with different positions of the patch on the\n",
      "images, sometimes moved, sometimes larger or smaller and rotated, so that the patch works in many\n",
      "\n",
      "¹⁰³Brown, Tom B., et al. “Adversarial patch.” arXiv preprint arXiv:1712.09665 (2017). [{'source': './data/xai.pdf', 'start_index': 406941}]\n",
      "* Prototypes and criticisms for a handwritten digits dataset.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "Advantages\n",
      "\n",
      "216\n",
      "\n",
      "In a user study the authors of MMD-critic gave images to the participants, which they had to visually\n",
      "match to one of two sets of images, each representing one of two classes (e.g. two dog breeds). The\n",
      "participants performed best when the sets showed prototypes and criticisms instead of random\n",
      "images of a class.\n",
      "\n",
      "You are free to choose the number of prototypes and criticisms.\n",
      "\n",
      "MMD-critic works with density estimates of the data. This works with any type of data and any\n",
      "type of machine learning model.\n",
      "\n",
      "The algorithm is easy to implement.\n",
      "\n",
      "MMD-critic is very flexible in the way it is used to increase interpretability. It can be used to\n",
      "understand complex data distributions. It can be used to build an interpretable machine learning\n",
      "model. Or it can shed light on the decision making of a black box machine learning model. [{'source': './data/xai.pdf', 'start_index': 433244}]\n",
      "* cut-off points) and leaf node predictions. Linear models, for example, look like as if they could be\n",
      "perfectly interpreted on a modular level, but the interpretation of a single weight is interlocked with\n",
      "all other weights. The interpretation of a single weight always comes with the footnote that the\n",
      "other input features remain at the same value, which is not the case with many real applications. A\n",
      "linear model that predicts the value of a house, that takes into account both the size of the house and\n",
      "the number of rooms, can have a negative weight for the room feature. It can happen because there\n",
      "is already the highly correlated house size feature. In a market where people prefer larger rooms, a\n",
      "house with fewer rooms could be worth more than a house with more rooms if both have the same\n",
      "size. The weights only make sense in the context of the other features in the model. But the weights\n",
      "in a linear model can still be interpreted better than the weights of a deep neural network. [{'source': './data/xai.pdf', 'start_index': 56342}]\n",
      "* Example\n",
      "\n",
      "In this example we classify YouTube comments as spam or normal.\n",
      "\n",
      "The black box model is a deep decision tree trained on the document word matrix. Each comment\n",
      "is one document (= one row) and each column is the number of occurrences of a given word. Short\n",
      "decision trees are easy to understand, but in this case the tree is very deep. Also in place of this\n",
      "tree there could have been a recurrent neural network or a support vector machine trained on word\n",
      "embeddings (abstract vectors). Let us look at the two comments of this dataset and the corresponding\n",
      "classes (1 for spam, 0 for normal comment):\n",
      "\n",
      "CONTENT\n",
      "PSY is a good guy\n",
      "For Christmas Song visit my channel! ;)\n",
      "\n",
      "267\n",
      "173\n",
      "\n",
      "CLASS\n",
      "0\n",
      "1\n",
      "\n",
      "The next step is to create some variations of the datasets used in a local model. For example, some\n",
      "variations of one of the comments:\n",
      "\n",
      "For\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "\n",
      "Christmas\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "Song\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "visit my\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "channel!\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      ";)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "prob\n",
      "0.09\n",
      "0.09\n",
      "0.99\n",
      "0.99\n",
      "0.09 [{'source': './data/xai.pdf', 'start_index': 341868}]\n",
      "* Štrumbelj, Erik, and Igor Kononenko. “Explaining prediction models and individual predictions with\n",
      "feature contributions.” Knowledge and information systems 41.3 (2014): 647-665.\n",
      "\n",
      "\fReferences\n",
      "\n",
      "246\n",
      "\n",
      "R Packages Used for Examples\n",
      "\n",
      "base. R Core Team (2018). R: A language and environment for statistical computing. R Foundation\n",
      "for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\n",
      "\n",
      "data.table. Matt Dowle and Arun Srinivasan (2019). data.table: Extension of data.frame. R package\n",
      "version 1.12.1. http://r-datatable.com\n",
      "\n",
      "dplyr. Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2018). dplyr: A Grammar\n",
      "of Data Manipulation. R package version 0.7.8. https://CRAN.R-project.org/package=dplyr\n",
      "\n",
      "ggplot2. Hadley Wickham, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi,\n",
      "Claus Wilke and Kara Woo (2018). ggplot2: Create Elegant Data Visualisations Using the Grammar\n",
      "of Graphics. R package version 3.1.0. https://CRAN.R-project.org/package=ggplot2 [{'source': './data/xai.pdf', 'start_index': 501070}]\n",
      "* Model-specific or model-agnostic? Model-specific interpretation tools are limited to specific model\n",
      "classes. The interpretation of regression weights in a linear model is a model-specific interpretation,\n",
      "since – by definition – the interpretation of intrinsically interpretable models is always model-\n",
      "specific. Tools that only work for the interpretation of e.g. neural networks are model-specific.\n",
      "Model-agnostic tools can be used on any machine learning model and are applied after the model\n",
      "has been trained (post hoc). These agnostic methods usually work by analyzing feature input and\n",
      "output pairs. By definition, these methods cannot have access to model internals such as weights or\n",
      "structural information.\n",
      "\n",
      "Local or global? Does the interpretation method explain an individual prediction or the entire model\n",
      "behavior? Or is the scope somewhere in between? Read more about the scope criterion in the next\n",
      "section.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "23\n",
      "\n",
      "Scope of Interpretability [{'source': './data/xai.pdf', 'start_index': 51913}]\n",
      "* Human level evaluation (simple task) is a simplified application level evaluation. The difference is\n",
      "that these experiments are not carried out with the domain experts, but with laypersons. This makes\n",
      "experiments cheaper (especially if the domain experts are radiologists) and it is easier to find more\n",
      "testers. An example would be to show a user different explanations and the user would choose the\n",
      "best one.\n",
      "\n",
      "Function level evaluation (proxy task) does not require humans. This works best when the class of\n",
      "model used has already been evaluated by someone else in a human level evaluation. For example,\n",
      "it might be known that the end users understand decision trees. In this case, a proxy for explanation\n",
      "quality may be the depth of the tree. Shorter trees would get a better explainability score. It would\n",
      "make sense to add the constraint that the predictive performance of the tree remains good and does\n",
      "not decrease too much compared to a larger tree. [{'source': './data/xai.pdf', 'start_index': 59889}]\n",
      "* How can we find influential instances? We have two ways of measuring influence: Our first option\n",
      "is to delete the instance from the training data, retrain the model on the reduced training dataset\n",
      "and observe the difference in the model parameters or predictions (either individually or over\n",
      "the complete dataset). The second option is to upweight a data instance by approximating the\n",
      "parameter changes based on the gradients of the model parameters. The deletion approach is easier\n",
      "to understand and motivates the upweighting approach, so we start with the former.\n",
      "\n",
      "Deletion Diagnostics [{'source': './data/xai.pdf', 'start_index': 442438}]\n",
      "* Interpretable Models\n",
      "\n",
      "72\n",
      "\n",
      "case, as you will see, in many examples throughout the book. The temperature feature has a linear,\n",
      "positive effect on the number of rental bikes, but at some point it flattens out and even has a negative\n",
      "effect at high temperatures. The linear model does not care, it will dutifully find the best linear plane\n",
      "(by minimizing the Euclidean distance).\n",
      "\n",
      "You can model nonlinear relationships using one of the following techniques:\n",
      "\n",
      "• Simple transformation of the feature (e.g. logarithm)\n",
      "• Categorization of the feature\n",
      "• Generalized Additive Models (GAMs) [{'source': './data/xai.pdf', 'start_index': 157874}]\n",
      "* The aim of the surrogate model is to approximate the decision boundaries of the black box model,\n",
      "but not necessarily to achieve the same accuracy.\n",
      "\n",
      "The authors tested this approach by attacking image classifiers trained on various cloud machine\n",
      "learning services. These services train image classifiers on user uploaded images and labels. The\n",
      "software trains the model automatically – sometimes with an algorithm unknown to the user – and\n",
      "deploys it. The classifier then gives predictions for uploaded images, but the model itself cannot be\n",
      "inspected or downloaded. The authors were able to find adversarial examples for various providers,\n",
      "with up to 84% of the adversarial examples being misclassified.\n",
      "\n",
      "The method even works if the black box model to be deceived is not a neural network. This includes\n",
      "machine learning models without gradients such as a decision trees.\n",
      "\n",
      "The Cybersecurity Perspective [{'source': './data/xai.pdf', 'start_index': 412689}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "208\n",
      "\n",
      "Prototypes and Criticisms\n",
      "\n",
      "A prototype is a data instance that is representative of all the data. A criticism is a data instance\n",
      "that is not well represented by the set of prototypes. The purpose of criticisms is to provide insights\n",
      "together with prototypes, especially for data points which the prototypes do not represent well.\n",
      "Prototypes and criticisms can be used independently from a machine learning model to describe\n",
      "the data, but they can also be used to create an interpretable model or to make a black box model\n",
      "interpretable. [{'source': './data/xai.pdf', 'start_index': 418797}]\n",
      "* Interpretable Models\n",
      "\n",
      "96\n",
      "\n",
      "p(yjx; d; (cid:11))\n",
      "\n",
      "is the likelihood of the observed y, given the decision list and the data. BRL assumes that y is\n",
      "generated by a Dirichlet-Multinomial distribution. The better the decision list d explains the data,\n",
      "the higher the likelihood.\n",
      "\n",
      "p(djA; (cid:21); (cid:17))\n",
      "\n",
      "is the prior distribution of the decision lists. It multiplicatively combines a truncated Poisson\n",
      "distribution (parameter (cid:21)) for the number of rules in the list and a truncated Poisson distribution\n",
      "(parameter (cid:17)) for the number of feature values in the conditions of the rules.\n",
      "\n",
      "A decision list has a high posterior probability if it explains the outcome y well and is also likely\n",
      "according to the prior assumptions. [{'source': './data/xai.pdf', 'start_index': 208393}]\n",
      "* 5. Repeat steps 2-3 and return the list of counterfactuals or the one that minimizes the loss.\n",
      "\n",
      "Examples\n",
      "\n",
      "Both examples are from the work of Wachter et. al (2017).\n",
      "\n",
      "In the first example, the authors train a three-layer fully-connected neural network to predict a\n",
      "student’s average grade of the first year at law school, based on grade point average (GPA) prior to\n",
      "law school, race and law school entrance exam scores. The goal is to find counterfactual explanations\n",
      "for each student that answer the following question: How would the input features need to be\n",
      "changed, to get a predicted score of 0? Since the scores have been normalized before, a student\n",
      "with a score of 0 is as good as the average of the students. A negative score means a below-average\n",
      "result, a positive score an above-average result.\n",
      "\n",
      "The following table shows the learned counterfactuals:\n",
      "\n",
      "Score\n",
      "0.17\n",
      "0.54\n",
      "-0.77\n",
      "-0.83\n",
      "-0.57\n",
      "\n",
      "GPA\n",
      "3.1\n",
      "3.7\n",
      "3.3\n",
      "2.4\n",
      "2.7\n",
      "\n",
      "LSAT\n",
      "39.0\n",
      "48.0\n",
      "28.0\n",
      "28.5\n",
      "18.3\n",
      "\n",
      "Race\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "GPA x’\n",
      "3.1\n",
      "3.7\n",
      "3.3\n",
      "2.4\n",
      "2.7 [{'source': './data/xai.pdf', 'start_index': 389343}]\n",
      "* Machine learning models take on real-world tasks that require safety measures and testing. Imagine\n",
      "a self-driving car automatically detects cyclists based on a deep learning system. You want to be 100%\n",
      "sure that the abstraction the system has learned is error-free, because running over cyclists is quite\n",
      "bad. An explanation might reveal that the most important learned feature is to recognize the two\n",
      "wheels of a bicycle, and this explanation helps you think about edge cases like bicycles with side\n",
      "bags that partially cover the wheels. [{'source': './data/xai.pdf', 'start_index': 38818}]\n",
      "* Prototypes and criticisms for two types of dog breeds from the ImageNet dataset.\n",
      "\n",
      "Another illustration of MMD-critic uses a handwritten digit dataset.\n",
      "\n",
      "Looking at the actual prototypes and criticisms, you might notice that the number of images per\n",
      "digit is different. This is because a fixed number of prototypes and criticisms were searched across\n",
      "the entire dataset and not with a fixed number per class. As expected, the prototypes show different\n",
      "ways of writing the digits. The criticisms include examples with unusually thick or thin lines, but\n",
      "also unrecognizable digits.\n",
      "\n",
      "Prototypes and criticisms for a handwritten digits dataset.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "Advantages\n",
      "\n",
      "216 [{'source': './data/xai.pdf', 'start_index': 432665}]\n",
      "* where ▽xJ is the gradient of the models loss function with respect to the original input pixel vector\n",
      "x, y is the true label vector for x and (cid:18) is the model parameter vector. From the gradient vector\n",
      "(which is as long as the vector of the input pixels) we only need the sign: The sign of the gradient\n",
      "is positive (+1) if an increase in pixel intensity increases the loss (the error the model makes) and\n",
      "negative (-1) if a decrease in pixel intensity increases the loss. This vulnerability occurs when a\n",
      "neural network treats a relationship between an input pixel intensity and the class score linearly.\n",
      "In particular, neural network architectures that favor linearity, such as LSTMs, maxout networks,\n",
      "networks with ReLU activation units or other linear machine learning algorithms such as logistic\n",
      "regression are vulnerable to the gradient sign method. The attack is carried out by extrapolation. [{'source': './data/xai.pdf', 'start_index': 402873}]\n",
      "* networks with ReLU activation units or other linear machine learning algorithms such as logistic\n",
      "regression are vulnerable to the gradient sign method. The attack is carried out by extrapolation.\n",
      "The linearity between the input pixel intensity and the class scores leads to vulnerability to outliers,\n",
      "i.e. the model can be deceived by moving pixel values into areas outside the data distribution. I\n",
      "expected these adversarial examples to be quite specific to a given neural network architecture. But\n",
      "it turns out that you can reuse adversarial examples to deceive networks with a different architecture\n",
      "trained on the same task. [{'source': './data/xai.pdf', 'start_index': 403581}]\n",
      "* Advantages of Identifying Influential Instances\n",
      "\n",
      "The approaches of deletion diagnostics and influence functions are very different from the mostly\n",
      "feature-perturbation based approaches presented in the Model-Agnostic chapter. A look at influ-\n",
      "ential instances emphasizes the role of training data in the learning process. This makes influence\n",
      "functions and deletion diagnostics one of the best debugging tools for machine learning models.\n",
      "Of the techniques presented in this book, they are the only ones that directly help to identify the\n",
      "instances which should be checked for errors.\n",
      "\n",
      "Deletion diagnostics are model-agnostic, meaning the approach can be applied to any model. Also\n",
      "influence functions based on the derivatives can be applied to a broad class of models.\n",
      "\n",
      "We can use these methods to compare different machine learning models and better understand\n",
      "their different behaviors, going beyond comparing only the predictive performance. [{'source': './data/xai.pdf', 'start_index': 467413}]\n",
      "* Interpretable Models\n",
      "\n",
      "93\n",
      "\n",
      "The RIPPER algorithm does not find any rule in the classification task for cervical cancer.\n",
      "\n",
      "When we use RIPPER on the regression task to predict bike counts some rules are found. Since\n",
      "RIPPER only works for classification, the bike counts must be turned into a categorical outcome. I\n",
      "achieved this by cutting the bike counts into the quartiles. For example (4548, 5956) is the interval\n",
      "covering predicted bike counts between 4548 and 5956. The following table shows the decision list\n",
      "of learned rules. [{'source': './data/xai.pdf', 'start_index': 199113}]\n",
      "* Introduction\n",
      "\n",
      "14\n",
      "\n",
      "An Instance is a row in the dataset. Other names for ‘instance’ are: (data) point, example,\n",
      "observation. An instance consists of the feature values x(i) and, if known, the target outcome yi.\n",
      "\n",
      "The Features are the inputs used for prediction or classification. A feature is a column in the dataset.\n",
      "Throughout the book, features are assumed to be interpretable, meaning it is easy to understand\n",
      "what they mean, like the temperature on a given day or the height of a person. The interpretability\n",
      "of the features is a big assumption. But if it is hard to understand the input features, it is even harder\n",
      "to understand what the model does. The matrix with all features is called X and x(i) for a single\n",
      "instance. The vector of a single feature for all instances is xj and the value for the feature j and\n",
      "instance i is x(i)\n",
      "j .\n",
      "\n",
      "The Target is the information the machine learns to predict. In mathematical formulas, the target\n",
      "is usually called y or yi for a single instance. [{'source': './data/xai.pdf', 'start_index': 30767}]\n",
      "* Local surrogate models benefit from the literature and experience of training and interpreting\n",
      "interpretable models.\n",
      "\n",
      "When using Lasso or short trees, the resulting explanations are short (= selective) and possibly\n",
      "contrastive. Therefore, they make human-friendly explanations. This is why I see LIME more in\n",
      "applications where the recipient of the explanation is a lay person or someone with very little time.\n",
      "It is not sufficient for complete attributions, so I do not see LIME in compliance scenarios where\n",
      "you might be legally required to fully explain a prediction. Also for debugging machine learning\n",
      "models, it is useful to have all the reasons instead of a few.\n",
      "\n",
      "LIME is one of the few methods that works for tabular data, text and images. [{'source': './data/xai.pdf', 'start_index': 346255}]\n",
      "* The Shapley value can be misinterpreted. The Shapley value of a feature value is not the difference\n",
      "of the predicted value after removing the feature from the model training. The interpretation of the\n",
      "Shapley value is: Given the current set of feature values, the contribution of a feature value to the\n",
      "difference between the actual prediction and the mean prediction is the estimated Shapley value.\n",
      "\n",
      "The Shapley value is the wrong explanation method if you seek sparse explanations (explanations\n",
      "that contain few features). Explanations created with the Shapley value method always use all the\n",
      "features. Humans prefer selective explanations, such as those produced by LIME. LIME might be the\n",
      "better choice for explanations lay-persons have to deal with. Another solution is SHAP⁹¹ introduced\n",
      "by Lundberg and Lee (2016)⁹², which is based on the Shapley value, but can also provide explanations\n",
      "with few features. [{'source': './data/xai.pdf', 'start_index': 367132}]\n",
      "* A kitten sits on the window ledge of a burning and uninhabited house. The fire department has\n",
      "already arrived and one of the firefighters ponders for a second whether he can risk going into the\n",
      "building to save the kitten. He remembers similar cases in his life as a firefighter: Old wooden houses\n",
      "that have been burning slowly for some time were often unstable and eventually collapsed. Because\n",
      "of the similarity of this case, he decides not to enter, because the risk of the house collapsing is too\n",
      "great. Fortunately, the kitty jumps out of the window, lands safely and nobody is harmed in the fire.\n",
      "Happy end.\n",
      "\n",
      "These stories illustrate how we humans think in examples or analogies. The blueprint of example-\n",
      "based explanations is: Thing B is similar to thing A and A caused Y, so I predict that B will cause Y\n",
      "\n",
      "⁹⁴Aamodt, Agnar, and Enric Plaza. “Case-based reasoning: Foundational issues, methodological variations, and system approaches.” AI\n",
      "\n",
      "communications 7.1 (1994): 39-59. [{'source': './data/xai.pdf', 'start_index': 373332}]\n",
      "* is a Japanese movie that tells alternative, contradictory stories (explanations) about the death of a\n",
      "samurai. For machine learning models, it is advantageous if a good prediction can be made from\n",
      "different features. Ensemble methods that combine multiple models with different features (different [{'source': './data/xai.pdf', 'start_index': 74698}]\n",
      "* There are many techniques to create adversarial examples. Most approaches suggest minimizing\n",
      "the distance between the adversarial example and the instance to be manipulated, while shifting\n",
      "the prediction to the desired (adversarial) outcome. Some methods require access to the gradients\n",
      "of the model, which of course only works with gradient based models such as neural networks,\n",
      "other methods only require access to the prediction function, which makes these methods model-\n",
      "agnostic. The methods in this section focus on image classifiers with deep neural networks, as a\n",
      "lot of research is done in this area and the visualization of adversarial images is very educational.\n",
      "Adversarial examples for images are images with intentionally perturbed pixels with the aim to\n",
      "deceive the model during application time. The examples impressively demonstrate how easily deep\n",
      "neural networks for object recognition can be deceived by images that appear harmless to humans. [{'source': './data/xai.pdf', 'start_index': 399080}]\n",
      "* ¹⁰³Brown, Tom B., et al. “Adversarial patch.” arXiv preprint arXiv:1712.09665 (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "204\n",
      "\n",
      "situations. In the end, this optimized image can be printed and used to deceive image classifiers in\n",
      "the wild.\n",
      "\n",
      "Never bring a 3D-printed turtle to a gunfight – even if your computer thinks it is a good idea:\n",
      "Robust adversarial examples\n",
      "\n",
      "The next method is literally adding another dimension to the toaster: Athalye et. al (2017)¹⁰⁴ 3D-\n",
      "printed a turtle that was designed to look like a rifle to a deep neural network from almost all\n",
      "possible angles. Yeah, you read that right. A physical object that looks like a turtle to humans looks\n",
      "like a rifle to the computer!\n",
      "\n",
      "A 3D-printed turtle that is recognized as a rifle by TensorFlowâ€™s standard pre-trained InceptionV3 classifier. Work\n",
      "by Athalye et. al (2017) [{'source': './data/xai.pdf', 'start_index': 407854}]\n",
      "* Interpretable Models\n",
      "\n",
      "92\n",
      "\n",
      "• Learn a decision tree (with CART or another tree learning algorithm).\n",
      "• Start at the root node and recursively select the purest node (e.g. with the lowest misclassifi-\n",
      "\n",
      "cation rate).\n",
      "\n",
      "• The majority class of the terminal node is used as the rule prediction; the path leading to that\n",
      "\n",
      "node is used as the rule condition.\n",
      "\n",
      "The following figure illustrates the beam search in a tree:\n",
      "\n",
      "Learning a rule by searching a path through a decision tree. A decision tree is grown to predict the target of interest.\n",
      "We start at the root node, greedily and iteratively follow the path which locally produces the purest subset (e.g.\n",
      "highest accuracy) and add all the split values to the rule condition. We end up with: If location=good and size=big,\n",
      "then value=high. [{'source': './data/xai.pdf', 'start_index': 197465}]\n",
      "* The authors have found a way to create an adversarial example in 3D for a 2D classifier that is\n",
      "adversarial over transformations, such as all possibilities to rotate the turtle, zoom in and so on.\n",
      "Other approaches such as the fast gradient method no longer work when the image is rotated or\n",
      "viewing angle changes. Athalye et. al (2017) propose the Expectation Over Transformation (EOT)\n",
      "algorithm, which is a method for generating adversarial examples that even work when the image\n",
      "is transformed. The main idea behind EOT is to optimize adversarial examples across many possible\n",
      "transformations. Instead of minimizing the distance between the adversarial example and the\n",
      "original image, EOT keeps the expected distance between the two below a certain threshold, given\n",
      "a selected distribution of possible transformations. The expected distance under transformation can\n",
      "be written as: [{'source': './data/xai.pdf', 'start_index': 408686}]\n",
      "* ¹¹²Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.” arXiv preprint arXiv:1703.04730 (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "227\n",
      "\n",
      "where (cid:18) is the model parameter vector and ^(cid:18)ϵ;z is the parameter vector after upweighting z by a very\n",
      "small number ϵ. L is the loss function with which the model was trained, zi is the training data and z\n",
      "is the training instance which we want to upweight to simulate its removal. The intuition behind this\n",
      "formula is: How much will the loss change if we upweight a particular instance zi from the training\n",
      "data by a little (ϵ) and downweight the other data instances accordingly? What would the parameter\n",
      "vector look like to optimize this new combined loss? The influence function of the parameters, i.e.\n",
      "the influence of upweighting training instance z on the parameters, can be calculated as follows.\n",
      "\n",
      "Iup,params(z) =\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "d^(cid:18)ϵ;z\n",
      "dϵ\n",
      "\n",
      "ϵ=0 [{'source': './data/xai.pdf', 'start_index': 455884}]\n",
      "* The Target is the information the machine learns to predict. In mathematical formulas, the target\n",
      "is usually called y or yi for a single instance.\n",
      "\n",
      "A Machine Learning Task is the combination of a dataset with features and a target. Depending\n",
      "on the type of the target, the task can be for example classification, regression, survival analysis,\n",
      "clustering, or outlier detection.\n",
      "\n",
      "The Prediction is what the machine learning model “guesses” what the target value should be based\n",
      "on the given features. In this book, the model prediction is denoted by ^f (x(i)) or ^y.\n",
      "\n",
      "\fInterpretability [{'source': './data/xai.pdf', 'start_index': 31608}]\n",
      "* Explain Individual Predictions\n",
      "\n",
      "How much has each feature of an instance contributed to the prediction? This can be answered by\n",
      "computing the effects for this instance. An interpretation of instance-specific effects only makes\n",
      "sense in comparison to the distribution of the effect for each feature. We want to explain the\n",
      "prediction of the linear model for the 6-th instance from the bicycle dataset. The instance has the\n",
      "following feature values.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "46\n",
      "\n",
      "Feature\n",
      "season\n",
      "yr\n",
      "mnth\n",
      "holiday\n",
      "weekday\n",
      "workingday\n",
      "weathersit\n",
      "temp\n",
      "hum\n",
      "windspeed\n",
      "cnt\n",
      "days_since_2011\n",
      "\n",
      "Value\n",
      "SPRING\n",
      "2011\n",
      "JAN\n",
      "NO HOLIDAY\n",
      "THU\n",
      "WORKING DAY\n",
      "GOOD\n",
      "1.604356\n",
      "51.8261\n",
      "6.000868\n",
      "1606\n",
      "5 [{'source': './data/xai.pdf', 'start_index': 109957}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "219\n",
      "\n",
      "Feature x follows a Gaussian distribution with an outlier at x=8.\n",
      "\n",
      "Outliers can be interesting data points (e.g. criticisms). When an outlier influences the model it is\n",
      "also an influential instance.\n",
      "\n",
      "Influential instance\n",
      "\n",
      "An influential instance is a data instance whose removal has a strong effect on the trained model.\n",
      "The more the model parameters or predictions change when the model is retrained with a particular\n",
      "instance removed from the training data, the more influential that instance is. Whether an instance\n",
      "is influential for a trained model also depends on its value for the target y. The following figure\n",
      "shows an influential instance for a linear regression model.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "220\n",
      "\n",
      "A linear model with one feature. Trained once on the full data and once without the influential instance. Removing\n",
      "the influential instance changes the fitted slope (weight/coefficient) drastically. [{'source': './data/xai.pdf', 'start_index': 439645}]\n",
      "* The second example shows counterfactual explanations for predicted risk of diabetes. A three-layer\n",
      "fully-connected neural network is trained to predict the risk for diabetes depending on age, BMI,\n",
      "number of pregnancies and so on for women of Pima heritage. The counterfactuals answer the\n",
      "question: Which feature values must be changed to increase or decrease the risk score of diabetes to\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "196\n",
      "\n",
      "0.5? The following counterfactuals were found:\n",
      "\n",
      "• Person 1: If your 2-hour serum insulin level was 154.3, you would have a score of 0.51\n",
      "• Person 2: If your 2-hour serum insulin level was 169.5, you would have a score of 0.51\n",
      "• Person 3: If your Plasma glucose concentration was 158.3 and your 2-hour serum insulin level\n",
      "\n",
      "was 160.5, you would have a score of 0.51\n",
      "\n",
      "Advantages [{'source': './data/xai.pdf', 'start_index': 391126}]\n",
      "* The following section explains the intuition and math behind influence functions.\n",
      "\n",
      "Math behind influence functions\n",
      "\n",
      "The key idea behind influence functions is to upweight the loss of a training instance by an\n",
      "infinitesimally small step ϵ, which results in new model parameters:\n",
      "\n",
      "^(cid:18)ϵ;z = arg min\n",
      "(cid:18)2(cid:2)\n",
      "\n",
      "(1 (cid:0) ϵ)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "L(zi; (cid:18)) + ϵL(z; (cid:18))\n",
      "\n",
      "¹¹²Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.” arXiv preprint arXiv:1703.04730 (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "227 [{'source': './data/xai.pdf', 'start_index': 455500}]\n",
      "* ¹⁸Kahneman, Daniel, and Amos Tversky. “The Simulation Heuristic.” Stanford Univ CA Dept of Psychology. (1981).\n",
      "¹⁹Štrumbelj, Erik, and Igor Kononenko. “A general method for visualizing and explaining black-box regression models.” In International\n",
      "\n",
      "Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "32 [{'source': './data/xai.pdf', 'start_index': 78729}]\n",
      "* Interpretable Models\n",
      "\n",
      "Effect Plot\n",
      "\n",
      "44\n",
      "\n",
      "The weights of the linear regression model can be more meaningfully analyzed when they are\n",
      "multiplied by the actual feature values. The weights depend on the scale of the features and will\n",
      "be different if you have a feature that measures e.g. a person’s height and you switch from meter to\n",
      "centimeter. The weight will change, but the actual effects in your data will not. It is also important\n",
      "to know the distribution of your feature in the data, because if you have a very low variance, it\n",
      "means that almost all instances have similar contribution from this feature. The effect plot can help\n",
      "you understand how much the combination of weight and feature contributes to the predictions in\n",
      "your data. Start by calculating the effects, which is the weight per feature times the feature value of\n",
      "an instance:\n",
      "\n",
      "effect(i)\n",
      "\n",
      "j = wjx(i)\n",
      "\n",
      "j [{'source': './data/xai.pdf', 'start_index': 107549}]\n",
      "* We can distinguish types of attacks by how much an attacker knows about the system. The attackers\n",
      "may have perfect knowledge (white box attack), meaning they know everything about the model like\n",
      "the type of model, the parameters and the training data; the attackers may have partial knowledge\n",
      "(gray box attack), meaning they might only know the feature representation and the type of model\n",
      "that was used, but have no access to the training data or the parameters; the attackers may have\n",
      "zero knowledge (black box attack), meaning they can only query the model in a black box manner\n",
      "but have no access to the training data or information about the model parameters. Depending on\n",
      "the level of information, the attackers can use different techniques to attack the model. As we have\n",
      "seen in the examples, even in the black box case adversarial examples can be created, so that hiding\n",
      "information about data and model is not sufficient to protect against attacks. [{'source': './data/xai.pdf', 'start_index': 417194}]\n",
      "* p(djx; y; A; (cid:11); (cid:21); (cid:17))\n",
      "}\n",
      "{z\n",
      "|\n",
      "posteriori\n",
      "\n",
      "|\n",
      "\n",
      "/ p(yjx; d; (cid:11))\n",
      "}\n",
      "{z\n",
      "likelihood\n",
      "\n",
      "(cid:1) p(djA; (cid:21); (cid:17))\n",
      "}\n",
      "{z\n",
      "|\n",
      "priori\n",
      "\n",
      "where d is a decision list, x are the features, y is the target, A the set of pre-mined conditions, (cid:21) the\n",
      "prior expected length of the decision lists, (cid:17) the prior expected number of conditions in a rule, (cid:11) the\n",
      "prior pseudo-count for the positive and negative classes which is best fixed at (1,1).\n",
      "\n",
      "p(djx; y; A; (cid:11); (cid:21); (cid:17))\n",
      "\n",
      "quantifies how probable a decision list is, given the observed data and the priori assumptions. This\n",
      "is proportional to the likelihood of the outcome y given the decision list and the data times the\n",
      "probability of the list given prior assumptions and the pre-mined conditions.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "96\n",
      "\n",
      "p(yjx; d; (cid:11)) [{'source': './data/xai.pdf', 'start_index': 207597}]\n",
      "* The method even works if the black box model to be deceived is not a neural network. This includes\n",
      "machine learning models without gradients such as a decision trees.\n",
      "\n",
      "The Cybersecurity Perspective\n",
      "\n",
      "Machine learning deals with known unknowns: predicting unknown data points from a known\n",
      "distribution. The defense against attacks deals with unknown unknowns: robustly predicting\n",
      "unknown data points from an unknown distribution of adversarial inputs. As machine learning\n",
      "is integrated into more and more systems, such as autonomous vehicles or medical devices, they are\n",
      "also becoming entry points for attacks. Even if the predictions of a machine learning model on a\n",
      "test dataset are 100% correct, adversarial examples can be found to deceive the model. The defense\n",
      "of machine learning models against cyber attacks is a new part of the field of cybersecurity. [{'source': './data/xai.pdf', 'start_index': 413394}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "187\n",
      "\n",
      "fairly distributed among the features. The Shapley value might be the only method to deliver a full\n",
      "explanation. In situations where the law requires explainability – like EU’s “right to explanations” –\n",
      "the Shapley value might be the only legally compliant method, because it is based on a solid theory\n",
      "and distributes the effects fairly. I am not a lawyer, so this reflects only my intuition about the\n",
      "requirements.\n",
      "\n",
      "The Shapley value allows contrastive explanations. Instead of comparing a prediction to the\n",
      "average prediction of the entire dataset, you could compare it to a subset or even to a single data\n",
      "point. This contrastiveness is also something that local models like LIME do not have. [{'source': './data/xai.pdf', 'start_index': 365173}]\n",
      "* A self-driving car crashes into another car because it ignores a stop sign. Someone had placed a\n",
      "picture over the sign, which looks like a stop sign with a little dirt for humans, but was designed to\n",
      "look like a parking prohibition sign for the sign recognition software of the car.\n",
      "\n",
      "A spam detector fails to classify an email as spam. The spam mail has been designed to resemble a\n",
      "normal email, but with the intention of cheating the recipient.\n",
      "\n",
      "A machine-learning powered scanner scans suitcases for weapons at the airport. A knife was\n",
      "developed to avoid detection by making the system think it is an umbrella.\n",
      "\n",
      "Let us take a look at some ways to create adversarial examples.\n",
      "\n",
      "Methods and Examples [{'source': './data/xai.pdf', 'start_index': 398379}]\n",
      "* Problem: The true relationship between the features and y is not linear.\n",
      "Example: Between 0 and 25 degrees Celsius, the influence of the temperature on my desire to ride\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "63\n",
      "\n",
      "a bike could be linear, which means that an increase from 0 to 1 degree causes the same increase in\n",
      "cycling desire as an increase from 20 to 21. But at higher temperatures my motivation to cycle levels\n",
      "off and even decreases - I do not like to bike when it is too hot.\n",
      "Solutions: Generalized Additive Models (GAMs); transformation of features. [{'source': './data/xai.pdf', 'start_index': 139332}]\n",
      "* Interpretability might enable people or programs to manipulate the system. Problems with users\n",
      "who deceive a system result from a mismatch between the goals of the creator and the user of a\n",
      "model. Credit scoring is such a system because banks want to ensure that loans are only given to\n",
      "applicants who are likely to return them, and applicants aim to get the loan even if the bank does\n",
      "not want to give them one. This mismatch between the goals introduces incentives for applicants to\n",
      "game the system to increase their chances of getting a loan. If an applicant knows that having more\n",
      "than two credit cards negatively affects his score, he simply returns his third credit card to improve\n",
      "his score, and organizes a new card after the loan has been approved. While his score improved, the\n",
      "actual probability of repaying the loan remained unchanged. The system can only be gamed if the\n",
      "inputs are proxies for a causal feature, but do not actually cause the outcome. Whenever possible, [{'source': './data/xai.pdf', 'start_index': 46660}]\n",
      "* data point, but it might at least be more likely or more meaningful. In this case, the weights times\n",
      "the feature values (feature effects) explain the contribution to the predicted outcome contrastive\n",
      "to the “mean-instance”. Another aspect of a good explanation is selectivity, which can be achieved\n",
      "in linear models by using less features or by training sparse linear models. But by default, linear\n",
      "models do not create selective explanations. Linear models create truthful explanations, as long as\n",
      "the linear equation is an appropriate model for the relationship between features and outcome. The\n",
      "more non-linearities and interactions there are, the less accurate the linear model will be and the less\n",
      "truthful the explanations become. Linearity makes the explanations more general and simpler. The\n",
      "linear nature of the model, I believe, is the main factor why people use linear models for explaining\n",
      "relationships. [{'source': './data/xai.pdf', 'start_index': 115695}]\n",
      "* Example\n",
      "\n",
      "82\n",
      "\n",
      "Let us have another look at the bike rental data. We want to predict the number of rented bikes on\n",
      "a certain day with a decision tree. The learned tree looks like this:\n",
      "\n",
      "Regression tree fitted on the bike rental data. The maximum allowed depth for the tree was set to 2. The trend feature\n",
      "(days since 2011) and the temperature (temp) have been selected for the splits. The boxplots show the distribution of\n",
      "bicycle counts in the terminal node.\n",
      "\n",
      "The feature importance tells us how much a feature helped to improve the purity of all nodes. Here,\n",
      "the variance was used, since predicting bicycle rentals is a regression task.\n",
      "\n",
      "The visualized tree shows that both temperature and time trend were used for the splits, but does\n",
      "not quantify which feature was more important. The feature importance measure shows that the\n",
      "time trend is far more important than temperature.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "83\n",
      "\n",
      "Importance of the features measured by how much the node purity is improved on average. [{'source': './data/xai.pdf', 'start_index': 176737}]\n",
      "* Interpretable Models\n",
      "\n",
      "48\n",
      "\n",
      "instances, the feature takes category A; for instances three and four, category B; and for the last two\n",
      "instances, category C.\n",
      "\n",
      "Treatment coding\n",
      "\n",
      "In treatment coding, the weight per category is the estimated difference in the prediction between\n",
      "the corresponding category and the reference category. The intercept of the linear model is the mean\n",
      "of the reference category (when all other features remain the same). The first column of the design\n",
      "matrix is the intercept, which is always 1. Column two indicates whether instance i is in category\n",
      "B, column three indicates whether it is in category C. There is no need for a column for category A,\n",
      "because then the linear equation would be overspecified and no unique solution for the weights can\n",
      "be found. It is sufficient to know that an instance is neither in category B or C.\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "@\n",
      "\n",
      "1 0\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "A\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "Feature matrix:\n",
      "\n",
      "Effect coding [{'source': './data/xai.pdf', 'start_index': 112621}]\n",
      "* By default, machine learning models pick up biases from the training data. This can turn your\n",
      "machine learning models into racists that discriminate against protected groups. Interpretability\n",
      "is a useful debugging tool for detecting bias in machine learning models. It might happen that the\n",
      "machine learning model you have trained for automatic approval or rejection of credit applications\n",
      "discriminates against a minority. Your main goal is to grant loans only to people who will eventually\n",
      "repay them. The incompleteness of the problem formulation in this case lies in the fact that you not\n",
      "only want to minimize loan defaults, but are also obliged not to discriminate on the basis of certain\n",
      "demographics. This is an additional constraint that is part of your problem formulation (granting\n",
      "loans in a low-risk and compliant way) that is not covered by the loss function the machine learning\n",
      "model was optimized for. [{'source': './data/xai.pdf', 'start_index': 39358}]\n",
      "* Software and Alternatives\n",
      "\n",
      "The iml R package was used for the examples. The DALEX R package and the Python Skater module\n",
      "also implement model-agnostic permutation feature importance.\n",
      "\n",
      "An algorithm called PIMP⁷⁸ adapts the feature importance algorithm to provide p-values for the\n",
      "importances.\n",
      "\n",
      "⁷⁸https://academic.oup.com/bioinformatics/article/26/10/1340/193348\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "Global Surrogate\n",
      "\n",
      "163\n",
      "\n",
      "A global surrogate model is an interpretable model that is trained to approximate the predictions of a\n",
      "black box model. We can draw conclusions about the black box model by interpreting the surrogate\n",
      "model. Solving machine learning interpretability by using more machine learning!\n",
      "\n",
      "Theory [{'source': './data/xai.pdf', 'start_index': 322878}]\n",
      "* This first influence analysis revealed the overall most influential instance. Now we select one of the\n",
      "instances, namely the 7-th instance, for which we want to explain the prediction by finding the most\n",
      "influential training data instances. It is like a counterfactual question: How would the prediction for\n",
      "instance 7 change if we omit instance i from the training process? We repeat this removal for all\n",
      "instances. Then we select the training instances that result in the biggest change in the prediction\n",
      "of instance 7 when they are omitted from the training and use them to explain the prediction of the\n",
      "model for that instance. I chose to explain the prediction for instance 7 because it is the instance\n",
      "with the highest predicted probability of cancer (7.35%), which I thought was an interesting case to\n",
      "analyze more deeply. We could return the, say, top 10 most influential instances for predicting the 7- [{'source': './data/xai.pdf', 'start_index': 449969}]\n",
      "* The big question is how to learn nonlinear functions. The answer is called “splines” or “spline\n",
      "functions”. Splines are functions that can be combined in order to approximate arbitrary functions.\n",
      "A bit like stacking Lego bricks to build something more complex. There is a confusing number of\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "74 [{'source': './data/xai.pdf', 'start_index': 162370}]\n",
      "* An alternative way to search counterfactuals is the Growing Spheres algorithm by Laugel et. al\n",
      "(2017)⁹⁸. The method first draws a sphere around the point of interest, samples points within that\n",
      "sphere, checks whether one of the sampled points yields the desired prediction, contracts or expands\n",
      "the sphere accordingly until a (sparse) counterfactual is found and finally returned. They do not\n",
      "use the word counterfactual in their paper, but the method is quite similar. They also define a loss\n",
      "function that favors counterfactuals with as few changes in the feature values as possible. Instead\n",
      "of directly optimizing the function, they suggest the above-mentioned search with spheres.\n",
      "\n",
      "⁹⁷Martens, David, and Foster Provost. “Explaining data-driven document classifications.” (2014).\n",
      "⁹⁸Laugel, Thibault, et al. “Inverse classification for comparison-based interpretability in machine learning.” arXiv preprint arXiv:1712.08443\n",
      "\n",
      "(2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "198 [{'source': './data/xai.pdf', 'start_index': 396024}]\n",
      "* explained” measure lies always between 0 (no association) and 1 (temperature can be perfectly\n",
      "predicted from the other feature). We calculate the explained variance of temperature, humidity\n",
      "and wind speed with all the other features. The higher the explained variance (correlation), the\n",
      "more (potential) problems with PD plots. The following figure visualizes how strongly the weather\n",
      "features are correlated with other features. [{'source': './data/xai.pdf', 'start_index': 282569}]\n",
      "* +j, but the value xm\n",
      "j\n",
      "\n",
      "Approximate Shapley estimation for single feature value:\n",
      "\n",
      "⁹⁰Štrumbelj, Erik, and Igor Kononenko. “Explaining prediction models and individual predictions with feature contributions.” Knowledge\n",
      "\n",
      "and information systems 41.3 (2014): 647-665.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "186\n",
      "\n",
      "• Output: Shapley value for the value of the j-th feature\n",
      "• Required: Number of iterations M, instance of interest x, feature index j, data matrix X, and\n",
      "\n",
      "machine learning model f\n",
      "– For all m = 1,…,M:\n",
      "\n",
      "* Draw random instance z from the data matrix X\n",
      "* Choose a random permutation o of the feature values\n",
      "* Order instance x: xo = (x(1); : : : ; x(j); : : : ; x(p))\n",
      "* Order instance z: zo = (z(1); : : : ; z(j); : : : ; z(p))\n",
      "* Construct two new instances\n",
      "\n",
      "·\n",
      "\n",
      "·\n",
      "\n",
      "*\n",
      "\n",
      "x+j = (x(1); : : : ; x(j(cid:0)1); x(j); z(j+1); : : : ; z(p))\n",
      "\n",
      "x(cid:0)j = (x(1); : : : ; x(j(cid:0)1); z(j); z(j+1); : : : ; z(p))\n",
      "\n",
      "• Compute Shapley value as the average: ϕj(x) = 1\n",
      "M\n",
      "\n",
      "M\n",
      "m=1 ϕm\n",
      "j\n",
      "\n",
      "j = ^f (x+j) (cid:0) ^f (x(cid:0)j)\n",
      "ϕm\n",
      "∑ [{'source': './data/xai.pdf', 'start_index': 362877}]\n",
      "* Advantages\n",
      "\n",
      "The surrogate model method is flexible: Any model from the interpretable models chapter can\n",
      "be used. This also means that you can exchange not only the interpretable model, but also the\n",
      "underlying black box model. Suppose you create some complex model and explain it to different\n",
      "teams in your company. One team is familiar with linear models, the other team can understand\n",
      "decision trees. You can train two surrogate models (linear model and decision tree) for the original\n",
      "black box model and offer two kinds of explanations. If you find a better performing black box\n",
      "model, you do not have to change your method of interpretation, because you can use the same\n",
      "class of surrogate models.\n",
      "\n",
      "I would argue that the approach is very intuitive and straightforward. This means it is easy to\n",
      "implement, but also easy to explain to people not familiar with data science or machine learning.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "167 [{'source': './data/xai.pdf', 'start_index': 330007}]\n",
      "* of everything, we will have ever bigger datasets and therefore the approach of machine learning\n",
      "becomes more attractive. We do not make assumptions, we approximate reality as close as possible\n",
      "(while avoiding overfitting of the training data). I argue that we should develop all the tools that we\n",
      "have in statistics to answer questions (hypothesis tests, correlation measures, interaction measures,\n",
      "visualization tools, confidence intervals, p-values, prediction intervals, probability distributions) and\n",
      "rewrite them for black box models. In a way, this is already happening: [{'source': './data/xai.pdf', 'start_index': 486510}]\n",
      "* There is some code available, but it is not yet implemented as nicely packaged and documented\n",
      "software.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "Code and Alternatives\n",
      "\n",
      "217\n",
      "\n",
      "An implementation of MMD-critic can be found here: https://github.com/BeenKim/MMD-critic¹⁰⁸.\n",
      "\n",
      "The simplest alternative to finding prototypes is k-medoids¹⁰⁹ by Kaufman et. al (1987).¹¹⁰\n",
      "\n",
      "¹⁰⁸https://github.com/BeenKim/MMD-critic\n",
      "¹⁰⁹https://en.wikipedia.org/wiki/K-medoids\n",
      "¹¹⁰Kaufman, Leonard, and Peter Rousseeuw. “Clustering by means of medoids”. North-Holland (1987).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "218\n",
      "\n",
      "Influential Instances [{'source': './data/xai.pdf', 'start_index': 435989}]\n",
      "* • Stability: How similar are the explanations for similar instances? While consistency compares\n",
      "explanations between models, stability compares explanations between similar instances for a\n",
      "fixed model. High stability means that slight variations in the features of an instance do not\n",
      "substantially change the explanation (unless these slight variations also strongly change the\n",
      "prediction). A lack of stability can be the result of a high variance of the explanation method. In\n",
      "other words, the explanation method is strongly affected by slight changes of the feature values\n",
      "of the instance to be explained. A lack of stability can also be caused by non-deterministic\n",
      "components of the explanation method, such as a data sampling step, like the local surrogate\n",
      "method uses. High stability is always desirable. [{'source': './data/xai.pdf', 'start_index': 65787}]\n",
      "* From this table, you can select a suitable interpretable model for your task, either regression (regr)\n",
      "or classification (class):\n",
      "\n",
      "Algorithm\n",
      "Linear regression\n",
      "Logistic regression\n",
      "Decision trees\n",
      "RuleFit\n",
      "Naive Bayes\n",
      "k-nearest neighbors\n",
      "\n",
      "Linear\n",
      "Yes\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "\n",
      "Monotone\n",
      "Yes\n",
      "Yes\n",
      "Some\n",
      "No\n",
      "Yes\n",
      "No\n",
      "\n",
      "Interaction\n",
      "No\n",
      "No\n",
      "Yes\n",
      "Yes\n",
      "No\n",
      "No\n",
      "\n",
      "Task\n",
      "regr\n",
      "class\n",
      "class,regr\n",
      "class,regr\n",
      "class\n",
      "class,regr\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "Linear Regression\n",
      "\n",
      "38\n",
      "\n",
      "A linear regression model predicts the target as a weighted sum of the feature inputs. The linearity\n",
      "of the learned relationship makes the interpretation easy. Linear regression models have long been\n",
      "used by statisticians, computer scientists and other people who tackle quantitative problems.\n",
      "\n",
      "Linear models can be used to model the dependence of a regression target y on some features x. The\n",
      "learned relationships are linear and can be written for a single instance i as follows:\n",
      "\n",
      "y = (cid:12)0 + (cid:12)1x1 + : : : + (cid:12)pxp + ϵ [{'source': './data/xai.pdf', 'start_index': 92356}]\n",
      "* ⁹⁶Wachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without opening the black box: Automated decisions\n",
      "\n",
      "and the GDPR.” (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "194\n",
      "\n",
      "means that we prefer counterfactuals that come close to the desired outcome y’, a lower value means\n",
      "that we prefer counterfactuals x’ that are very similar to x in the feature values. If (cid:21) is very large, the\n",
      "instance with the prediction that comes closest to y’ will be selected, regardless how far it is away\n",
      "from x. Ultimately, the user must decide how to balance the requirement that the prediction for the\n",
      "counterfactual matches the desired outcome with the requirement that the counterfactual is similar\n",
      "to x. The authors of the method suggest instead of selecting a value for (cid:21) to select a tolerance ϵ for\n",
      "how far away the prediction of the counterfactual instance is allowed to be from y’. This constraint\n",
      "can be written as:\n",
      "\n",
      "j ^f (x′) (cid:0) y′j (cid:20) ϵ [{'source': './data/xai.pdf', 'start_index': 385956}]\n",
      "* (\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "min(cid:12)\n",
      "\n",
      ")\n",
      "\n",
      "(y(i) (cid:0) xT\n",
      "\n",
      "i (cid:12))2\n",
      "\n",
      "³⁶http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/\n",
      "³⁷http://heidiseibold.github.io/page7/\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "50\n",
      "\n",
      "Lasso adds a term to this optimization problem.\n",
      "\n",
      "(\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "min(cid:12)\n",
      "\n",
      ")\n",
      "\n",
      "(y(i) (cid:0) xT\n",
      "\n",
      "i (cid:12))2 + (cid:21)jj(cid:12)jj1\n",
      "\n",
      "The term jj(cid:12)jj1, the L1-norm of the feature vector, leads to a penalization of large weights. Since\n",
      "the L1-norm is used, many of the weights receive an estimate of 0 and the others are shrunk. The\n",
      "parameter lambda ((cid:21)) controls the strength of the regularizing effect and is usually tuned by cross-\n",
      "validation. Especially when lambda is large, many weights become 0. The feature weights can be\n",
      "visualized as a function of the penalty term lambda. Each feature weight is represented by a curve\n",
      "in the following figure. [{'source': './data/xai.pdf', 'start_index': 117486}]\n",
      "* measure.” arXiv preprint arXiv:1805.04755 (2018).\n",
      "\n",
      "⁷⁵https://github.com/koalaverse/vip\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "154\n",
      "\n",
      "Feature Importance\n",
      "\n",
      "The importance of a feature is the increase in the prediction error of the model after we permuted\n",
      "the feature’s values, which breaks the relationship between the feature and the true outcome.\n",
      "\n",
      "Theory [{'source': './data/xai.pdf', 'start_index': 306282}]\n",
      "* The counterfactuals method lacks a general software implementation. And a method is only useful\n",
      "if it is implemented. Fortunately, it should be easy to implement and hopefully I can remove this\n",
      "statement here soon.\n",
      "\n",
      "Software and Alternatives\n",
      "\n",
      "Unfortunately there is currently no software available for counterfactual explanations.\n",
      "\n",
      "A very similar approach was proposed by Martens et. al (2014) for explaining document classifi-\n",
      "cations. In their work, they focus on explaining why a document was or was not classified as a\n",
      "particular class. The difference to the method presented in this chapter is that Martens et. al (2014)\n",
      "focus on text classifiers, which have word occurrences as inputs. [{'source': './data/xai.pdf', 'start_index': 395331}]\n",
      "* Theory\n",
      "\n",
      "Surrogate models are also used in engineering: If an outcome of interest is expensive, time-\n",
      "consuming or otherwise difficult to measure (e.g. because it comes from a complex computer\n",
      "simulation), a cheap and fast surrogate model of the outcome can be used instead. The difference\n",
      "between the surrogate models used in engineering and in interpretable machine learning is that the\n",
      "underlying model is a machine learning model (not a simulation) and that the surrogate model must\n",
      "be interpretable. The purpose of (interpretable) surrogate models is to approximate the predictions of\n",
      "the underlying model as accurately as possible and to be interpretable at the same time. The idea of\n",
      "surrogate models can be found under different names: Approximation model, metamodel, response\n",
      "surface model, emulator, … [{'source': './data/xai.pdf', 'start_index': 323570}]\n",
      "* Similar to counterfactuals, the 1-pixel attack looks for a modified example x’ which comes close to\n",
      "the original image x, but changes the prediction to an adversarial outcome. However, the definition\n",
      "of closeness differs: Only a single pixel may change. The 1-pixel attack uses differential evolution to\n",
      "find out which pixel is to be changed and how. Differential evolution is loosely inspired by biological\n",
      "evolution of species. A population of individuals called candidate solutions recombines generation\n",
      "by generation until a solution is found. Each candidate solution encodes a pixel modification and is\n",
      "represented by a vector of five elements: the x- and y-coordinates and the red, green and blue (RGB)\n",
      "values. The search starts with, for example, 400 candidate solutions (= pixel modification suggestions)\n",
      "\n",
      "¹⁰²Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep neural networks.” IEEE Transactions on\n",
      "\n",
      "Evolutionary Computation (2019). [{'source': './data/xai.pdf', 'start_index': 405132}]\n",
      "* • Start with an empty list of rules (rlist).\n",
      "• Learn a rule r.\n",
      "• While the list of rules is below a certain quality threshold (or positive examples are not yet\n",
      "\n",
      "covered):\n",
      "\n",
      "– Add rule r to rlist.\n",
      "– Remove all data points covered by rule r.\n",
      "– Learn another rule on the remaining data.\n",
      "\n",
      "• Return the decision list.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "91\n",
      "\n",
      "The covering algorithm works by sequentially covering the feature space with single rules and removing the data\n",
      "points that are already covered by those rules. For visualization purposes, the features x1 and x2 are continuous, but\n",
      "most rule learning algorithms require categorical features. [{'source': './data/xai.pdf', 'start_index': 195531}]\n",
      "* What I am telling you here is actually nothing new. So why switch from analyzing assumption-\n",
      "based, transparent models to analyzing assumption-free black box models? Because making all these\n",
      "assumptions is problematic: They are usually wrong (unless you believe that most of the world\n",
      "follows a Gaussian distribution), difficult to check, very inflexible and hard to automate. In many\n",
      "domains, assumption-based models typically have a worse predictive performance on untouched test\n",
      "data than black box machine learning models. This is only true for big datasets, since interpretable\n",
      "models with good assumptions often perform better with small datasets than black box models.\n",
      "The black box machine learning approach requires a lot of data to work well. With the digitization\n",
      "of everything, we will have ever bigger datasets and therefore the approach of machine learning\n",
      "becomes more attractive. We do not make assumptions, we approximate reality as close as possible [{'source': './data/xai.pdf', 'start_index': 485735}]\n",
      "* The interaction of two categorical features works similarly. We create additional features which\n",
      "represent combinations of categories. Here is some artificial data containing working day (work)\n",
      "and a categorical weather feature (wthr):\n",
      "\n",
      "work\n",
      "Y\n",
      "N\n",
      "N\n",
      "Y\n",
      "\n",
      "wthr\n",
      "Good\n",
      "Bad\n",
      "Ok\n",
      "Good\n",
      "\n",
      "Next, we include interaction terms:\n",
      "\n",
      "Intercept workY wthrGood wthrOk workY.wthrGood workY.wthrOk\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "The first column serves to estimate the intercept. The second column is the encoded work feature.\n",
      "Columns three and four are for the weather feature, which requires two columns because you\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "70\n",
      "\n",
      "need two weights to capture the effect for three categories, one of which is the reference category.\n",
      "The rest of the columns capture the interactions. For each category of both features (except for\n",
      "the reference categories), we create a new feature column that is 1 if both features have a certain\n",
      "category, otherwise 0. [{'source': './data/xai.pdf', 'start_index': 152918}]\n",
      "* We do not analyze data, we analyze models.\n",
      "\n",
      "\fA Look into the Crystal Ball\n",
      "\n",
      "238\n",
      "\n",
      "The raw data itself is always useless. (I exaggerate on purpose. The reality is that you need a deep\n",
      "understanding of the data to conduct a meaningful analysis.) I don’t care about the data; I care\n",
      "about the knowledge contained in the data. Interpretable machine learning is a great way to distill\n",
      "knowledge from data. You can probe the model extensively, the model automatically recognizes if\n",
      "and how features are relevant for the prediction (many models have built-in feature selection), the\n",
      "model can automatically detect how relationships are represented, and – if trained correctly – the\n",
      "final model is a very good approximation of reality.\n",
      "\n",
      "Many analytical tools are already based on data models (because they are based on distribution\n",
      "assumptions): [{'source': './data/xai.pdf', 'start_index': 484617}]\n",
      "* Interpretable Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "38\n",
      "Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "54\n",
      "Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "61\n",
      "GLM, GAM and more . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "79\n",
      "Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "85\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Decision Rules\n",
      "RuleFit\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n",
      "Other Interpretable Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 [{'source': './data/xai.pdf', 'start_index': 2394}]\n",
      "* The idea is quite intuitive. First, forget about the training data and imagine you only have the black\n",
      "box model where you can input data points and get the predictions of the model. You can probe\n",
      "the box as often as you want. Your goal is to understand why the machine learning model made\n",
      "a certain prediction. LIME tests what happens to the predictions when you give variations of your\n",
      "data into the machine learning model. LIME generates a new dataset consisting of permuted samples\n",
      "and the corresponding predictions of the black box model. On this new dataset LIME then trains an\n",
      "interpretable model, which is weighted by the proximity of the sampled instances to the instance of\n",
      "interest. The interpretable model can be anything from the interpretable models chapter, for example\n",
      "Lasso or a decision tree. The learned model should be a good approximation of the machine learning\n",
      "model predictions locally, but it does not have to be a good global approximation. This kind of [{'source': './data/xai.pdf', 'start_index': 332959}]\n",
      "* Disturbed panda: Fast gradient sign method\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "201\n",
      "\n",
      "Goodfellow et. al (2014)¹⁰¹ invented the fast gradient sign method for generating adversarial images.\n",
      "The gradient sign method uses the gradient of the underlying model to find adversarial examples.\n",
      "The original image x is manipulated by adding or subtracting a small error ϵ to each pixel. Whether\n",
      "we add or subtract ϵ depends on whether the sign of the gradient for a pixel is positive or negative.\n",
      "Adding errors in the direction of the gradient means that the image is intentionally altered so that\n",
      "the model classification fails.\n",
      "\n",
      "Goodfellow et. al (2014) make a panda look like a gibbon for a neural network. By adding small perturbations (middle\n",
      "image) to the original panda pixels (left image), the authors create an adversarial example that is classified as a gibbon\n",
      "(right image) but looks like a panda to humans.\n",
      "\n",
      "The following formula describes the core of the fast gradient sign method: [{'source': './data/xai.pdf', 'start_index': 401849}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "232\n",
      "\n",
      "Disadvantages of Identifying Influential Instances\n",
      "\n",
      "Deletion diagnostics are very expensive to calculate because they require retraining. But history\n",
      "has shown that computer resources are constantly increasing. A calculation that 20 years ago\n",
      "was unthinkable in terms of resources can easily be performed with your smartphone. You can\n",
      "train models with thousands of training instances and hundreds of parameters on a laptop in\n",
      "seconds/minutes. It is therefore not a big leap to assume that deletion diagnostics will work without\n",
      "problems even with large neural networks in 10 years. [{'source': './data/xai.pdf', 'start_index': 469558}]\n",
      "* 46\n",
      "\n",
      "Feature\n",
      "season\n",
      "yr\n",
      "mnth\n",
      "holiday\n",
      "weekday\n",
      "workingday\n",
      "weathersit\n",
      "temp\n",
      "hum\n",
      "windspeed\n",
      "cnt\n",
      "days_since_2011\n",
      "\n",
      "Value\n",
      "SPRING\n",
      "2011\n",
      "JAN\n",
      "NO HOLIDAY\n",
      "THU\n",
      "WORKING DAY\n",
      "GOOD\n",
      "1.604356\n",
      "51.8261\n",
      "6.000868\n",
      "1606\n",
      "5\n",
      "\n",
      "To obtain the feature effects of this instance, we have to multiply its feature values by the\n",
      "corresponding weights from the linear regression model. For the value “WORKING DAY” of feature\n",
      "“workingday”, the effect is, 124.9. For a temperature of 1.6 degrees Celsius, the effect is 177.6. We add\n",
      "these individual effects as crosses to the effect plot, which shows us the distribution of the effects in\n",
      "the data. This allows us to compare the individual effects with the distribution of effects in the data.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "47\n",
      "\n",
      "The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest. [{'source': './data/xai.pdf', 'start_index': 110429}]\n",
      "* Examples\n",
      "\n",
      "I have taken the examples from the MMD-critic paper. Both applications are based on image datasets.\n",
      "Each image was represented by image embeddings with 2048 dimensions. An image embedding\n",
      "is a vector with numbers which capture abstract attributes of an image. Embedding vectors are\n",
      "usually extracted from neural networks which are trained to solve an image recognition task, in this\n",
      "case the ImageNet challenge. The kernel distances between the images were calculated using these\n",
      "embedding vectors.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "215\n",
      "\n",
      "The first dataset contains different dog breeds from the ImageNet dataset. MMD-critic is applied on\n",
      "data from two dog breed classes. With the dogs on the left, the prototypes usually show the face\n",
      "of the dog, while the criticisms are images without the dog faces or in different colors (like black\n",
      "and white). On the right side, the prototypes contain outdoor images of dogs. The criticisms contain\n",
      "dogs in costumes and other unusual cases. [{'source': './data/xai.pdf', 'start_index': 431679}]\n",
      "* Interpretable Models\n",
      "\n",
      "105\n",
      "\n",
      "^f (x) = ^(cid:12)0 +\n",
      "\n",
      "K∑\n",
      "\n",
      "k=1\n",
      "\n",
      "^(cid:11)krk(x) +\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "^(cid:12)jlj(xj)\n",
      "\n",
      "where ^(cid:11) is the estimated weight vector for the rule features and ^(cid:12) the weight vector for the original\n",
      "features. Since RuleFit uses Lasso, the loss function gets the additional constraint that forces some\n",
      "of the weights to get a zero estimate:\n",
      "\n",
      "(f^(cid:11)gK\n",
      "\n",
      "1 ; f ^(cid:12)gp\n",
      "\n",
      "0) = argminf ^(cid:11)gK\n",
      "\n",
      "1 ;f ^(cid:12)gp\n",
      "\n",
      "0\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "L(y(i); f (x(i))) + (cid:21) (cid:1)\n",
      "\n",
      "0\n",
      "\n",
      "@\n",
      "\n",
      "K∑\n",
      "\n",
      "k=1\n",
      "\n",
      "j(cid:11)kj +\n",
      "\n",
      "1\n",
      "\n",
      "A\n",
      "\n",
      "jbjj\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "The result is a linear model that has linear effects for all of the original features and for the rules.\n",
      "The interpretation is the same as for linear models, the only difference is that some features are now\n",
      "binary rules.\n",
      "\n",
      "Step 3 (optional): Feature importance\n",
      "\n",
      "For the linear terms of the original features, the feature importance is measured with the standard-\n",
      "ized predictor:\n",
      "\n",
      "Ij = j ^(cid:12)jj (cid:1) std(lj(xj)) [{'source': './data/xai.pdf', 'start_index': 229665}]\n",
      "* The k-nearest neighbor model differs from the other interpretable models presented in this book\n",
      "because it is an instance-based learning algorithm. How can k-nearest neighbors be interpreted?\n",
      "First of all, there are no parameters to learn, so there is no interpretability on a modular level.\n",
      "Furthermore, there is a lack of global model interpretability because the model is inherently local\n",
      "and there are no global weights or structures explicitly learned. Maybe it is interpretable at the\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "109 [{'source': './data/xai.pdf', 'start_index': 237495}]\n",
      "* predicted probability changes from 2% to 10% (or close to 10%)? Another quality criterion is that a\n",
      "counterfactual should be as similar as possible to the instance regarding feature values. This\n",
      "requires a distance measure between two instances. The counterfactual should not only be close to\n",
      "the original instance, but should also change as few features as possible. This can be achieved\n",
      "by selecting an appropriate distance measure like the Manhattan distance. The last requirement is\n",
      "that a counterfactual instance should have feature values that are likely. It would not make\n",
      "sense to generate a counterfactual explanation for the rent example where the size of an apartment\n",
      "is negative or the number of rooms is set to 200. It is even better when the counterfactual is likely\n",
      "according to the joint distribution of the data, e.g. an apartment with 10 rooms and 20 m² should\n",
      "not be regarded as counterfactual explanation. [{'source': './data/xai.pdf', 'start_index': 383376}]\n",
      "* Naive Bayes is an interpretable model because of the independence assumption. It can be interpreted\n",
      "on the modular level. It is very clear for each feature how much it contributes towards a certain class\n",
      "prediction, since we can interpret the conditional probability.\n",
      "\n",
      "K-Nearest Neighbors\n",
      "\n",
      "The k-nearest neighbor method can be used for regression and classification and uses the nearest\n",
      "neighbors of a data point for prediction. For classification, the k-nearest neighbor method assigns\n",
      "the most common class of the nearest neighbors of an instance. For regression, it takes the average of\n",
      "the outcome of the neighbors. The tricky parts are finding the right k and deciding how to measure\n",
      "the distance between instances, which ultimately defines the neighborhood. [{'source': './data/xai.pdf', 'start_index': 236730}]\n",
      "* ⁸²https://github.com/marcotcr/lime/tree/ce2db6f20f47c3330beb107bb17fd25840ca4606\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "Example\n",
      "\n",
      "172\n",
      "\n",
      "Let us look at a concrete example. We go back to the bike rental data and turn the prediction problem\n",
      "into a classification: After taking into account the trend that the bicycle rental has become more\n",
      "popular over time, we want to know on a certain day whether the number of bicycles rented will be\n",
      "above or below the trend line. You can also interpret “above” as being above the average number of\n",
      "bicycles, but adjusted for the trend.\n",
      "\n",
      "First we train a random forest with 100 trees on the classification task. On what day will the number\n",
      "of rental bikes be above the trend-free average, based on weather and calendar information?\n",
      "\n",
      "The explanations are created with 2 features. The results of the sparse local linear models trained\n",
      "for two instances with different predicted classes: [{'source': './data/xai.pdf', 'start_index': 340156}]\n",
      "* One way to measure how well the surrogate replicates the black box model is the R-squared measure:\n",
      "\n",
      "R2 = 1 (cid:0) SSE\n",
      "SST\n",
      "\n",
      "= 1 (cid:0)\n",
      "\n",
      "∑\n",
      "i=1(^y(i)\n",
      "n\n",
      "(cid:3) (cid:0) ^y(i))2\n",
      "∑\n",
      "i=1(^y(i) (cid:0) (cid:22)^y)2\n",
      "n\n",
      "\n",
      "where ^y(i)\n",
      "is the prediction for the i-th instance of the surrogate model, ^y(i) the prediction of the\n",
      "(cid:3)\n",
      "black box model and (cid:22)^y the mean of the black box model predictions. SSE stands for sum of squares\n",
      "error and SST for sum of squares total. The R-squared measure can be interpreted as the percentage\n",
      "of variance that is captured by the surrogate model. If R-squared is close to 1 (= low SSE), then the\n",
      "interpretable model approximates the behavior of the black box model very well. If the interpretable\n",
      "model is very close, you might want to replace the complex model with the interpretable model. If\n",
      "the R-squared is close to 0 (= high SSE), then the interpretable model fails to explain the black box\n",
      "model. [{'source': './data/xai.pdf', 'start_index': 326166}]\n",
      "* There is a solution to all these problems:\n",
      "\n",
      "Problem: The target outcome y given the features does not follow a Gaussian distribution.\n",
      "Example: Suppose I want to predict how many minutes I will ride my bike on a given day. As features\n",
      "I have the type of day, the weather and so on. If I use a linear model, it could predict negative minutes\n",
      "because it assumes a Gaussian distribution which does not stop at 0 minutes. Also if I want to predict\n",
      "probabilities with a linear model, I can get probabilities that are negative or greater than 1.\n",
      "Solution: Generalized Linear Models (GLMs).\n",
      "\n",
      "Problem: The features interact.\n",
      "Example: On average, light rain has a slight negative effect on my desire to go cycling. But in\n",
      "summer, during rush hour, I welcome rain, because then all the fair-weather cyclists stay at home\n",
      "and I have the bicycle paths for myself! This is an interaction between time and weather that cannot\n",
      "be captured by a purely additive model.\n",
      "Solution: Adding interactions manually. [{'source': './data/xai.pdf', 'start_index': 138340}]\n",
      "* Before discussing how to create counterfactuals, I would like to discuss some use cases for\n",
      "counterfactuals and how a good counterfactual explanation looks like.\n",
      "\n",
      "In this first example, Peter applies for a loan and gets rejected by the (machine learning powered)\n",
      "banking software. He wonders why his application was rejected and how he might improve his\n",
      "chances to get a loan. The question of “why” can be formulated as a counterfactual: What is the\n",
      "smallest change to the features (income, number of credit cards, age, …) that would change the\n",
      "prediction from rejected to approved? One possible answer could be: If Peter would earn 10,000\n",
      "Euro more per year, he would get the loan. Or if Peter had fewer credit cards and had not defaulted\n",
      "on a loan 5 years ago, he would get the loan. Peter will never know the reasons for the rejection, as\n",
      "the bank has no interest in transparency, but that is another story. [{'source': './data/xai.pdf', 'start_index': 379236}]\n",
      "* Computing the global feature importances reveals that temperature and time trend are the most\n",
      "important features:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "102\n",
      "\n",
      "Feature importance measures for a RuleFit model predicting bike counts. The most important features for the\n",
      "predictions were temperature and time trend.\n",
      "\n",
      "The feature importance measurement includes the importance of the raw feature term and all the\n",
      "decision rules in which the feature appears.\n",
      "\n",
      "Interpretation template\n",
      "\n",
      "The interpretation is analogous to linear models: The predicted outcome changes by (cid:12)j if feature xj\n",
      "changes by one unit, provided all other features remain unchanged. The weight interpretation of a\n",
      "decision rule is a special case: If all conditions of a decision rule rk apply, the predicted outcome\n",
      "changes by (cid:11)k (the learned weight of rule rk in the linear model). [{'source': './data/xai.pdf', 'start_index': 223596}]\n",
      "* Machine learning will fuel a lot of things.\n",
      "\n",
      "Based on the principle “Whatever can be automated will be automated”, I conclude that whenever\n",
      "possible, tasks will be formulated as prediction problems and solved with machine learning. Machine\n",
      "learning is a form of automation or can at least be part of it. Many tasks currently performed by\n",
      "humans are replaced by machine learning. Here are some examples of tasks where machine learning\n",
      "is used to automate parts of it:\n",
      "\n",
      "• Sorting / decision-making / completion of documents (e.g. in insurance companies, the legal\n",
      "\n",
      "sector or consulting firms)\n",
      "\n",
      "• Data-driven decisions such as credit applications\n",
      "• Drug discovery\n",
      "• Quality controls in assembly lines\n",
      "• Self-driving cars\n",
      "• Diagnosis of diseases\n",
      "• Translation. For this book, I used a translation service called (DeepL¹¹⁷) powered by deep neural\n",
      "networks to improve my sentences by translating them from English into German and back\n",
      "into English.\n",
      "\n",
      "• … [{'source': './data/xai.pdf', 'start_index': 480153}]\n",
      "* Interpretable Models\n",
      "\n",
      "65\n",
      "\n",
      "how well you slept the night before on a scale of 1 to 10 and whether you had to work on that day.\n",
      "The goal is to predict the number of coffees given the features stress, sleep and work. I simulated\n",
      "data for 200 days. Stress and sleep were drawn uniformly between 1 and 10 and work yes/no was\n",
      "drawn with a 50/50 chance (what a life!). For each day, the number of coffees was then drawn from\n",
      "a Poisson distribution, modelling the intensity (cid:21) (which is also the expected value of the Poisson\n",
      "distribution) as a function of the features sleep, stress and work. You can guess where this story will\n",
      "lead: “Hey, let us model this data with a linear model … Oh it does not work … Let us try a GLM with\n",
      "Poisson distribution … SURPRISE! Now it works!”. I hope I did not spoil the story too much for you.\n",
      "\n",
      "Let us look at the distribution of the target variable, the number of coffees on a given day:\n",
      "\n",
      "Simulated distribution of number of daily coffees for 200 days. [{'source': './data/xai.pdf', 'start_index': 145398}]\n",
      "* We need a couple of ingredients to find prototypes and criticisms for a dataset with MMD-critic. As\n",
      "the most basic ingredient, we need a kernel function to estimate the data densities. A kernel is a\n",
      "function that weighs two data points according to their proximity. Based on density estimates, we\n",
      "need a measure that tells us how different two distributions are so that we can determine whether the\n",
      "distribution of the prototypes we select is close to the data distribution. This is solved by measuring\n",
      "the maximum mean discrepancy (MMD). Also based on the kernel function, we need the witness\n",
      "function to tell us how different two distributions are at a particular data point. With the witness\n",
      "function, we can select criticisms, i.e. data points at which the distribution of prototypes and data\n",
      "diverges and the witness function takes on large absolute values. The last ingredient is a search\n",
      "strategy for good prototypes and criticisms, which is solved with a simple greedy search. [{'source': './data/xai.pdf', 'start_index': 421661}]\n",
      "* Interpretable Models\n",
      "\n",
      "69\n",
      "\n",
      "Intercept\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "workY\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "temp\n",
      "25\n",
      "12\n",
      "30\n",
      "5\n",
      "\n",
      "The first column is the intercept term. The second column encodes the categorical feature, with 0\n",
      "for the reference category and 1 for the other. The third column contains the temperature.\n",
      "\n",
      "If we want the linear model to consider the interaction between temperature and the workingday\n",
      "feature, we have to add a column for the interaction:\n",
      "\n",
      "Intercept\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "workY\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "temp\n",
      "25\n",
      "12\n",
      "30\n",
      "5\n",
      "\n",
      "workY.temp\n",
      "25\n",
      "0\n",
      "0\n",
      "5 [{'source': './data/xai.pdf', 'start_index': 151634}]\n",
      "* p\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "45\n",
      "\n",
      "The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per\n",
      "feature.\n",
      "\n",
      "The largest contributions to the expected number of rented bicycles comes from the temperature\n",
      "feature and the days feature, which captures the trend of bike rentals over time. The temperature\n",
      "has a broad range of how much it contributes to the prediction. The day trend feature goes from zero\n",
      "to large positive contributions, because the first day in the dataset (01.01.2011) has a very small trend\n",
      "effect and the estimated weight for this feature is positive (4.93). This means that the effect increases\n",
      "with each day and is highest for the last day in the dataset (31.12.2012). Note that for effects with\n",
      "a negative weight, the instances with a positive effect are those that have a negative feature value.\n",
      "For example, days with a high negative effect of windspeed are the ones with high wind speeds.\n",
      "\n",
      "Explain Individual Predictions [{'source': './data/xai.pdf', 'start_index': 108994}]\n",
      "* screening.” In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017).\n",
      "\n",
      "³³https://github.com/christophM/interpretable-ml-book/blob/master/R/get-cervical-cancer-dataset.R\n",
      "³⁴https://github.com/christophM/interpretable-ml-book/blob/master/data/cervical.RData\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "The easiest way to achieve interpretability is to use only a subset of algorithms that create\n",
      "interpretable models. Linear regression, logistic regression and the decision tree are commonly used\n",
      "interpretable models.\n",
      "\n",
      "In the following chapters we will talk about these models. Not in detail, only the basics, because\n",
      "there is already a ton of books, videos, tutorials, papers and more material available. We will focus\n",
      "on how to interpret the models. The book discusses linear regression, logistic regression, other linear\n",
      "regression extensions, decision trees, decision rules and the RuleFit algorithm in more detail. It also\n",
      "lists other interpretable models. [{'source': './data/xai.pdf', 'start_index': 90257}]\n",
      "* interpretations of the model. There are already first products and I argue that for many applications\n",
      "it will be sufficient to use these automated machine learning services. Today anyone can build\n",
      "websites without knowing HTML, CSS and Javascript, but there are still many web developers\n",
      "around. Similarly, I believe that everyone will be able to train machine learning models without\n",
      "knowing how to program, and there will still be a need for machine learning experts. [{'source': './data/xai.pdf', 'start_index': 484146}]\n",
      "* ¹¹³https://github.com/christophM/interpretable-ml-book/blob/master/manuscript/06.5-example-based-influence-fct.Rmd\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "233\n",
      "\n",
      "For linear models and generalized linear models many influence measures like Cook’s distance are\n",
      "implemented in R in the stats package.\n",
      "\n",
      "Koh and Liang published the Python code for influence functions from their paper in a repository¹¹⁴.\n",
      "That is great! Unfortunately it is “only” the code of the paper and not a maintained and documented\n",
      "Python module. The code is focused on the Tensorflow library, so you cannot use it directly for black\n",
      "box models using other frameworks, like sci-kit learn.\n",
      "\n",
      "Keita Kurita wrote a great blog post for influence functions¹¹⁵ that helped me understand Koh\n",
      "and Liang’s paper better. The blog post goes a little deeper into the mathematics behind influence\n",
      "functions for black box models and also talks about some of the mathematical ‘tricks’ with which\n",
      "the method is efficiently implemented. [{'source': './data/xai.pdf', 'start_index': 472632}]\n",
      "* Interpretable Models\n",
      "\n",
      "107\n",
      "\n",
      "then 10. In the cases where the second rule applies, the first rule applies as well. The interpretation of\n",
      "the estimated weight for the second rule is: “Assuming all other features remain fixed, the predicted\n",
      "number of bikes increases by (cid:12)2 when the weather is good and temperature above 15 degrees.”. But,\n",
      "now it becomes really clear that the ‘all other feature fixed’ is problematic, because if rule 2 applies,\n",
      "also rule 1 applies and the interpretation is nonsensical.\n",
      "\n",
      "Software and Alternative\n",
      "\n",
      "The RuleFit algorithm is implemented in R by Fokkema and Christoffersen (2017)⁵⁸ and you can find\n",
      "a Python version on Github⁵⁹.\n",
      "\n",
      "A very similar framework is scope-rules⁶⁰, a Python module that also extracts rules from ensembles.\n",
      "It differs in the way it learns the final rules: First, similar and duplicate rules are removed. Then\n",
      "scope-rules chooses rules based on recall and precision instead of relying on Lasso. [{'source': './data/xai.pdf', 'start_index': 234113}]\n",
      "* Fill the earth with heat.\n",
      "\n",
      "Everything is dying,\n",
      "\n",
      "And we are complying.\n",
      "\n",
      "Like horses with blinders we race the race of our own creation,\n",
      "\n",
      "Towards the Great Filter of civilization.\n",
      "\n",
      "And so we march on relentlessly.\n",
      "\n",
      "As we are part of the machine.\n",
      "\n",
      "Embracing entropy.\n",
      "\n",
      "“A dark memory,” the teacher said to break the silence in the room. “It will be uploaded to your\n",
      "library. Your homework is to memorise it until next week.” Xola sighed. She managed to catch one\n",
      "of the little drones. The drone was warm from the CPU and the engines. Xola liked how it warmed\n",
      "her hands.\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "10\n",
      "\n",
      "What Is Machine Learning?\n",
      "\n",
      "Machine learning is a set of methods that computers use to make and improve predictions or\n",
      "behaviors based on data. [{'source': './data/xai.pdf', 'start_index': 23236}]\n",
      "* to describe an instance is usually not useful. It works well if there are only a handful of features or\n",
      "if we have a way to summarize an instance. [{'source': './data/xai.pdf', 'start_index': 372222}]\n",
      "* You: I want to know the influence a training instance has on a particular prediction.\n",
      "Research: You can delete the training instance, retrain the model, and measure the difference in the\n",
      "prediction.\n",
      "You: Great! But do you have a method for me that works without retraining? It takes so much time.\n",
      "Research: Do you have a model with a loss function that is twice differentiable with respect to its\n",
      "parameters?\n",
      "You: I trained a neural network with the logistic loss. So yes.\n",
      "Research: Then you can approximate the influence of the instance on the model parameters\n",
      "and on the prediction with influence functions. The influence function is a measure of how\n",
      "strongly the model parameters or predictions depend on a training instance. Instead of deleting the\n",
      "instance, the method upweights the instance in the loss by a very small step. This method involves\n",
      "approximating the loss around the current model parameters using the gradient and Hessian matrix. [{'source': './data/xai.pdf', 'start_index': 453556}]\n",
      "* Machine learning (or “AI”) is associated with a lot of promises and expectations. But let’s start with\n",
      "a less optimistic observation: While science develops a lot of fancy machine learning tools, in my\n",
      "experience it is quite difficult to integrate them into existing processes and products. Not because it\n",
      "is not possible, but simply because it takes time for companies and institutions to catch up. In the\n",
      "gold rush of the current AI hype, companies open up “AI labs”, “Machine Learning Units” and hire\n",
      "“Data Scientists”, “Machine Learning Experts”, “AI engineers”, and so on, but the reality is, in my\n",
      "experience, rather frustrating. Often companies do not even have data in the required form and the\n",
      "data scientists wait idle for months. Sometimes companies have such high expectation of AI and\n",
      "Data Science due to the media that data scientists could never fulfill them. And often nobody knows\n",
      "\n",
      "¹¹⁶http://www.decisionproblem.com/paperclips/index2.html\n",
      "\n",
      "\fA Look into the Crystal Ball\n",
      "\n",
      "236 [{'source': './data/xai.pdf', 'start_index': 477802}]\n",
      "* Description\n",
      "days_since_2011 > 111 & weathersit in (“GOOD”, “MISTY”)\n",
      "37.25 <= hum <= 90\n",
      "days_since_2011 > 428 & temp > 5\n",
      "temp > 13 & days_since_2011 > 554\n",
      "temp > 8 & weathersit in (“GOOD”, “MISTY”)\n",
      "\n",
      "Weight\n",
      "664\n",
      "-17\n",
      "460\n",
      "550\n",
      "409\n",
      "\n",
      "Importance\n",
      "253\n",
      "227\n",
      "225\n",
      "194\n",
      "188\n",
      "\n",
      "The most important rule was: “days_since_2011 > 111 & weathersit in (“GOOD”, “MISTY”)” and\n",
      "the corresponding weight is 664. The interpretation is: If days_since_2011 > 111 & weathersit in\n",
      "(“GOOD”, “MISTY”), then the predicted number of bikes increases by 664, when all other feature\n",
      "values remain fixed. In total, 278 such rules were created from the original 8 features. Quite a lot!\n",
      "But thanks to Lasso, only 39 of the 278 have a weight different from 0.\n",
      "\n",
      "Computing the global feature importances reveals that temperature and time trend are the most\n",
      "important features:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "102 [{'source': './data/xai.pdf', 'start_index': 222880}]\n",
      "* Derivative ICE Plot\n",
      "\n",
      "Another way to make it visually easier to spot heterogeneity is to look at the individual derivatives\n",
      "of the prediction function with respect to a feature. The resulting plot is called the derivative ICE\n",
      "plot (d-ICE). The derivatives of a function (or curve) tell you whether changes occur and in which\n",
      "direction they occur. With the derivative ICE plot, it is easy to spot ranges of feature values where\n",
      "the black box predictions change for (at least some) instances. If there is no interaction between the\n",
      "analyzed feature xS and the other features xC, then the prediction function can be expressed as:\n",
      "\n",
      "^f (x) = ^f (xS; xC) = g(xS) + h(xC); with\n",
      "\n",
      "(cid:14) ^f (x)\n",
      "(cid:14)xS [{'source': './data/xai.pdf', 'start_index': 258898}]\n",
      "* For deletion diagnostics and influence functions, we considered the difference in the prediction and\n",
      "for the influence function the increase of the loss. But, really, the approach is generalizable to any\n",
      "question of the form: “What happens to … when we delete or upweight instance z?”, where you\n",
      "can fill “…” with any function of your model of your desire. You can analyze how much a training\n",
      "instance influences the overall loss of the model. You can analyze how much a training instance\n",
      "influences the feature importance. You can analyze how much a training instance influences which\n",
      "feature is selected for the first split when training a decision tree.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "232\n",
      "\n",
      "Disadvantages of Identifying Influential Instances [{'source': './data/xai.pdf', 'start_index': 468899}]\n",
      "* • Temperature in degrees Celsius.\n",
      "• Relative humidity in percent (0 to 100).\n",
      "• Wind speed in km per hour.\n",
      "\n",
      "For the examples in this book, the data has been slightly processed. You can find the processing\n",
      "R-script in the book’s Github repository²⁴ together with the final RData file²⁵.\n",
      "\n",
      "²¹https://www.capitalbikeshare.com/\n",
      "²²Fanaee-T, Hadi, and Joao Gama. “Event labeling combining ensemble detectors and background knowledge.” Progress in Artificial\n",
      "\n",
      "Intelligence. Springer Berlin Heidelberg, 1–15. doi:10.1007/s13748-013-0040-3. (2013).\n",
      "\n",
      "²³http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
      "²⁴https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R\n",
      "²⁵https://github.com/christophM/interpretable-ml-book/blob/master/data/bike.RData\n",
      "\n",
      "\fDatasets\n",
      "\n",
      "35\n",
      "\n",
      "YouTube Spam Comments (Text Classification) [{'source': './data/xai.pdf', 'start_index': 85123}]\n",
      "* That is it with theory and intuition. The next section explains how influence functions can be applied.\n",
      "\n",
      "Application of Influence Functions\n",
      "\n",
      "Influence functions have many applications, some of which have already been presented in this\n",
      "chapter.\n",
      "\n",
      "Understanding model behavior [{'source': './data/xai.pdf', 'start_index': 462811}]\n",
      "* does not capture the true goal with all its constraints that we really strive for: For example, we\n",
      "do not appreciate a company killing people to make money, poisoning rivers, or simply printing\n",
      "its own money. We have invented laws, regulations, sanctions, compliance procedures, labor\n",
      "unions and more to patch up the imperfect goal specification. Another example that you can\n",
      "experience for yourself is Paperclips¹¹⁶, a game in which you play a machine with the goal of\n",
      "producing as many paperclips as possible. WARNING: It is addictive. I do not want to spoil it too\n",
      "much, but let’s say things get out of control really fast. In machine learning, the imperfections in\n",
      "the goal specification come from imperfect data abstractions (biased populations, measurement\n",
      "errors, …), unconstrained loss functions, lack of knowledge of the constraints, shifting of the\n",
      "distribution between training and application data and much more. [{'source': './data/xai.pdf', 'start_index': 476390}]\n",
      "* memory of the average human. I argue that you cannot really imagine a linear model with 5 features,\n",
      "because it would mean drawing the estimated hyperplane mentally in a 5-dimensional space. Any\n",
      "feature space with more than 3 dimensions is simply inconceivable for humans. Usually, when people\n",
      "try to comprehend a model, they consider only parts of it, such as the weights in linear models. [{'source': './data/xai.pdf', 'start_index': 54931}]\n",
      "* M ADj = mediani2f1;:::;ng(jxi;j (cid:0) medianl2f1;:::;ng(xl;j)j)\n",
      "\n",
      "The median of a vector is the value at which half of the vector values are greater and the other half\n",
      "smaller. The MAD is the equivalent of the variance of a feature, but instead of using the mean as the\n",
      "center and summing over the square distances, we use the median as the center and sum over the\n",
      "absolute distances. The proposed distance function has the advantage over the Euclidean distance\n",
      "that it introduces sparsity. This means that two points are closer to each other when less features\n",
      "are different. And it is more robust to outliers. Scaling with the MAD is necessary to bring all the\n",
      "features to the same scale – it should not matter whether you measure the size of an apartment in\n",
      "square meters or square feet.\n",
      "\n",
      "The recipe for producing the counterfactuals is simple:\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "195\n",
      "\n",
      "1. Select an instance x to be explained, the desired outcome y’, a tolerance ϵ and a (low) initial [{'source': './data/xai.pdf', 'start_index': 388019}]\n",
      "* I want to integrate prior knowledge into my models.\n",
      "Search for Bayesian inference.\n",
      "\n",
      "I am feeling a bit down lately.\n",
      "Search for “Amazon Alexa Gone Wild!!! Full version from beginning to end”.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "Decision Tree\n",
      "\n",
      "79\n",
      "\n",
      "Linear regression and logistic regression models fail in situations where the relationship between\n",
      "features and outcome is nonlinear or where features interact with each other. Time to shine for the\n",
      "decision tree! Tree based models split the data multiple times according to certain cutoff values in the\n",
      "features. Through splitting, different subsets of the dataset are created, with each instance belonging\n",
      "to one subset. The final subsets are called terminal or leaf nodes and the intermediate subsets are\n",
      "called internal nodes or split nodes. To predict the outcome in each leaf node, the average outcome\n",
      "of the training data in this node is used. Trees can be used for classification and regression. [{'source': './data/xai.pdf', 'start_index': 170890}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "226\n",
      "\n",
      "you will have to retrain your model thousands of times. Assuming the model takes one day to\n",
      "train and you have 1000 training instances, then the computation of influential instances – without\n",
      "parallelization – will take almost 3 years. Nobody has time for this. In the rest of this chapter, I will\n",
      "show you a method that does not require retraining the model.\n",
      "\n",
      "Influence Functions [{'source': './data/xai.pdf', 'start_index': 453141}]\n",
      "* Feature matrix:\n",
      "\n",
      "0\n",
      "\n",
      "1 (cid:0)1 (cid:0)1\n",
      "B\n",
      "1 (cid:0)1 (cid:0)1\n",
      "B\n",
      "B\n",
      "B\n",
      "0\n",
      "1\n",
      "1\n",
      "B\n",
      "B\n",
      "B\n",
      "0\n",
      "1\n",
      "1\n",
      "B\n",
      "@\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "A\n",
      "\n",
      "Dummy coding\n",
      "\n",
      "The (cid:12) per category is the estimated mean value of y for each category (given all other feature values\n",
      "are zero or the reference category). Note that the intercept has been omitted here so that a unique\n",
      "solution can be found for the linear model weights.\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "Feature matrix:\n",
      "\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "@\n",
      "\n",
      "1 0\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "0 0\n",
      "0 0\n",
      "\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "C\n",
      "A\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "49\n",
      "\n",
      "If you want to dive a little deeper into the different encodings of categorical features, checkout this\n",
      "overview webpage³⁶ and this blog post³⁷.\n",
      "\n",
      "Do Linear Models Create Good Explanations? [{'source': './data/xai.pdf', 'start_index': 114263}]\n",
      "* Can we do the same for any type of model? It would be great to have this as a model-agnostic tool.\n",
      "Since we usually do not have similar weights in other model types, we need a different solution.\n",
      "\n",
      "Help comes from unexpected places: cooperative game theory. The Shapley value is a solution for\n",
      "computing feature contributions for single predictions for any machine learning model.\n",
      "\n",
      "The Shapley Value\n",
      "\n",
      "The Shapley value is defined via a value function val of players in S.\n",
      "\n",
      "The Shapley value of a feature value is its contribution to the payout, weighted and summed over\n",
      "all possible feature value combinations:\n",
      "\n",
      "ϕj(val) =\n",
      "\n",
      "∑\n",
      "\n",
      "S(cid:18)fx1;:::;xpgnfxj g\n",
      "\n",
      "jSj! (p (cid:0) jSj (cid:0) 1)!\n",
      "p!\n",
      "\n",
      "(val (S [ fxjg) (cid:0) val(S))\n",
      "\n",
      "where S is a subset of the features used in the model, x is the vector of feature values of the instance\n",
      "to be explained and p the number of features. valx(S) is the prediction for feature values in set S that\n",
      "are marginalized over features that are not included in set S:\n",
      "\n",
      "∫ [{'source': './data/xai.pdf', 'start_index': 358558}]\n",
      "* Let us start with the maximum mean discrepancy (MMD), which measures the discrepancy\n",
      "between two distributions. The selection of prototypes creates a density distribution of prototypes.\n",
      "We want to evaluate whether the prototypes distribution differs from the data distribution. We\n",
      "estimate both with kernel density functions. The maximum mean discrepancy measures the\n",
      "difference between two distributions, which is the supremum over a function space of differences\n",
      "between the expectations according to the two distributions. All clear? Personally, I understand these\n",
      "\n",
      "¹⁰⁷Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.”\n",
      "\n",
      "Advances in Neural Information Processing Systems (2016).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "210\n",
      "\n",
      "concepts much better when I see how something is calculated with data. The following formula\n",
      "shows how to calculate the squared MMD measure (MMD2):\n",
      "\n",
      "M M D2 =\n",
      "\n",
      "1\n",
      "m2\n",
      "\n",
      "m∑\n",
      "\n",
      "i;j=1\n",
      "\n",
      "k(zi; zj) (cid:0) 2\n",
      "mn [{'source': './data/xai.pdf', 'start_index': 422647}]\n",
      "* LIME for Images\n",
      "\n",
      "This section was written by Verena Haunschmid.\n",
      "\n",
      "LIME for images works differently than LIME for tabular data and text. Intuitively, it would not\n",
      "make much sense to perturb individual pixels, since many more than one pixel contribute to one\n",
      "class. Randomly changing individual pixels would probably not change the predictions by much.\n",
      "Therefore, variations of the images are created by segmenting the image into “superpixels” and\n",
      "turning superpixels off or on. Superpixels are interconnected pixels with similar colors and can be\n",
      "turned off by replacing each pixel with a user-defined color such as gray. The user can also specify\n",
      "a probability for turning off a superpixel in each permutation.\n",
      "\n",
      "Example [{'source': './data/xai.pdf', 'start_index': 344012}]\n",
      "* Et(cid:24)T [d(t(x′); t(x))] < ϵ\n",
      "\n",
      "and x 2 [0; 1]d\n",
      "\n",
      "I think we should be concerned about the possibilities this method enables. The other methods are\n",
      "based on the manipulation of digital images. However, these 3D-printed, robust adversarial examples\n",
      "can be inserted into any real scene and deceive a computer to wrongly classify an object. Let us turn\n",
      "it around: What if someone creates a rifle which looks like a turtle?\n",
      "\n",
      "The blindfolded adversary: Black box attack [{'source': './data/xai.pdf', 'start_index': 410454}]\n",
      "* RuleFit\n",
      "\n",
      "100\n",
      "\n",
      "The RuleFit algorithm by Friedman and Popescu (2008)⁵⁷ learns sparse linear models that include\n",
      "automatically detected interaction effects in the form of decision rules.\n",
      "\n",
      "The linear regression model does not account for interactions between features. Would it not be\n",
      "convenient to have a model that is as simple and interpretable as linear models, but also integrates\n",
      "feature interactions? RuleFit fills this gap. RuleFit learns a sparse linear model with the original\n",
      "features and also a number of new features that are decision rules. These new features capture\n",
      "interactions between the original features. RuleFit automatically generates these features from\n",
      "decision trees. Each path through a tree can be transformed into a decision rule by combining the\n",
      "split decisions into a rule. The node predictions are discarded and only the splits are used in the\n",
      "decision rules:\n",
      "\n",
      "4 rules can be generated from a tree with 3 terminal nodes. [{'source': './data/xai.pdf', 'start_index': 219674}]\n",
      "* Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. “Transfer learning with partial observ-\n",
      "ability applied to cervical cancer screening.” In Iberian Conference on Pattern Recognition and Image\n",
      "Analysis, 243–50. Springer. (2017).\n",
      "\n",
      "Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “Model Class Reliance: Variable im-\n",
      "portance measures for any machine learning model class, from the ‘Rashomon’ perspective.”\n",
      "http://arxiv.org/abs/1801.01489 (2018).\n",
      "\n",
      "\fReferences\n",
      "\n",
      "244\n",
      "\n",
      "Fokkema, Marjolein, and Benjamin Christoffersen. “Pre: Prediction rule ensembles”. https://CRAN.R-\n",
      "project.org/package=pre (2017).\n",
      "\n",
      "Friedman, Jerome H, and Bogdan E Popescu. “Predictive learning via rule ensembles.” The Annals\n",
      "of Applied Statistics. JSTOR, 916–54. (2008).\n",
      "\n",
      "Friedman, Jerome H. “Greedy function approximation: A gradient boosting machine.” Annals of\n",
      "statistics (2001): 1189-1232. [{'source': './data/xai.pdf', 'start_index': 495419}]\n",
      "* Influence functions are a good alternative to deletion diagnostics, but only for models with\n",
      "differentiable parameters, such as neural networks. They do not work for tree-based methods\n",
      "like random forests, boosted trees or decision trees. Even if you have models with parameters and\n",
      "a loss function, the loss may not be differentiable. But for the last problem, there is a trick: Use\n",
      "a differentiable loss as substitute for calculating the influence when, for example, the underlying\n",
      "model uses the Hinge loss instead of some differentiable loss. The loss is replaced by a smoothed\n",
      "version of the problematic loss for the influence functions, but the model can still be trained with\n",
      "the non-smooth loss. [{'source': './data/xai.pdf', 'start_index': 470175}]\n",
      "* For the bike rental dataset, we also train a random forest to predict the number of rented bikes\n",
      "for a day, given weather and calendar information. The explanations created for the random forest\n",
      "prediction of a particular day:\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "182\n",
      "\n",
      "Shapley values for day 285. With a predicted 2329 rental bikes, this day is -2189 below the average prediction of 4517.\n",
      "The weather situation and humidity and had the largest negative contributions. The temperature on this day had a\n",
      "positive contribution. The sum of Shapley values yields the difference of actual and average prediction (-2189).\n",
      "\n",
      "Be careful to interpret the Shapley value correctly: The Shapley value is the average contribution of\n",
      "a feature value to the prediction in different coalitions. The Shapley value is NOT the difference in\n",
      "prediction when we would remove the feature from the model.\n",
      "\n",
      "The Shapley Value in Detail [{'source': './data/xai.pdf', 'start_index': 356185}]\n",
      "* ⁴¹Friedman,\n",
      "\n",
      "Jerome,\n",
      "\n",
      "Trevor\n",
      "\n",
      "Hastie,\n",
      "\n",
      "and\n",
      "\n",
      "Robert\n",
      "\n",
      "Tibshirani.\n",
      "\n",
      "“The\n",
      "\n",
      "elements\n",
      "\n",
      "of\n",
      "\n",
      "statistical\n",
      "\n",
      "learning”.\n",
      "\n",
      "www.web.stanford.edu/(cid:24)hastie/ElemStatLearn/ (2009).\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "80\n",
      "\n",
      "Decision tree with artificial data. Instances with a value greater than 3 for feature x1 end up in node 5. All other\n",
      "instances are assigned to node 3 or node 4, depending on whether values of feature x2 exceed 1.\n",
      "\n",
      "The following formula describes the relationship between the outcome y and features x.\n",
      "\n",
      "^y = ^f (x) =\n",
      "\n",
      "M∑\n",
      "\n",
      "m=1\n",
      "\n",
      "cmIfx 2 Rmg\n",
      "\n",
      "Each instance falls into exactly one leaf node (=subset Rm). Ifx2Rmg is the identity function that\n",
      "returns 1 if x is in the subset Rm and 0 otherwise. If an instance falls into a leaf node Rl, the predicted\n",
      "outcome is ^y = cl, where cl is the average of all training instances in leaf node Rl. [{'source': './data/xai.pdf', 'start_index': 172429}]\n",
      "* was 160.5, you would have a score of 0.51\n",
      "\n",
      "Advantages\n",
      "\n",
      "The interpretation of counterfactual explanations is very clear. If the feature values of an\n",
      "instance are changed according to the counterfactual, the prediction changes to the predefined\n",
      "prediction. There are no additional assumptions and no magic in the background. This also means\n",
      "it is not as dangerous as methods like LIME, where it is unclear how far we can extrapolate the local\n",
      "model for the interpretation.\n",
      "\n",
      "The counterfactual method creates a new instance, but we can also summarize a counterfactual by\n",
      "reporting which feature values have changed. This gives us two options for reporting our results.\n",
      "You can either report the counterfactual instance or highlight which features have been changed\n",
      "between the instance of interest and the counterfactual instance. [{'source': './data/xai.pdf', 'start_index': 391872}]\n",
      "* The influence measures only take into account the deletion of individual instances and not the\n",
      "deletion of several instances at once. Larger groups of data instances may have some interactions\n",
      "that strongly influence model training and prediction. But the problem lies in combinatorics: There\n",
      "are n possibilities to delete an individual instance from the data. There are n times (n-1) possibilities\n",
      "to delete two instances from the training data. There are n times (n-1) times (n-2) possibilities to\n",
      "delete three … I guess you can see where this is going, there are just too many combinations.\n",
      "\n",
      "Software and Alternatives\n",
      "\n",
      "Deletion diagnostics are very simple to implement. Take a look at the code¹¹³ I wrote for the examples\n",
      "in this chapter.\n",
      "\n",
      "¹¹³https://github.com/christophM/interpretable-ml-book/blob/master/manuscript/06.5-example-based-influence-fct.Rmd\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "233 [{'source': './data/xai.pdf', 'start_index': 471889}]\n",
      "* 1. OneR learns rules from a single feature. OneR is characterized by its simplicity, interpretability\n",
      "\n",
      "and its use as a benchmark.\n",
      "\n",
      "2. Sequential covering is a general procedure that iteratively learns rules and removes the\n",
      "data points that are covered by the new rule. This procedure is used by many rule learning\n",
      "algorithms.\n",
      "\n",
      "3. Bayesian Rule Lists combine pre-mined frequent patterns into a decision list using Bayesian\n",
      "statistics. Using pre-mined patterns is a common approach used by many rule learning\n",
      "algorithms.\n",
      "\n",
      "Let’s start with the simplest approach: Using the single best feature to learn rules.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "87\n",
      "\n",
      "Learn Rules from a Single Feature (OneR)\n",
      "\n",
      "The OneR algorithm suggested by Holte (1993)⁴⁴ is one of the simplest rule induction algorithms.\n",
      "From all the features, OneR selects the one that carries the most information about the outcome of\n",
      "interest and creates decision rules from this feature. [{'source': './data/xai.pdf', 'start_index': 187046}]\n",
      "* What intuition is behind this influence function formula? The formula comes from forming a\n",
      "quadratic expansion around the parameters ^(cid:18). That means we do not actually know, or it is too\n",
      "complex to calculate how exactly the loss of instance z will change when it is removed/upweighted.\n",
      "We approximate the function locally by using information about the steepness (= gradient) and the\n",
      "curvature (= Hessian matrix) at the current model parameter setting. With this loss approximation,\n",
      "we can calculate what the new parameters would approximately look like if we upweighted instance\n",
      "z:\n",
      "\n",
      "^(cid:18)(cid:0)z (cid:25) ^(cid:18) (cid:0) 1\n",
      "n\n",
      "\n",
      "Iup,params(z)\n",
      "\n",
      "The approximate parameter vector is basically the original parameter minus the gradient of the loss\n",
      "of z (because we want to decrease the loss) scaled by the curvature (= multiplied by the inverse\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "228\n",
      "\n",
      "Hessian matrix) and scaled by 1 over n, because that is the weight of a single training instance. [{'source': './data/xai.pdf', 'start_index': 458415}]\n",
      "* How can we protect our machine learning systems against adversarial examples? A proactive\n",
      "approach is the iterative retraining of the classifier with adversarial examples, also called adversarial\n",
      "training. Other approaches are based on game theory, such as learning invariant transformations\n",
      "of the features or robust optimization (regularization). Another proposed method is to use multiple\n",
      "classifiers instead of just one and have them vote the prediction (ensemble), but that has no guarantee\n",
      "to work, since they could all suffer from similar adversarial examples. Another approach that does\n",
      "not work well either is gradient masking, which constructs a model without useful gradients by\n",
      "using a nearest neighbor classifier instead of the original model. [{'source': './data/xai.pdf', 'start_index': 416436}]\n",
      "* ³⁸https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "55\n",
      "\n",
      "A linear model classifies tumors as malignant (1) or benign (0) given their size. The lines show the prediction of\n",
      "the linear model. For the data on the left, we can use 0.5 as classification threshold. After introducing a few more\n",
      "malignant tumor cases, the regression line shifts and a threshold of 0.5 no longer separates the classes. Points are\n",
      "slightly jittered to reduce over-plotting.\n",
      "\n",
      "Theory\n",
      "\n",
      "A solution for classification is logistic regression. Instead of fitting a straight line or hyperplane,\n",
      "the logistic regression model uses the logistic function to squeeze the output of a linear equation\n",
      "between 0 and 1. The logistic function is defined as:\n",
      "\n",
      "logistic((cid:17)) =\n",
      "\n",
      "1\n",
      "1 + exp((cid:0)(cid:17))\n",
      "\n",
      "And it looks like this:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "56\n",
      "\n",
      "The logistic function. It outputs numbers between 0 and 1. At input 0, it outputs 0.5. [{'source': './data/xai.pdf', 'start_index': 126574}]\n",
      "* Is this a realistic, relevant scenario at all? When you train a model, the learning algorithm minimizes\n",
      "the loss for the existing training data instances. Weird stuff can happen outside the distribution\n",
      "of training data, because the model is not penalized for doing weird stuff in these areas. Leaving\n",
      "the data distribution is called extrapolation, which can also be used to fool machine learning\n",
      "models, described in the chapter on adversarial examples. See in our little example how the partial\n",
      "dependence plots behave compared to ALE plots.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "135\n",
      "\n",
      "Comparison of the feature effects computed with PDP (upper row) and ALE (lower row). The PDP estimates are\n",
      "influenced by the odd behavior of the model outside the data distribution (steep jumps in the plots). The ALE plots\n",
      "correctly identify that the machine learning model has a linear relationship between features and prediction, ignoring\n",
      "areas without data. [{'source': './data/xai.pdf', 'start_index': 279234}]\n",
      "* communications 7.1 (1994): 39-59.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "190\n",
      "\n",
      "as well. Implicitly, some machine learning approaches work example-based. Decision trees partition\n",
      "the data into nodes based on the similarities of the data points in the features that are important\n",
      "for predicting the target. A decision tree gets the prediction for a new data instance by finding the\n",
      "instances that are similar (= in the same terminal node) and returning the average of the outcomes\n",
      "of those instances as the prediction. The k-nearest neighbors (knn) method works explicitly with\n",
      "example-based predictions. For a new instance, a knn model locates the k-nearest neighbors (e.g. the\n",
      "k=3 closest instances) and returns the average of the outcomes of those neighbors as a prediction.\n",
      "The prediction of a knn can be explained by returning the k neighbors, which – again – is only\n",
      "meaningful if we have a good way to represent a single instance. [{'source': './data/xai.pdf', 'start_index': 374280}]\n",
      "* This would also work for the difference in model parameters or the difference in the loss. In the\n",
      "following example we will use these simple influence measures.\n",
      "\n",
      "Deletion diagnostics example\n",
      "\n",
      "In the following example, we train a support vector machine to predict cervical cancer given risk\n",
      "factors and measure which training instances were most influential overall and for a particular\n",
      "prediction. Since the prediction of cancer is a classification problem, we measure the influence as the\n",
      "difference in predicted probability for cancer. An instance is influential if the predicted probability\n",
      "strongly increases or decreases on average in the dataset when the instance is removed from model\n",
      "training. The measurement of the influence for all 858 training instances requires to train the model\n",
      "once on all data and retrain it 858 times (= size of training data) with one of the instances removed\n",
      "each time. [{'source': './data/xai.pdf', 'start_index': 446748}]\n",
      "* Next, we apply the SBRL algorithm to the bike rental prediction task. This only works if the\n",
      "\n",
      "⁴⁸Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. “Scalable Bayesian rule lists.” Proceedings of the 34th International Conference on\n",
      "\n",
      "Machine Learning-Volume 70. JMLR. org, 2017.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "98\n",
      "\n",
      "regression problem of predicting bike counts is converted into a binary classification task. I have\n",
      "arbitrarily created a classification task by creating a label that is 1 if the number of bikes exceeds\n",
      "4000 bikes on a day, else 0.\n",
      "\n",
      "The following list was learned by SBRL: [{'source': './data/xai.pdf', 'start_index': 213290}]\n",
      "* In the current implementations in R⁸⁰ and Python⁸¹, for example, linear regression can be chosen as\n",
      "interpretable surrogate model. In advance, you have to select K, the number of features you want\n",
      "to have in your interpretable model. The lower K, the easier it is to interpret the model. A higher\n",
      "K potentially produces models with higher fidelity. There are several methods for training models\n",
      "with exactly K features. A good choice is Lasso. A Lasso model with a high regularization parameter\n",
      "(cid:21) yields a model without any feature. By retraining the Lasso models with slowly decreasing (cid:21),\n",
      "one after the other, the features get weight estimates that differ from zero. If there are K features\n",
      "in the model, you have reached the desired number of features. Other strategies are forward or\n",
      "backward selection of features. This means you either start with the full model (= containing all [{'source': './data/xai.pdf', 'start_index': 335588}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "127\n",
      "\n",
      "Strongly correlated features x1 and x2. M-Plots average over the conditional distribution. Here the conditional\n",
      "distribution of x2 at x1 = 0.75. Averaging the local predictions leads to mixing the effects of both features. [{'source': './data/xai.pdf', 'start_index': 264998}]\n",
      "* effect(i)\n",
      "\n",
      "j = wjx(i)\n",
      "\n",
      "j\n",
      "\n",
      "The effects can be visualized with boxplots. A box in a boxplot contains the effect range for half of\n",
      "your data (25% to 75% effect quantiles). The vertical line in the box is the median effect, i.e. 50% of\n",
      "the instances have a lower and the other half a higher effect on the prediction. The horizontal lines\n",
      "extend to (cid:6)1:58IQR/\n",
      "n, with IQR being the inter quartile range (75% quantile minus 25% quantile).\n",
      "The dots are outliers. The categorical feature effects can be summarized in a single boxplot, compared\n",
      "to the weight plot, where each category has its own row.\n",
      "\n",
      "p\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "45\n",
      "\n",
      "The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per\n",
      "feature. [{'source': './data/xai.pdf', 'start_index': 108395}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "171\n",
      "\n",
      "also influence the model. If you look at LIME’s Python implementation (file lime/lime_tabular.py)⁸²\n",
      "you will see that it uses an exponential smoothing kernel (on the normalized data) and the kernel\n",
      "width is 0.75 times the square root of the number of columns of the training data. It looks like an\n",
      "innocent line of code, but it is like an elephant sitting in your living room next to the good porcelain\n",
      "you got from your grandparents. The big problem is that we do not have a good way to find the best\n",
      "kernel or width. And where does the 0.75 even come from? In certain scenarios, you can easily turn\n",
      "your explanation around by changing the kernel width, as shown in the following figure: [{'source': './data/xai.pdf', 'start_index': 338660}]\n",
      "* Interpretable Models\n",
      "\n",
      "Visual Interpretation\n",
      "\n",
      "43\n",
      "\n",
      "Various visualizations make the linear regression model easy and quick to grasp for humans.\n",
      "\n",
      "Weight Plot\n",
      "\n",
      "The information of the weight table (weight and variance estimates) can be visualized in a weight\n",
      "plot. The following plot shows the results from the previous linear regression model.\n",
      "\n",
      "Weights are displayed as points and the 95% confidence intervals as lines. [{'source': './data/xai.pdf', 'start_index': 106270}]\n",
      "* How it works:\n",
      "\n",
      "1. Start with a few images that come from the same domain as the training data, e.g. if the classifier\n",
      "to be attacked is a digit classifier, use images of digits. The knowledge of the domain is required,\n",
      "but not the access to the training data.\n",
      "\n",
      "2. Get predictions for the current set of images from the black box.\n",
      "3. Train a surrogate model on the current set of images (for example a neural network).\n",
      "4. Create a new set of synthetic images using a heuristic that examines for the current set of\n",
      "images in which direction to manipulate the pixels to make the model output have more\n",
      "variance.\n",
      "\n",
      "¹⁰⁵Papernot, Nicolas, et al. “Practical black-box attacks against machine learning.” Proceedings of the 2017 ACM on Asia Conference on\n",
      "\n",
      "Computer and Communications Security. ACM (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "206 [{'source': './data/xai.pdf', 'start_index': 411643}]\n",
      "* Interpretable Models\n",
      "\n",
      "66\n",
      "\n",
      "Predicted number of coffees dependent on stress, sleep and work. The linear model predicts negative values.\n",
      "\n",
      "The linear model does not make sense, because it predicts negative number of coffees. This problem\n",
      "can be solved with Generalized Linear Models (GLMs). We can change the link function and the\n",
      "assumed distribution. One possibility is to keep the Gaussian distribution and use a link function\n",
      "that always leads to positive predictions such as the log-link (the inverse is the exp-function) instead\n",
      "of the identity function. Even better: We choose a distribution that corresponds to the data generating\n",
      "process and an appropriate link function. Since the outcome is a count, the Poisson distribution is a\n",
      "natural choice, along with the logarithm as link function. In this case, the data was even generated\n",
      "with the Poisson distribution, so the Poisson GLM is the perfect choice. The fitted Poisson GLM\n",
      "leads to the following distribution of predicted values: [{'source': './data/xai.pdf', 'start_index': 146903}]\n",
      "* ⁴⁶Letham, Benjamin, et al. “Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.” The\n",
      "\n",
      "Annals of Applied Statistics 9.3 (2015): 1350-1371.\n",
      "\n",
      "⁴⁷Borgelt, C. “An implementation of the FP-growth algorithm.” Proceedings of the 1st International Workshop on Open Source Data Mining\n",
      "\n",
      "Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907 (2005).\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "94\n",
      "\n",
      "frequently occurring patterns from them. A pattern can be a single feature value such as size=medium\n",
      "or a combination of feature values such as size=medium AND location=bad.\n",
      "\n",
      "The frequency of a pattern is measured with its support in the dataset:\n",
      "\n",
      "Support(xj = A) =\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "I(x(i)\n",
      "\n",
      "j = A) [{'source': './data/xai.pdf', 'start_index': 201521}]\n",
      "* For example: We have a task and dataset for predicting the values of houses from size, location\n",
      "and whether pets are allowed. We learn the first rule, which turns out to be: If size=big and\n",
      "location=good, then value=high. Then we remove all big houses in good locations from the dataset.\n",
      "With the remaining data we learn the next rule. Maybe: If location=good, then value=medium. Note\n",
      "that this rule is learned on data without big houses in good locations, leaving only medium and\n",
      "small houses in good locations. [{'source': './data/xai.pdf', 'start_index': 196163}]\n",
      "* How does that help? Remember when Google’s image classifier identified black people as gorillas?\n",
      "Perhaps they should have used the procedure described here before deploying their image recog-\n",
      "nition model. It is not enough just to check the performance of the model, because if it were 99%\n",
      "correct, this issue could still be in the 1%. And labels can also be wrong! Going through all the\n",
      "training data and performing a sanity check if the prediction is problematic might have revealed\n",
      "the problem, but would be infeasible. But the selection of – say a few thousand – prototypes and\n",
      "criticisms is feasible and could have revealed a problem with the data: It might have shown that\n",
      "there is a lack of images of people with dark skin, which indicates a problem with the diversity in\n",
      "the dataset. Or it could have shown one or more images of a person with dark skin as a prototype or\n",
      "(probably) as a criticism with the notorious “gorilla” classification. I do not promise that MMD-critic [{'source': './data/xai.pdf', 'start_index': 430614}]\n",
      "* Influence functions are only approximate, because the approach forms a quadratic expansion\n",
      "around the parameters. The approximation can be wrong and the influence of an instance is\n",
      "actually higher or lower when removed. Koh and Liang (2017) showed for some examples that the\n",
      "influence calculated by the influence function was close to the influence measure obtained when\n",
      "the model was actually retrained after the instance was deleted. But there is no guarantee that the\n",
      "approximation will always be so close. [{'source': './data/xai.pdf', 'start_index': 470880}]\n",
      "* We can use these methods to compare different machine learning models and better understand\n",
      "their different behaviors, going beyond comparing only the predictive performance.\n",
      "\n",
      "We have not talked about this topic in this chapter, but influence functions via derivatives can\n",
      "also be used to create adversarial training data. These are instances that are manipulated in such a\n",
      "way that the model cannot predict certain test instances correctly when the model is trained on those\n",
      "manipulated instances. The difference to the methods in the Adversarial Examples chapter is that\n",
      "the attack takes place during training time, also known as poisoning attacks. If you are interested,\n",
      "read the paper by Koh and Liang (2017). [{'source': './data/xai.pdf', 'start_index': 468184}]\n",
      "* Deletion diagnostics and influence functions can also be applied to the parameters or predictions of\n",
      "machine learning models to understand their behavior better or to explain individual predictions.\n",
      "Before we look at these two approaches for finding influential instances, we will examine the\n",
      "difference between an outlier and an influential instance.\n",
      "\n",
      "Outlier\n",
      "\n",
      "An outlier is an instance that is far away from the other instances in the dataset. “Far away” means\n",
      "that the distance, for example the Euclidean distance, to all the other instances is very large. In\n",
      "a dataset of newborns, a newborn weighting 6 kg would be considered an outlier. In a dataset of\n",
      "bank accounts with mostly checking accounts, a dedicated loan account (large negative balance,\n",
      "few transactions) would be considered an outlier. The following figure shows an outlier for a 1-\n",
      "dimensional distribution.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "219\n",
      "\n",
      "Feature x follows a Gaussian distribution with an outlier at x=8. [{'source': './data/xai.pdf', 'start_index': 438766}]\n",
      "* ⁸⁹Shapley, Lloyd S. “A value for n-person games.” Contributions to the Theory of Games 2.28 (1953): 307-317.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "178\n",
      "\n",
      "Players? Game? Payout? What is the connection to machine learning predictions and interpretabil-\n",
      "ity? The “game” is the prediction task for a single instance of the dataset. The “gain” is the actual\n",
      "prediction for this instance minus the average prediction for all instances. The “players” are the\n",
      "feature values of the instance that collaborate to receive the gain (= predict a certain value). In our\n",
      "apartment example, the feature values park-nearby, cat-banned, area-50 and floor-2nd worked\n",
      "together to achieve the prediction of €300,000. Our goal is to explain the difference between the\n",
      "actual prediction (€300,000) and the average prediction (€310,000): a difference of -€10,000. [{'source': './data/xai.pdf', 'start_index': 351313}]\n",
      "* The usefulness of a decision rule is usually summarized in two numbers: Support and accuracy.\n",
      "\n",
      "Support or coverage of a rule: The percentage of instances to which the condition of a rule applies\n",
      "is called the support. Take for example the rule size=big AND location=good THEN value=high for\n",
      "predicting house values. Suppose 100 of 1000 houses are big and in a good location, then the support\n",
      "of the rule is 10%. The prediction (THEN-part) is not important for the calculation of support.\n",
      "\n",
      "Accuracy or confidence of a rule: The accuracy of a rule is a measure of how accurate the rule is in\n",
      "predicting the correct class for the instances to which the condition of the rule applies. For example:\n",
      "Let us say of the 100 houses, where the rule size=big AND location=good THEN value=high applies,\n",
      "85 have value=high, 14 have value=medium and 1 has value=low, then the accuracy of the rule is\n",
      "85%.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "86 [{'source': './data/xai.pdf', 'start_index': 183817}]\n",
      "* Interpretability\n",
      "\n",
      "19\n",
      "\n",
      "Doge, our vacuum cleaner, got stuck. As an explanation for the accident, Doge told us that it needs to be on an even\n",
      "surface. [{'source': './data/xai.pdf', 'start_index': 43126}]\n",
      "* Interpretable Models\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "54\n",
      "\n",
      "Logistic regression models the probabilities for classification problems with two possible outcomes.\n",
      "It’s an extension of the linear regression model for classification problems.\n",
      "\n",
      "What is Wrong with Linear Regression for Classification?\n",
      "\n",
      "The linear regression model can work well for regression, but fails for classification. Why is that?\n",
      "In case of two classes, you could label one of the classes with 0 and the other with 1 and use linear\n",
      "regression. Technically it works and most linear model programs will spit out weights for you. But\n",
      "there are a few problems with this approach:\n",
      "\n",
      "A linear model does not output probabilities, but it treats the classes as numbers (0 and 1) and fits\n",
      "the best hyperplane (for a single feature, it is a line) that minimizes the distances between the points\n",
      "and the hyperplane. So it simply interpolates between the points, and you cannot interpret it as\n",
      "probabilities. [{'source': './data/xai.pdf', 'start_index': 124668}]\n",
      "* Enough bicycles for now, let’s turn to a classification task. We train a random forest to predict the\n",
      "probability of cervical cancer based on risk factors. We visualize the accumulated local effects for\n",
      "two of the features:\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "142\n",
      "\n",
      "ALE plots for the effect of age and years with hormonal contraceptives on the predicted probability of cervical cancer.\n",
      "For the age feature, the ALE plot shows that the predicted cancer probability is low on average up to age 40 and\n",
      "increases after that. The number of years with hormonal contraceptives is associated with a higher predicted cancer\n",
      "risk after 8 years.\n",
      "\n",
      "Next, we look at the interaction between number of pregnancies and age.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "143 [{'source': './data/xai.pdf', 'start_index': 287981}]\n",
      "* ⁶⁵Apley, Daniel W. “Visualizing the effects of predictor variables in black box supervised learning models.” arXiv preprint arXiv:1612.08468\n",
      "\n",
      "(2016).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "126\n",
      "\n",
      "Strongly correlated features x1 and x2. To calculate the feature effect of x1 at 0.75, the PDP replaces x1 of all instances\n",
      "with 0.75, falsely assuming that the distribution of x2 at x1 = 0.75 is the same as the marginal distribution of x2 (vertical\n",
      "line). This results in unlikely combinations of x1 and x2 (e.g. x2=0.2 at x1=0.75), which the PDP uses for the calculation\n",
      "of the average effect. [{'source': './data/xai.pdf', 'start_index': 263323}]\n",
      "* ⁶⁶https://cran.r-project.org/web/packages/ALEPlot/index.html\n",
      "⁶⁷https://cran.r-project.org/web/packages/iml/index.html\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "146\n",
      "\n",
      "Feature Interaction\n",
      "\n",
      "When features interact with each other in a prediction model, the prediction cannot be expressed\n",
      "as the sum of the feature effects, because the effect of one feature depends on the value of the other\n",
      "feature. Aristotle’s predicate “The whole is greater than the sum of its parts” applies in the presence\n",
      "of interactions.\n",
      "\n",
      "Feature Interaction?\n",
      "\n",
      "If a machine learning model makes a prediction based on two features, we can decompose the\n",
      "prediction into four terms: a constant term, a term for the first feature, a term for the second feature\n",
      "and a term for the interaction between the two features.\n",
      "The interaction between two features is the change in the prediction that occurs by varying the\n",
      "features after considering the individual feature effects. [{'source': './data/xai.pdf', 'start_index': 293758}]\n",
      "* Interpretable Models\n",
      "\n",
      "67\n",
      "\n",
      "Predicted number of coffees dependent on stress, sleep and work. The GLM with Poisson assumption and log link is\n",
      "an appropriate model for this dataset.\n",
      "\n",
      "No negative amounts of coffees, looks much better now.\n",
      "\n",
      "Interpretation of GLM weights\n",
      "\n",
      "The assumed distribution together with the link function determines how the estimated feature\n",
      "weights are interpreted. In the coffee count example, I used a GLM with Poisson distribution and\n",
      "log link, which implies the following relationship between the features and the expected outcome.\n",
      "\n",
      "ln(E(coffeesjstress; sleep; work)) = (cid:12)0 + (cid:12)stressxstress + (cid:12)sleepxsleep + (cid:12)workxwork\n",
      "\n",
      "To interpret the weights we invert the link function so that we can interpret the effect of the features\n",
      "on the expected outcome and not on the logarithm of the expected outcome.\n",
      "\n",
      "E(coffeesjstress; sleep; work) = exp((cid:12)0 + (cid:12)stressxstress + (cid:12)sleepxsleep + (cid:12)workxwork) [{'source': './data/xai.pdf', 'start_index': 147896}]\n",
      "* Partial dependence based feature interaction by Greenwell et. al (2018)⁷⁴ measures the interaction\n",
      "between two features. This approach measures the feature importance (defined as the variance of\n",
      "the partial dependence function) of one feature conditional on different, fixed points of the other\n",
      "feature. If the variance is high, then the features interact with each other, if it is zero, they do not\n",
      "interact. The corresponding R package vip is available on Github⁷⁵. The package also covers partial\n",
      "dependence plots and feature importance.\n",
      "\n",
      "⁷³Hooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM SIGKDD international conference\n",
      "\n",
      "on Knowledge discovery and data mining. (2004).\n",
      "\n",
      "⁷⁴Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. “A simple and effective model-based variable importance\n",
      "\n",
      "measure.” arXiv preprint arXiv:1805.04755 (2018).\n",
      "\n",
      "⁷⁵https://github.com/koalaverse/vip\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "154\n",
      "\n",
      "Feature Importance [{'source': './data/xai.pdf', 'start_index': 305429}]\n",
      "* It is mind-blowing to explain a prediction as a game played by the feature values.\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "The Shapley value requires a lot of computing time. In 99.9% of real-world problems, only the\n",
      "approximate solution is feasible. An exact computation of the Shapley value is computationally\n",
      "expensive because there are 2 possible coalitions of the feature values and the “absence” of a feature\n",
      "has to be simulated by drawing random instances, which increases the variance for the estimate of\n",
      "the Shapley values estimation. The exponential number of the coalitions is dealt with by sampling\n",
      "coalitions and limiting the number of iterations M. Decreasing M reduces computation time, but\n",
      "increases the variance of the Shapley value. It is unclear how to choose a sensitive M. It should be\n",
      "possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for\n",
      "Shapley values for machine learning predictions. [{'source': './data/xai.pdf', 'start_index': 366203}]\n",
      "* ⁶²Friedman, Jerome H. “Greedy function approximation: A gradient boosting machine.” Annals of statistics (2001): 1189-1232.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "114\n",
      "\n",
      "value for “summer”, we replace the season of all data instances with “summer” and average the\n",
      "predictions.\n",
      "\n",
      "Examples\n",
      "\n",
      "In practice, the set of features S usually only contains one feature or a maximum of two, because\n",
      "one feature produces 2D plots and two features produce 3D plots. Everything beyond that is quite\n",
      "tricky. Even 3D on a 2D paper or monitor is already challenging. [{'source': './data/xai.pdf', 'start_index': 246571}]\n",
      "* The predicted price for a 50 m² 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain\n",
      "how each of these feature values contributed to the prediction.\n",
      "\n",
      "The average prediction for all apartments is €310,000. How much has each feature value contributed\n",
      "to the prediction compared to the average prediction?\n",
      "\n",
      "The answer is simple for linear regression models. The effect of each feature is the weight of the\n",
      "feature times the feature value. This only works because of the linearity of the model. For more\n",
      "complex models, we need a different solution. For example, LIME suggests local models to estimate\n",
      "effects. Another solution comes from cooperative game theory: The Shapley value, coined by Shapley\n",
      "(1953)⁸⁹, is a method for assigning payouts to players depending on their contribution to the total\n",
      "payout. Players cooperate in a coalition and receive a certain profit from this cooperation. [{'source': './data/xai.pdf', 'start_index': 350388}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "179\n",
      "\n",
      "One sample repetition to estimate the contribution of cat-banned to the prediction when added to the coalition of\n",
      "park-nearby and area-50.\n",
      "\n",
      "We repeat this computation for all possible coalitions. The Shapley value is the average of all the\n",
      "marginal contributions to all possible coalitions. The computation time increases exponentially with\n",
      "the number of features. One solution to keep the computation time manageable is to compute\n",
      "contributions for only a few samples of the possible coalitions.\n",
      "\n",
      "The following figure shows all coalitions of feature values that are needed to determine the Shapley\n",
      "value for cat-banned. The first row shows the coalition without any feature values. The second,\n",
      "third and fourth rows show different coalitions with increasing coalition size, separated by “|”. All\n",
      "in all, the following coalitions are possible:\n",
      "\n",
      "• No feature values\n",
      "• park-nearby\n",
      "• size-50\n",
      "• floor-2nd\n",
      "• park-nearby+size-50\n",
      "• park-nearby+floor-2nd\n",
      "• size-50+floor-2nd [{'source': './data/xai.pdf', 'start_index': 353649}]\n",
      "* temperature at 8:00 AM. But having more features is always good, right? I train a random forest with\n",
      "the two temperature features and the uncorrelated features. Some of the trees in the random forest\n",
      "pick up the 8:00 AM temperature, others the 9:00 AM temperature, again others both and again\n",
      "others none. The two temperature features together have a bit more importance than the single\n",
      "temperature feature before, but instead of being at the top of the list of important features, each\n",
      "temperature is now somewhere in the middle. By introducing a correlated feature, I kicked the most\n",
      "important feature from the top of the importance ladder to mediocrity. On one hand this is fine,\n",
      "because it simply reflects the behavior of the underlying machine learning model, here the random\n",
      "forest. The 8:00 AM temperature has simply become less important because the model can now rely\n",
      "on the 9:00 AM measurement as well. On the other hand, it makes the interpretation of the feature [{'source': './data/xai.pdf', 'start_index': 321413}]\n",
      "* Contents\n",
      "\n",
      "Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "1\n",
      "\n",
      "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Story Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "What Is Machine Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "2\n",
      "4\n",
      "10\n",
      "12 [{'source': './data/xai.pdf', 'start_index': 627}]\n",
      "* rule that applies.\n",
      "\n",
      "The next step is to generate many new lists starting from this initial sample to obtain many samples\n",
      "from the posterior distribution of decision lists.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "97\n",
      "\n",
      "The new decision lists are sampled by starting from the initial list and then randomly either moving\n",
      "a rule to a different position in the list or adding a rule to the current decision list from the pre-mined\n",
      "conditions or removing a rule from the decision list. Which of the rules is switched, added or deleted\n",
      "is chosen at random. At each step, the algorithm evaluates the posteriori probability of the decision\n",
      "list (mixture of accuracy and shortness). The Metropolis Hastings algorithm ensures that we sample\n",
      "decision lists that have a high posterior probability. This procedure provides us with many samples\n",
      "from the distribution of decision lists. The BRL algorithm selects the decision list of the samples with\n",
      "the highest posterior probability.\n",
      "\n",
      "Examples [{'source': './data/xai.pdf', 'start_index': 210778}]\n",
      "* Interpretable Models\n",
      "\n",
      "60\n",
      "\n",
      "the weight for that feature would not converge, because the optimal weight would be infinite. This\n",
      "is really a bit unfortunate, because such a feature is really useful. But you do not need machine\n",
      "learning if you have a simple rule that separates both classes. The problem of complete separation\n",
      "can be solved by introducing penalization of the weights or defining a prior probability distribution\n",
      "of weights.\n",
      "\n",
      "On the good side, the logistic regression model is not only a classification model, but also gives\n",
      "you probabilities. This is a big advantage over models that can only provide the final classification.\n",
      "Knowing that an instance has a 99% probability for a class compared to 51% makes a big difference.\n",
      "\n",
      "Logistic regression can also be extended from binary classification to multi-class classification. Then\n",
      "it is called Multinomial Regression.\n",
      "\n",
      "Software [{'source': './data/xai.pdf', 'start_index': 134609}]\n",
      "* Encoding of Categorical Features\n",
      "\n",
      "There are several ways to encode a categorical feature, and the choice influences the interpretation\n",
      "of the weights.\n",
      "\n",
      "The standard in linear regression models is treatment coding, which is sufficient in most cases. Using\n",
      "different encodings boils down to creating different (design) matrices from a single column with the\n",
      "categorical feature. This section presents three different encodings, but there are many more. The\n",
      "example used has six instances and a categorical feature with three categories. For the first two\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "48\n",
      "\n",
      "instances, the feature takes category A; for instances three and four, category B; and for the last two\n",
      "instances, category C.\n",
      "\n",
      "Treatment coding [{'source': './data/xai.pdf', 'start_index': 112066}]\n",
      "* Next, let us see ALE plots in action for a categorical feature. The month is a categorical feature for\n",
      "which we want to analyze the effect on the predicted number of bikes. Arguably, the months already\n",
      "have a certain order (January to December), but let us try to see what happens if we first reorder\n",
      "the categories by similarity and then compute the effects. The months are ordered by the similarity\n",
      "of days of each month based on the other features, such as temperature or whether it is a holiday.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "139\n",
      "\n",
      "ALE plot for the categorical feature month. The months are ordered by their similarity to each other, based on the\n",
      "distributions of the other features by month. We observe that January, March and April, but especially December and\n",
      "November, have a lower effect on the predicted number of rented bikes compared to the other months. [{'source': './data/xai.pdf', 'start_index': 283937}]\n",
      "* ⁴⁹https://cran.r-project.org/web/packages/OneR/\n",
      "⁵⁰(https://www.eecs.yorku.ca/tdb/_doc.php/userg/sw/weka/doc/weka/classifiers/rules/package-summary.html)\n",
      "⁵¹https://cran.r-project.org/web/packages/RWeka/index.html\n",
      "⁵²https://cran.r-project.org/web/packages/sbrl/index.html\n",
      "⁵³https://github.com/datascienceinc/Skater\n",
      "⁵⁴https://github.com/Hongyuy/sbrlmod\n",
      "⁵⁵Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. “Foundations of rule learning.” Springer Science & Business Media, (2012).\n",
      "⁵⁶http://weka.sourceforge.net/doc.dev/weka/classifiers/rules/package-summary.html\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "RuleFit\n",
      "\n",
      "100\n",
      "\n",
      "The RuleFit algorithm by Friedman and Popescu (2008)⁵⁷ learns sparse linear models that include\n",
      "automatically detected interaction effects in the form of decision rules. [{'source': './data/xai.pdf', 'start_index': 219085}]\n",
      "* ³⁹https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "64\n",
      "\n",
      "always positive (e.g. time between two events)? Then the exponential distribution could be a good\n",
      "choice.\n",
      "\n",
      "Let us consider the classic linear model as a special case of a GLM. The link function for the Gaussian\n",
      "distribution in the classic linear model is simply the identity function. The Gaussian distribution is\n",
      "parameterized by the mean and the variance parameters. The mean describes the value that we\n",
      "expect on average and the variance describes how much the values vary around this mean. In the\n",
      "linear model, the link function links the weighted sum of the features to the mean of the Gaussian\n",
      "distribution. [{'source': './data/xai.pdf', 'start_index': 142637}]\n",
      "* (2008).\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "101\n",
      "\n",
      "> 6.64 AND concentration of nitric oxide <0.67 THEN 1 ELSE 0.\n",
      "\n",
      "RuleFit also comes with a feature importance measure that helps to identify linear terms and rules\n",
      "that are important for the predictions. Feature importance is calculated from the weights of the\n",
      "regression model. The importance measure can be aggregated for the original features (which are\n",
      "used in their “raw” form and possibly in many decision rules).\n",
      "\n",
      "RuleFit also introduces partial dependence plots to show the average change in prediction by\n",
      "changing a feature. The partial dependence plot is a model-agnostic method that can be used with\n",
      "any model, and is explained in the book chapter on partial dependence plots.\n",
      "\n",
      "Interpretation and Example [{'source': './data/xai.pdf', 'start_index': 221356}]\n",
      "* For example, a model predicts the value of a house, using house size (big or small) and location (good\n",
      "or bad) as features, which yields four possible predictions:\n",
      "\n",
      "Location\n",
      "good\n",
      "good\n",
      "bad\n",
      "bad\n",
      "\n",
      "Size\n",
      "big\n",
      "small\n",
      "big\n",
      "small\n",
      "\n",
      "Prediction\n",
      "300,000\n",
      "200,000\n",
      "250,000\n",
      "150,000\n",
      "\n",
      "We decompose the model prediction into the following parts: A constant term (150,000), an effect for\n",
      "the size feature (+100,000 if big; +0 if small) and an effect for the location (+50,000 if good; +0 if bad).\n",
      "This decomposition fully explains the model predictions. There is no interaction effect, because the\n",
      "model prediction is a sum of the single feature effects for size and location. When you make a small\n",
      "house big, the prediction always increases by 100,000, regardless of location. Also, the difference in\n",
      "prediction between a good and a bad location is 50,000, regardless of size.\n",
      "\n",
      "Let’s now look at an example with interaction:\n",
      "\n",
      "Location\n",
      "good\n",
      "good\n",
      "bad\n",
      "bad\n",
      "\n",
      "Size\n",
      "big\n",
      "small\n",
      "big\n",
      "small [{'source': './data/xai.pdf', 'start_index': 294681}]\n",
      "* In the example where we are predicting the number of bicycles with a GAM using only the\n",
      "temperature, the model feature matrix looks like this:\n",
      "\n",
      "(Intercept)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "s(temp).1\n",
      "0.93\n",
      "0.83\n",
      "1.32\n",
      "1.32\n",
      "1.29\n",
      "1.32\n",
      "\n",
      "s(temp).2\n",
      "-0.14\n",
      "-0.27\n",
      "0.71\n",
      "0.70\n",
      "0.58\n",
      "0.68\n",
      "\n",
      "s(temp).3\n",
      "0.21\n",
      "0.27\n",
      "-0.39\n",
      "-0.38\n",
      "-0.26\n",
      "-0.36\n",
      "\n",
      "s(temp).4\n",
      "-0.83\n",
      "-0.72\n",
      "-1.63\n",
      "-1.61\n",
      "-1.47\n",
      "-1.59\n",
      "\n",
      "Each row represents an individual instance from the data (one day). Each spline column contains\n",
      "the value of the spline function at the particular temperature values. The following figure shows\n",
      "how these spline functions look like:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "75\n",
      "\n",
      "To smoothly model the temperature effect, we use 4 spline functions. Each temperature value is mapped to (here) 4\n",
      "spline values. If an instance has a temperature of 30 ￿C, the value for the first spline feature is -1, for the second 0.7,\n",
      "for the third -0.8 and for the 4th 1.7.\n",
      "\n",
      "The GAM assigns weights to each temperature spline feature:\n",
      "\n",
      "(Intercept)\n",
      "s(temp).1\n",
      "s(temp).2\n",
      "s(temp).3\n",
      "s(temp).4 [{'source': './data/xai.pdf', 'start_index': 163914}]\n",
      "* Pay attention to the social environment of your machine learning application and the target\n",
      "audience. Getting the social part of the machine learning model right depends entirely on your\n",
      "specific application. Find experts from the humanities (e.g. psychologists and sociologists) to help\n",
      "you. [{'source': './data/xai.pdf', 'start_index': 76307}]\n",
      "* ∑\n",
      "\n",
      "Jj(x) = Il(x) +\n",
      "\n",
      "Ik(x)/mk\n",
      "\n",
      "xj 2rk\n",
      "\n",
      "where Il is the importance of the linear term and Ik the importance of the decision rules in which xj\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "106\n",
      "\n",
      "appears, and mk is the number of features constituting the rule rk. Adding the feature importance\n",
      "from all instances gives us the global feature importance:\n",
      "\n",
      "Jj(X) =\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "Jj(x(i))\n",
      "\n",
      "It is possible to select a subset of instances and calculate the feature importance for this group.\n",
      "\n",
      "Advantages\n",
      "\n",
      "RuleFit automatically adds feature interactions to linear models. Therefore, it solves the problem\n",
      "of linear models that you have to add interaction terms manually and it helps a bit with the issue of\n",
      "modeling nonlinear relationships.\n",
      "\n",
      "RuleFit can handle both classification and regression tasks. [{'source': './data/xai.pdf', 'start_index': 231351}]\n",
      "* Having a formula is great and the scientific and accurate way of showing things. But I think it\n",
      "is very important to get some intuition about what the formula means. The formula for Iup,loss\n",
      "states that the influence function of the training instance z on the prediction of an instance ztest\n",
      "is “how strongly the instance reacts to a change of the model parameters” multiplied by “how much\n",
      "the parameters change when we upweight the instance z”. Another way to read the formula: The\n",
      "influence is proportional to how large the gradients for the training and test loss are. The higher the\n",
      "gradient of the training loss, the higher its influence on the parameters and the higher the influence\n",
      "on the test prediction. The higher the gradient of the test prediction, the more influenceable the\n",
      "test instance. The entire construct can also be seen as a measure of the similarity (as learned by the\n",
      "model) between the training and the test instance. [{'source': './data/xai.pdf', 'start_index': 461867}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "185\n",
      "\n",
      "then\n",
      "\n",
      "S (cid:18) fx1; : : : ; xpg\n",
      "\n",
      "ϕj = 0\n",
      "\n",
      "Additivity\n",
      "For a game with combined payouts val+val⁺ the respective Shapley values are as follows:\n",
      "\n",
      "ϕj + ϕ+\n",
      "j\n",
      "\n",
      "Suppose you trained a random forest, which means that the prediction is an average of many decision\n",
      "trees. The Additivity property guarantees that for a feature value, you can calculate the Shapley\n",
      "value for each tree individually, average them, and get the Shapley value for the feature value for\n",
      "the random forest.\n",
      "\n",
      "Intuition\n",
      "\n",
      "An intuitive way to understand the Shapley value is the following illustration: The feature values\n",
      "enter a room in random order. All feature values in the room participate in the game (= contribute\n",
      "to the prediction). The Shapley value of a feature value is the average change in the prediction that\n",
      "the coalition already in the room receives when the feature value joins them.\n",
      "\n",
      "Estimating the Shapley Value [{'source': './data/xai.pdf', 'start_index': 361127}]\n",
      "* Alternatives to PDPs presented in this book are ALE plots and ICE curves.\n",
      "\n",
      "⁶³Zhao, Qingyuan, and Trevor Hastie. “Causal interpretations of black-box models.” Journal of Business & Economic Statistics, to appear.\n",
      "\n",
      "(2017).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "119\n",
      "\n",
      "Individual Conditional Expectation (ICE)\n",
      "\n",
      "Individual Conditional Expectation (ICE) plots display one line per instance that shows how the\n",
      "instance’s prediction changes when a feature changes. [{'source': './data/xai.pdf', 'start_index': 253730}]\n",
      "* Note that we have not talked about the model performance of the underlying black box model, i.e.\n",
      "how good or bad it performs in predicting the actual outcome. The performance of the black box\n",
      "model does not play a role in training the surrogate model. The interpretation of the surrogate model\n",
      "is still valid because it makes statements about the model and not about the real world. But of course\n",
      "the interpretation of the surrogate model becomes irrelevant if the black box model is bad, because\n",
      "then the black box model itself is irrelevant. [{'source': './data/xai.pdf', 'start_index': 327105}]\n",
      "* Example and Interpretation\n",
      "\n",
      "I show examples for classification and regression.\n",
      "\n",
      "Cervical cancer (classification)\n",
      "\n",
      "We fit a random forest model to predict cervical cancer. We measure the error increase by 1-AUC (1\n",
      "minus the area under the ROC curve). Features associated with a model error increase by a factor\n",
      "of 1 (= no change) were not important for predicting cervical cancer.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "159\n",
      "\n",
      "The importance of each of the features for predicting cervical cancer with a random forest. The most important feature\n",
      "was Age. Permuting Age resulted in an increase in 1-AUC by a factor of 5.76\n",
      "\n",
      "The feature with the highest importance was Age associated with an error increase of 5.76 after\n",
      "permutation.\n",
      "\n",
      "Bike sharing (regression)\n",
      "\n",
      "We fit a support vector machine model to predict the number of rented bikes, given weather\n",
      "conditions and calendar information. As error measurement we use the mean absolute error.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "160 [{'source': './data/xai.pdf', 'start_index': 314615}]\n",
      "* Model-Agnostic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n",
      "Partial Dependence Plot (PDP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n",
      "Individual Conditional Expectation (ICE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "Accumulated Local Effects (ALE) Plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n",
      "Feature Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n",
      "Feature Importance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n",
      "\n",
      "\fCONTENTS\n",
      "\n",
      "Global Surrogate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n",
      "Local Surrogate (LIME) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n",
      "Shapley Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 [{'source': './data/xai.pdf', 'start_index': 3280}]\n",
      "* This decomposition expresses the partial dependence (or full prediction) function without interac-\n",
      "tions (between features j and k, or respectively j and all other features). In a next step, we measure\n",
      "the difference between the observed partial dependence function and the decomposed one without\n",
      "interactions. We calculate the variance of the output of the partial dependence (to measure the\n",
      "interaction between two features) or of the entire function (to measure the interaction between a\n",
      "feature and all other features). The amount of the variance explained by the interaction (difference\n",
      "between observed and no-interaction PD) is used as interaction strength statistic. The statistic is 0\n",
      "if there is no interaction at all and 1 if all of the variance of the P Djk or ^f is explained by the sum of\n",
      "the partial dependence functions. An interaction statistic of 1 between two features means that each [{'source': './data/xai.pdf', 'start_index': 297670}]\n",
      "* Weights are displayed as points and the 95% confidence intervals as lines.\n",
      "\n",
      "The weight plot shows that rainy/snowy/stormy weather has a strong negative effect on the\n",
      "predicted number of bikes. The weight of the working day feature is close to zero and zero is included\n",
      "in the 95% interval, which means that the effect is not statistically significant. Some confidence\n",
      "intervals are very short and the estimates are close to zero, yet the feature effects were statistically\n",
      "significant. Temperature is one such candidate. The problem with the weight plot is that the features\n",
      "are measured on different scales. While for the weather the estimated weight reflects the difference\n",
      "between good and rainy/stormy/snowy weather, for temperature it only reflects an increase of 1\n",
      "degree Celsius. You can make the estimated weights more comparable by scaling the features (zero\n",
      "mean and standard deviation of one) before fitting the linear model.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "Effect Plot\n",
      "\n",
      "44 [{'source': './data/xai.pdf', 'start_index': 106610}]\n",
      "* iml. Christoph Molnar (2019). iml: Interpretable Machine Learning. R package version 0.9.0.\n",
      "https://github.com/christophM/iml\n",
      "\n",
      "knitr. Yihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R\n",
      "package version 1.20. https://CRAN.R-project.org/package=knitr\n",
      "\n",
      "libcoin. Torsten Hothorn (2019). libcoin: Linear Test Statistics for Permutation Inference. R package\n",
      "version 1.0-3. https://CRAN.R-project.org/package=libcoin\n",
      "\n",
      "memoise. Hadley Wickham, Jim Hester, Kirill Müller and Daniel Cook (2017). memoise: Memoisa-\n",
      "tion of Functions. R package version 1.1.0. https://CRAN.R-project.org/package=memoise\n",
      "\n",
      "mlr. Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Zachary Jones,\n",
      "Giuseppe Casalicchio, Mason Gallo and Patrick Schratz (2018). mlr: Machine Learning in R. R\n",
      "package version 2.13. https://CRAN.R-project.org/package=mlr [{'source': './data/xai.pdf', 'start_index': 502064}]\n",
      "* The contribution ϕj of the j-th feature on the prediction ^f (x) is:\n",
      "\n",
      "ϕj( ^f ) = (cid:12)jxj (cid:0) E((cid:12)jXj) = (cid:12)jxj (cid:0) (cid:12)jE(Xj)\n",
      "\n",
      "where E((cid:12)jXj) is the mean effect estimate for feature j. The contribution is the difference between\n",
      "the feature effect minus the average effect. Nice! Now we know how much each feature contributed\n",
      "to the prediction. If we sum all the feature contributions for one instance, the result is the following:\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "ϕj( ^f ) =\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "((cid:12)jxj (cid:0) E((cid:12)jXj))\n",
      "\n",
      "p∑\n",
      "\n",
      "=((cid:12)0 +\n",
      "\n",
      "(cid:12)jxj) (cid:0) ((cid:12)0 +\n",
      "\n",
      "j=1\n",
      "= ^f (x) (cid:0) E( ^f (X))\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "E((cid:12)jXj))\n",
      "\n",
      "This is the predicted value for the data point x minus the average predicted value. Feature\n",
      "contributions can be negative.\n",
      "\n",
      "Can we do the same for any type of model? It would be great to have this as a model-agnostic tool.\n",
      "Since we usually do not have similar weights in other model types, we need a different solution. [{'source': './data/xai.pdf', 'start_index': 357788}]\n",
      "* The Shapley value is the only attribution method that satisfies the properties Efficiency, Symmetry,\n",
      "Dummy and Additivity, which together can be considered a definition of a fair payout.\n",
      "\n",
      "Efficiency\n",
      "The feature contributions must add up to the difference of prediction for x and the average.\n",
      "\n",
      "∑p\n",
      "\n",
      "j=1\n",
      "\n",
      "ϕj = ^f (x) (cid:0) EX ( ^f (X))\n",
      "\n",
      "Symmetry\n",
      "The contributions of two feature values j and k should be the same if they contribute equally to all\n",
      "possible coalitions. If\n",
      "\n",
      "for all\n",
      "\n",
      "then\n",
      "\n",
      "val(S [ fxjg) = val(S [ fxkg)\n",
      "\n",
      "S (cid:18) fx1; : : : ; xpg n fxj; xkg\n",
      "\n",
      "ϕj = ϕk\n",
      "\n",
      "Dummy\n",
      "A feature j that does not change the predicted value – regardless of which coalition of feature values\n",
      "it is added to – should have a Shapley value of 0. If\n",
      "\n",
      "for all\n",
      "\n",
      "val(S [ fxjg) = val(S)\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "185\n",
      "\n",
      "then\n",
      "\n",
      "S (cid:18) fx1; : : : ; xpg\n",
      "\n",
      "ϕj = 0\n",
      "\n",
      "Additivity\n",
      "For a game with combined payouts val+val⁺ the respective Shapley values are as follows:\n",
      "\n",
      "ϕj + ϕ+\n",
      "j [{'source': './data/xai.pdf', 'start_index': 360363}]\n",
      "* Another tricky thing: Adding a correlated feature can decrease the importance of the associated\n",
      "feature by splitting the importance between both features. Let me give you an example of what I\n",
      "mean by “splitting” feature importance: We want to predict the probability of rain and use the\n",
      "temperature at 8:00 AM of the day before as a feature along with other uncorrelated features. I train\n",
      "a random forest and it turns out that the temperature is the most important feature and all is well\n",
      "and I sleep well the next night. Now imagine another scenario in which I additionally include the\n",
      "temperature at 9:00 AM as a feature that is strongly correlated with the temperature at 8:00 AM.\n",
      "The temperature at 9:00 AM does not give me much additional information if I already know the\n",
      "temperature at 8:00 AM. But having more features is always good, right? I train a random forest with\n",
      "the two temperature features and the uncorrelated features. Some of the trees in the random forest [{'source': './data/xai.pdf', 'start_index': 320635}]\n",
      "* The Shapley Value in Detail\n",
      "\n",
      "This section goes deeper into the definition and computation of the Shapley value for the curious\n",
      "reader. Skip this section and go directly to “Advantages and Disadvantages” if you are not interested\n",
      "in the technical details.\n",
      "\n",
      "We are interested in how each feature affects the prediction of a data point. In a linear model it is\n",
      "easy to calculate the individual effects. Here is what a linear model prediction looks like for one data\n",
      "instance:\n",
      "\n",
      "^f (x) = (cid:12)0 + (cid:12)1x1 + : : : + (cid:12)pxp\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "183\n",
      "\n",
      "where x is the instance for which we want to compute the contributions. Each xj is a feature value,\n",
      "with j = 1,…,p. The (cid:12)j is the weight corresponding to feature j.\n",
      "\n",
      "The contribution ϕj of the j-th feature on the prediction ^f (x) is:\n",
      "\n",
      "ϕj( ^f ) = (cid:12)jxj (cid:0) E((cid:12)jXj) = (cid:12)jxj (cid:0) (cid:12)jE(Xj) [{'source': './data/xai.pdf', 'start_index': 357055}]\n",
      "* Influence((cid:0)i)\n",
      "\n",
      "j =\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)^yj (cid:0) ^y((cid:0)i)\n",
      "\n",
      "j\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "This would also work for the difference in model parameters or the difference in the loss. In the\n",
      "following example we will use these simple influence measures.\n",
      "\n",
      "Deletion diagnostics example [{'source': './data/xai.pdf', 'start_index': 446639}]\n",
      "* How do you get the variations of the data? This depends on the type of data, which can be either text,\n",
      "image or tabular data. For text and images, the solution is to turn single words or super-pixels on or\n",
      "off. In the case of tabular data, LIME creates new samples by perturbing each feature individually,\n",
      "drawing from a normal distribution with mean and standard deviation taken from the feature.\n",
      "\n",
      "LIME for Tabular Data\n",
      "\n",
      "Tabular data is data that comes in tables, with each row representing an instance and each column\n",
      "a feature. LIME samples are not taken around the instance of interest, but from the training data’s\n",
      "mass center, which is problematic. But it increases the probability that the result for some of the\n",
      "sample points predictions differ from the data point of interest and that LIME can learn at least\n",
      "some explanation.\n",
      "\n",
      "It is best to visually explain how sampling and local model training works:\n",
      "\n",
      "⁸⁰https://github.com/thomasp85/lime\n",
      "⁸¹https://github.com/marcotcr/lime [{'source': './data/xai.pdf', 'start_index': 336667}]\n",
      "* analyze more deeply. We could return the, say, top 10 most influential instances for predicting the 7-\n",
      "th instance printed as a table. Not very useful, because we could not see much. Again, it makes more\n",
      "sense to find out what distinguishes the influential instances from the non-influential instances by\n",
      "analyzing their features. We use a decision tree trained to predict the influence given the features, but\n",
      "in reality we misuse it only to find a structure and not to actually predict something. The following\n",
      "decision tree shows which kind of training instances where most influential for predicting the 7-th\n",
      "instance. [{'source': './data/xai.pdf', 'start_index': 450778}]\n",
      "* Examples\n",
      "\n",
      "I have simulated a dataset on coffee drinking behavior to highlight the need for GLMs. Suppose you\n",
      "have collected data about your daily coffee drinking behavior. If you do not like coffee, pretend it\n",
      "is about tea. Along with number of cups, you record your current stress level on a scale of 1 to 10,\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "65 [{'source': './data/xai.pdf', 'start_index': 145085}]\n",
      "* The interpretation of a weight can be unintuitive because it depends on all other features. A feature\n",
      "with high positive correlation with the outcome y and another feature might get a negative weight\n",
      "in the linear model, because, given the other correlated feature, it is negatively correlated with y in\n",
      "the high-dimensional space. Completely correlated features make it even impossible to find a unique\n",
      "solution for the linear equation. An example: You have a model to predict the value of a house and\n",
      "have features like number of rooms and size of the house. House size and number of rooms are\n",
      "highly correlated: the bigger a house is, the more rooms it has. If you take both features into a linear\n",
      "model, it might happen, that the size of the house is the better predictor and gets a large positive\n",
      "weight. The number of rooms might end up getting a negative weight, because, given that a house\n",
      "has the same size, increasing the number of rooms could make it less valuable or the linear equation [{'source': './data/xai.pdf', 'start_index': 123610}]\n",
      "* Next, we look at the interaction between number of pregnancies and age.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "143\n",
      "\n",
      "ALE plot of the 2nd-order effect of number of pregnancies and age. The interpretation of the plot is a bit inconclusive,\n",
      "showing what seems like overfitting. For example, the plot shows an odd model behavior at age of 18-20 and more\n",
      "than 3 pregnancies (up to 5 percentage point increase in cancer probability). There are not many women in the data\n",
      "with this constellation of age and number of pregnancies (actual data are displayed as points), so the model is not\n",
      "severely penalized during the training for making mistakes for those women.\n",
      "\n",
      "Advantages\n",
      "\n",
      "ALE plots are unbiased, which means they still work when features are correlated. Partial\n",
      "dependence plots fail in this scenario because they marginalize over unlikely or even physically\n",
      "impossible combinations of feature values. [{'source': './data/xai.pdf', 'start_index': 288606}]\n",
      "* ALE plots for the interaction of two features\n",
      "\n",
      "ALE plots can also show the interaction effect of two features. The calculation principles are the same\n",
      "as for a single feature, but we work with rectangular cells instead of intervals, because we have to\n",
      "accumulate the effects in two dimensions. In addition to adjusting for the overall mean effect, we\n",
      "also adjust for the main effects of both features. This means that ALE for two features estimate the\n",
      "second-order effect, which does not include the main effects of the features. In other words, ALE\n",
      "for two features only shows the additional interaction effect of the two features. I spare you the\n",
      "formulas for 2D ALE plots because they are long and unpleasant to read. If you are interested in\n",
      "the calculation, I refer you to the paper, formulas (13) – (16). I will rely on visualizations to develop\n",
      "intuition about the second-order ALE calculation.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "132 [{'source': './data/xai.pdf', 'start_index': 273223}]\n",
      "* The interpretation of the features in the linear regression model can be automated by using following\n",
      "text templates.\n",
      "\n",
      "Interpretation of a Numerical Feature\n",
      "\n",
      "An increase of feature xk by one unit increases the prediction for y by (cid:12)k units when all other feature\n",
      "values remain fixed.\n",
      "\n",
      "Interpretation of a Categorical Feature\n",
      "\n",
      "Changing feature xk from the reference category to the other category increases the prediction for\n",
      "y by (cid:12)k when all other features remain fixed.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "41\n",
      "\n",
      "Another important measurement for interpreting linear models is the R-squared measurement. R-\n",
      "squared tells you how much of the total variance of your target outcome is explained by the model.\n",
      "The higher R-squared, the better your model explains the data. The formula for calculating R-\n",
      "squared is:\n",
      "\n",
      "SSE is the squared sum of the error terms:\n",
      "\n",
      "R2 = 1 (cid:0) SSE/SST\n",
      "\n",
      "SSE =\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "(y(i) (cid:0) ^y(i))2\n",
      "\n",
      "SST is the squared sum of the data variance:\n",
      "\n",
      "SST =\n",
      "\n",
      "n∑\n",
      "\n",
      "i=1 [{'source': './data/xai.pdf', 'start_index': 101255}]\n",
      "* Perform the following steps to obtain a surrogate model:\n",
      "\n",
      "1. Select a dataset X. This can be the same dataset that was used for training the black box model\n",
      "or a new dataset from the same distribution. You could even select a subset of the data or a\n",
      "grid of points, depending on your application.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "164\n",
      "\n",
      "2. For the selected dataset X, get the predictions of the black box model.\n",
      "3. Select an interpretable model type (linear model, decision tree, …).\n",
      "4. Train the interpretable model on the dataset X and its predictions.\n",
      "5. Congratulations! You now have a surrogate model.\n",
      "6. Measure how well the surrogate model replicates the predictions of the black box model.\n",
      "7. Interpret the surrogate model.\n",
      "\n",
      "You may find approaches for surrogate models that have some extra steps or differ a little, but the\n",
      "general idea is usually as described here.\n",
      "\n",
      "One way to measure how well the surrogate replicates the black box model is the R-squared measure:\n",
      "\n",
      "R2 = 1 (cid:0) SSE\n",
      "SST [{'source': './data/xai.pdf', 'start_index': 325298}]\n",
      "* Trees are also quite unstable. A few changes in the training dataset can create a completely different\n",
      "tree. This is because each split depends on the parent split. And if a different feature is selected as\n",
      "the first split feature, the entire tree structure changes. It does not create confidence in the model if\n",
      "the structure changes so easily.\n",
      "\n",
      "Decision trees are very interpretable – as long as they are short. The number of terminal nodes\n",
      "increases quickly with depth. The more terminal nodes and the deeper the tree, the more difficult\n",
      "it becomes to understand the decision rules of a tree. A depth of 1 means 2 terminal nodes. Depth\n",
      "of 2 means max. 4 nodes. Depth of 3 means max. 8 nodes. The maximum number of terminal nodes\n",
      "in a tree is 2 to the power of the depth.\n",
      "\n",
      "Software [{'source': './data/xai.pdf', 'start_index': 180585}]\n",
      "* I believe we need to better explain to non-experts what types of problems can be formulated as\n",
      "machine learning problems. I know many highly paid data scientists who perform Excel calculations\n",
      "or classic business intelligence with reporting and SQL queries instead of applying machine learning.\n",
      "But a few companies are already successfully using machine learning, with the big Internet\n",
      "companies at the forefront. We need to find better ways to integrate machine learning into processes\n",
      "and products, train people and develop machine learning tools that are easy to use. I believe that\n",
      "machine learning will become much easier to use: We can already see that machine learning is\n",
      "becoming more accessible, for example through cloud services (“Machine Learning as a service” –\n",
      "just to throw a few buzzwords around). Once machine learning has matured – and this toddler has\n",
      "already taken its first steps – my next prediction is:\n",
      "\n",
      "Machine learning will fuel a lot of things. [{'source': './data/xai.pdf', 'start_index': 479226}]\n",
      "* Result of the interpretation method The various interpretation methods can be roughly differen-\n",
      "tiated according to their results.\n",
      "\n",
      "• Feature summary statistic: Many interpretation methods provide summary statistics for each\n",
      "feature. Some methods return a single number per feature, such as feature importance, or a\n",
      "more complex result, such as the pairwise feature interaction strengths, which consist of a\n",
      "number for each feature pair.\n",
      "\n",
      "• Feature summary visualization: Most of the feature summary statistics can also be visualized.\n",
      "Some feature summaries are actually only meaningful if they are visualized and a table would\n",
      "be a wrong choice. The partial dependence of a feature is such a case. Partial dependence plots\n",
      "are curves that show a feature and the average predicted outcome. The best way to present\n",
      "partial dependences is to actually draw the curve instead of printing the coordinates. [{'source': './data/xai.pdf', 'start_index': 49286}]\n",
      "* Deletion Diagnostics\n",
      "\n",
      "Statisticians have already done a lot of research in the area of influential instances, especially for\n",
      "(generalized) linear regression models. When you search for “influential observations”, the first\n",
      "search results are about measures like DFBETA and Cook’s distance. DFBETA measures the effect\n",
      "of deleting an instance on the model parameters. Cook’s distance (Cook, 1977¹¹¹) measures the\n",
      "effect of deleting an instance on model predictions. For both measures we have to retrain the model\n",
      "\n",
      "¹¹¹Cook, R. Dennis. “Detection of influential observation in linear regression.” Technometrics 19.1 (1977): 15-18.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "222\n",
      "\n",
      "repeatedly, omitting individual instances each time. The parameters or predictions of the model with\n",
      "all instances is compared with the parameters or predictions of the model with one of the instances\n",
      "deleted from the training data.\n",
      "\n",
      "DFBETA is defined as:\n",
      "\n",
      "DF BET Ai = (cid:12) (cid:0) (cid:12)((cid:0)i) [{'source': './data/xai.pdf', 'start_index': 443005}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "149\n",
      "\n",
      "The interaction strength (H-statistic) for each feature with all other features for a support vector machine predicting\n",
      "bicycle rentals. Overall, the interaction effects between the features are very weak (below 10% of variance explained\n",
      "per feature).\n",
      "\n",
      "In the next example, we calculate the interaction statistic for a classification problem. We analyze\n",
      "the interactions between features in a random forest trained to predict cervical cancer, given some\n",
      "risk factors.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "150\n",
      "\n",
      "The interaction strength (H-statistic) for each feature with all other features for a random forest predicting the\n",
      "probability of cervical cancer. The years on hormonal contraceptives has the highest relative interaction effect with\n",
      "all other features, followed by the number of pregnancies. [{'source': './data/xai.pdf', 'start_index': 300869}]\n",
      "* l(cid:3)\n",
      "j (xj) = min((cid:14)+\n",
      "\n",
      "j ; max((cid:14)(cid:0)\n",
      "\n",
      "j ; xj))\n",
      "\n",
      "j and (cid:14)+\n",
      "\n",
      "where (cid:14)(cid:0)\n",
      "j are the (cid:14) quantiles of the data distribution of feature xj. A choice of 0.05 for (cid:14) means\n",
      "that any value of feature xj that is in the 5% lowest or 5% highest values will be set to the quantiles\n",
      "at 5% or 95% respectively. As a rule of thumb, you can choose (cid:14) = 0.025. In addition, the linear terms\n",
      "have to be normalized so that they have the same prior importance as a typical decision rule:\n",
      "\n",
      "lj(xj) = 0:4 (cid:1) l(cid:3)\n",
      "\n",
      "j (xj)/std(l(cid:3)\n",
      "\n",
      "j (xj))\n",
      "\n",
      "The 0:4 is the average standard deviation of rules with a uniform support distribution of sk (cid:24) U (0; 1).\n",
      "\n",
      "We combine both types of features to generate a new feature matrix and train a sparse linear model\n",
      "with Lasso, with the following structure:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "105\n",
      "\n",
      "^f (x) = ^(cid:12)0 +\n",
      "\n",
      "K∑\n",
      "\n",
      "k=1\n",
      "\n",
      "^(cid:11)krk(x) +\n",
      "\n",
      "p∑\n",
      "\n",
      "j=1\n",
      "\n",
      "^(cid:12)jlj(xj) [{'source': './data/xai.pdf', 'start_index': 228827}]\n",
      "* mvtnorm. Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi and Torsten Hothorn (2018).\n",
      "mvtnorm: Multivariate Normal and t Distributions. R package version 1.0-8. https://CRAN.R-\n",
      "project.org/package=mvtnorm\n",
      "\n",
      "NLP. Kurt Hornik (2018). NLP: Natural Language Processing Infrastructure. R package version 0.2-0.\n",
      "https://CRAN.R-project.org/package=NLP\n",
      "\n",
      "ParamHelpers. Bernd Bischl, Michel Lang, Jakob Richter, Jakob Bossek, Daniel Horn and Pascal\n",
      "Kerschke (2018). ParamHelpers: Helpers for Parameters in Black-Box Optimization, Tuning and\n",
      "Machine Learning. R package version 1.11. https://CRAN.R-project.org/package=ParamHelpers\n",
      "\n",
      "partykit. Torsten Hothorn and Achim Zeileis (2019). partykit: A Toolkit for Recursive Partytioning.\n",
      "R package version 1.2-3. https://CRAN.R-project.org/package=partykit\n",
      "\n",
      "pre. Marjolein Fokkema and Benjamin Christoffersen (2018). pre: Prediction Rule Ensembles. R\n",
      "package version 0.6.0. https://CRAN.R-project.org/package=pre [{'source': './data/xai.pdf', 'start_index': 502941}]\n",
      "* Machines surpass humans in many tasks, such as playing chess (or more recently Go) or predicting\n",
      "the weather. Even if the machine is as good as a human or a bit worse at a task, there remain great\n",
      "advantages in terms of speed, reproducibility and scaling. A once implemented machine learning\n",
      "model can complete a task much faster than humans, reliably delivers consistent results and can\n",
      "be copied infinitely. Replicating a machine learning model on another machine is fast and cheap.\n",
      "The training of a human for a task can take decades (especially when they are young) and is very\n",
      "costly. A major disadvantage of using machine learning is that insights about the data and the task\n",
      "the machine solves is hidden in increasingly complex models. You need millions of numbers to\n",
      "\n",
      "\fIntroduction\n",
      "\n",
      "11 [{'source': './data/xai.pdf', 'start_index': 26664}]\n",
      "* non-actionable knowledge, because she cannot enlarge her apartment. Finally, by tweaking only the\n",
      "feature values under her control (built-in kitchen yes/no, pets allowed yes/no, type of floor, etc.),\n",
      "she finds out that if she allows pets and installs windows with better insulation, she can charge 1000\n",
      "Euro. Anna had intuitively worked with counterfactuals to change the outcome. [{'source': './data/xai.pdf', 'start_index': 380928}]\n",
      "* The end product of the RuleFit procedure is a linear model with additional fancy features (the\n",
      "decision rules). But since it is a linear model, the weight interpretation is still unintuitive. It comes\n",
      "with the same “footnote” as a usual linear regression model: “… given all features are fixed.” It gets\n",
      "a bit more tricky when you have overlapping rules. For example, one decision rule (feature) for the\n",
      "bicycle prediction could be: “temp > 10” and another rule could be “temp > 15 & weather=’GOOD’”. If\n",
      "the weather is good and the temperature is above 15 degrees, the temperature is automatically greater\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "107 [{'source': './data/xai.pdf', 'start_index': 233505}]\n",
      "* Let us return to the regression example, in which we predict the number of bikes that will be rented\n",
      "on a given day. First we fit a machine learning model, then we analyze the partial dependencies.\n",
      "In this case, we have fitted a random forest to predict the number of bicycles and use the partial\n",
      "dependence plot to visualize the relationships the model has learned. The influence of the weather\n",
      "features on the predicted bike counts is visualized in the following figure.\n",
      "\n",
      "PDPs for the bicycle count prediction model and temperature, humidity and wind speed. The largest differences can\n",
      "be seen in the temperature. The hotter, the more bikes are rented. This trend goes up to 20 degrees Celsius, then\n",
      "flattens and drops slightly at 30. Marks on the x-axis indicate the data distribution.\n",
      "\n",
      "For warm but not too hot weather, the model predicts on average a high number of rented bicycles.\n",
      "Potential bikers are increasingly inhibited in renting a bike when humidity exceeds 60%. In addition, [{'source': './data/xai.pdf', 'start_index': 247105}]\n",
      "* The other parameters are the choice of the kernel and the kernel scaling parameter. We have the\n",
      "same problem as with the number of prototypes and criticisms: How do we select a kernel and its\n",
      "scaling parameter? Again, when we use MMD-critic as a nearest prototype classifier, we can tune\n",
      "the kernel parameters. For the unsupervised use cases of MMD-critic, however, it is unclear. (Maybe\n",
      "I am a bit harsh here, since all unsupervised methods have this problem.)\n",
      "\n",
      "It takes all the features as input, disregarding the fact that some features might not be relevant\n",
      "for predicting the outcome of interest. One solution is to use only relevant features, for example\n",
      "image embeddings instead of raw pixels. This works as long as we have a way to project the original\n",
      "instance onto a representation that contains only relevant information.\n",
      "\n",
      "There is some code available, but it is not yet implemented as nicely packaged and documented\n",
      "software.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "Code and Alternatives\n",
      "\n",
      "217 [{'source': './data/xai.pdf', 'start_index': 435155}]\n",
      "* Intercept\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "workY\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "temp\n",
      "25\n",
      "12\n",
      "30\n",
      "5\n",
      "\n",
      "workY.temp\n",
      "25\n",
      "0\n",
      "0\n",
      "5\n",
      "\n",
      "The new column “workY.temp” captures the interaction between the features working day (work)\n",
      "and temperature (temp). This new feature column is zero for an instance if the work feature is at\n",
      "the reference category (“N” for no working day), otherwise it assumes the values of the instances\n",
      "temperature feature. With this type of encoding, the linear model can learn a different linear effect of\n",
      "temperature for both types of days. This is the interaction effect between the two features. Without\n",
      "an interaction term, the combined effect of a categorical and a numerical feature can be described\n",
      "by a line that is vertically shifted for the different categories. If we include the interaction, we allow\n",
      "the effect of the numerical features (the slope) to have a different value in each category. [{'source': './data/xai.pdf', 'start_index': 152051}]\n",
      "* Interpretable Models\n",
      "\n",
      "71\n",
      "\n",
      "It is easier to understand the interaction visually. By introducing an interaction term between a\n",
      "categorical and a numerical feature, we get two slopes for the temperature instead of one. The\n",
      "temperature slope for days on which people do not have to work (‘NO WORKING DAY’) can be\n",
      "read directly from the table (125.4). The temperature slope for days on which people have to work\n",
      "(‘WORKING DAY’) is the sum of both temperature weights (125.4 -21.8 = 103.6). The intercept of\n",
      "the ‘NO WORKING DAY’-line at temperature = 0 is determined by the intercept term of the linear\n",
      "model (2185.8). The intercept of the ‘WORKING DAY’-line at temperature = 0 is determined by the\n",
      "intercept term + the effect of working day (2185.8 + 451.9 = 2637.7).\n",
      "\n",
      "The effect (including interaction) of temperature and working day on the predicted number of bikes for a linear\n",
      "model. Effectively, we get two slopes for the temperature, one for each category of the working day feature. [{'source': './data/xai.pdf', 'start_index': 156260}]\n",
      "* Feature categorization\n",
      "\n",
      "Another possibility to achieve a nonlinear effect is to discretize the feature; turn it into a categorical\n",
      "feature. For example, you could cut the temperature feature into 20 intervals with the levels [-10,\n",
      "-5), [-5, 0), … and so on. When you use the categorized temperature instead of the continuous\n",
      "temperature, the linear model would estimate a step function because each level gets its own\n",
      "estimate. The problem with this approach is that it needs more data, it is more likely to overfit\n",
      "and it is unclear how to discretize the feature meaningfully (equidistant intervals or quantiles? how\n",
      "many intervals?). I would only use discretization if there is a very strong case for it. For example, to\n",
      "make the model comparable to another study.\n",
      "\n",
      "Generalized Additive Models (GAMs) [{'source': './data/xai.pdf', 'start_index': 160690}]\n",
      "* Second-order effect plots can be a bit annoying to interpret, as you always have to keep the\n",
      "main effects in mind. It is tempting to read the heat maps as the total effect of the two features,\n",
      "but it is only the additional effect of the interaction. The pure second-order effect is interesting\n",
      "for discovering and exploring interactions, but for interpreting what the effect looks like, I think it\n",
      "makes more sense to integrate the main effects into the plot.\n",
      "\n",
      "The implementation of ALE plots is much more complex and less intuitive compared to partial\n",
      "dependence plots.\n",
      "\n",
      "Even though ALE plots are not biased in case of correlated features, interpretation remains difficult\n",
      "when features are strongly correlated. Because if they have a very strong correlation, it only makes\n",
      "sense to analyze the effect of changing both features together and not in isolation. This disadvantage\n",
      "is not specific to ALE plots, but a general problem of strongly correlated features. [{'source': './data/xai.pdf', 'start_index': 292126}]\n",
      "* About the theory: There is actually not much theory needed to understand surrogate models. We\n",
      "want to approximate our black box prediction function f as closely as possible with the surrogate\n",
      "model prediction function g, under the constraint that g is interpretable. For the function g any\n",
      "interpretable model – for example from the interpretable models chapter – can be used.\n",
      "\n",
      "For example a linear model:\n",
      "\n",
      "Or a decision tree:\n",
      "\n",
      "g(x) = (cid:12)0 + (cid:12)1x1 + : : : + (cid:12)pxp\n",
      "\n",
      "g(x) =\n",
      "\n",
      "M∑\n",
      "\n",
      "m=1\n",
      "\n",
      "cmIfx 2 Rmg\n",
      "\n",
      "Training a surrogate model is a model-agnostic method, since it does not require any information\n",
      "about the inner workings of the black box model, only access to data and the prediction function is\n",
      "necessary. If the underlying machine learning model was replaced with another, you could still use\n",
      "the surrogate method. The choice of the black box model type and of the surrogate model type is\n",
      "decoupled.\n",
      "\n",
      "Perform the following steps to obtain a surrogate model: [{'source': './data/xai.pdf', 'start_index': 324382}]\n",
      "* Biggio et. al (2018)¹⁰⁶ give a nice review of ten years of research on adversarial machine learning, on\n",
      "which this section is based. Cybersecurity is an arms-race in which attackers and defenders outwit\n",
      "each other time and again.\n",
      "\n",
      "There are three golden rules in cybersecurity: 1) know your adversary 2) be proactive and 3)\n",
      "protect yourself.\n",
      "\n",
      "Different applications have different adversaries. People who try to defraud other people via email\n",
      "for their money are adversary agents of users and providers of email services. The providers want\n",
      "to protect their users, so that they can continue using their mail program, the attackers want to get\n",
      "people to give them money. Knowing your adversaries means knowing their goals. Assuming you\n",
      "do not know that these spammers exist and the only abuse of the email service is sending pirated\n",
      "copies of music, then the defense would be different (e.g. scanning the attachments for copyrighted\n",
      "material instead of analyzing the text for spam indicators). [{'source': './data/xai.pdf', 'start_index': 414254}]\n",
      "* Theory\n",
      "\n",
      "The concept is really straightforward: We measure the importance of a feature by calculating the\n",
      "increase in the model’s prediction error after permuting the feature. A feature is “important” if\n",
      "shuffling its values increases the model error, because in this case the model relied on the feature for\n",
      "the prediction. A feature is “unimportant” if shuffling its values leaves the model error unchanged,\n",
      "because in this case the model ignored the feature for the prediction. The permutation feature\n",
      "importance measurement was introduced by Breiman (2001)⁷⁶ for random forests. Based on this idea,\n",
      "Fisher, Rudin, and Dominici (2018)⁷⁷ proposed a model-agnostic version of the feature importance\n",
      "and called it model reliance. They also introduced more advanced ideas about feature importance,\n",
      "for example a (model-specific) version that takes into account that many prediction models may\n",
      "predict the data well. Their paper is worth reading. [{'source': './data/xai.pdf', 'start_index': 306614}]\n",
      "* The permutation feature importance algorithm based on Fisher, Rudin, and Dominici (2018):\n",
      "\n",
      "Input: Trained model f, feature matrix X, target vector y, error measure L(y,f).\n",
      "\n",
      "1. Estimate the original model error eʳⁱ = L(y, f(X)) (e.g. mean squared error)\n",
      "2. For each feature j = 1,…,p do:\n",
      "\n",
      "• Generate feature matrix Xʳ by permuting feature j in the data X. This breaks the\n",
      "\n",
      "association between feature j and true outcome y.\n",
      "\n",
      "• Estimate error eʳ = L(Y,f(Xʳ)) based on the predictions of the permuted data.\n",
      "• Calculate permutation feature importance FIʲ= eʳ/eʳⁱ. Alternatively, the difference can\n",
      "\n",
      "be used: FIʲ = eʳ - eʳⁱ\n",
      "\n",
      "3. Sort features by descending FI. [{'source': './data/xai.pdf', 'start_index': 307559}]\n",
      "* Examples\n",
      "\n",
      "Let us see ALE plots in action. I have constructed a scenario in which partial dependence plots fail.\n",
      "The scenario consists of a prediction model and two strongly correlated features. The prediction\n",
      "model is mostly a linear regression model, but does something weird at a combination of the two\n",
      "features for which we have never observed instances.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "134\n",
      "\n",
      "Two features and the predicted outcome. The model predicts the sum of the two features (shaded background), with\n",
      "the exception that if x1 is greater than 0.7 and x2 less than 0.3, the model always predicts 2. This area is far from\n",
      "the distribution of data (point cloud) and does not affect the performance of the model and also should not affect its\n",
      "interpretation. [{'source': './data/xai.pdf', 'start_index': 278478}]\n",
      "* between features to predict the target outcome. You can include interactions in any type of model\n",
      "by manually creating interaction features. Interactions can improve predictive performance, but too\n",
      "many or too complex interactions can hurt interpretability. Some models handle only regression,\n",
      "some only classification, and still others both. [{'source': './data/xai.pdf', 'start_index': 92012}]\n",
      "* How can we get the linear model to include interactions? Before you fit the linear model, add a\n",
      "column to the feature matrix that represents the interaction between the features and fit the model\n",
      "as usual. The solution is elegant in a way, since it does not require any change of the linear model,\n",
      "only additional columns in the data. In the working day and temperature example, we would add a\n",
      "new feature that has zeros for no-work days, otherwise it has the value of the temperature feature,\n",
      "assuming that working day is the reference category. Suppose our data looks like this:\n",
      "\n",
      "work\n",
      "Y\n",
      "N\n",
      "N\n",
      "Y\n",
      "\n",
      "temp\n",
      "25\n",
      "12\n",
      "30\n",
      "5\n",
      "\n",
      "The data matrix used by the linear model looks slightly different. The following table shows what\n",
      "the data prepared for the model looks like if we do not specify any interactions. Normally, this\n",
      "transformation is performed automatically by any statistical software.\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "69\n",
      "\n",
      "Intercept\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "workY\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "\n",
      "temp\n",
      "25\n",
      "12\n",
      "30\n",
      "5 [{'source': './data/xai.pdf', 'start_index': 150753}]\n",
      "* (cid:22)R2 = R2 (cid:0) (1 (cid:0) R2)\n",
      "\n",
      "p\n",
      "n (cid:0) p (cid:0) 1\n",
      "\n",
      "where p is the number of features and n the number of instances.\n",
      "\n",
      "It is not meaningful to interpret a model with very low (adjusted) R-squared, because such a model\n",
      "basically does not explain much of the variance. Any interpretation of the weights would not be\n",
      "meaningful.\n",
      "\n",
      "Feature Importance\n",
      "\n",
      "The importance of a feature in a linear regression model can be measured by the absolute value of\n",
      "its t-statistic. The t-statistic is the estimated weight scaled with its standard error.\n",
      "\n",
      "t ^(cid:12)j\n",
      "\n",
      "=\n",
      "\n",
      "^(cid:12)j\n",
      "SE( ^(cid:12)j)\n",
      "\n",
      "Let us examine what this formula tells us: The importance of a feature increases with increasing\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "42\n",
      "\n",
      "weight. This makes sense. The more variance the estimated weight has (= the less certain we are\n",
      "about the correct value), the less important the feature is. This also makes sense.\n",
      "\n",
      "Example [{'source': './data/xai.pdf', 'start_index': 103017}]\n",
      "* • No feature values\n",
      "• park-nearby\n",
      "• size-50\n",
      "• floor-2nd\n",
      "• park-nearby+size-50\n",
      "• park-nearby+floor-2nd\n",
      "• size-50+floor-2nd\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "180\n",
      "\n",
      "• park-nearby+size-50+floor-2nd.\n",
      "\n",
      "For each of these coalitions we compute the predicted apartment price with and without the feature\n",
      "value cat-banned and take the difference to get the marginal contribution. The Shapley value is the\n",
      "(weighted) average of marginal contributions. We replace the feature values of features that are not\n",
      "in a coalition with random feature values from the apartment dataset to get a prediction from the\n",
      "machine learning model.\n",
      "\n",
      "All 8 coalitions needed for computing the exact Shapley value of the cat-banned feature value.\n",
      "\n",
      "If we estimate the Shapley values for all feature values, we get the complete distribution of the\n",
      "prediction (minus the average) among the feature values.\n",
      "\n",
      "Examples and Interpretation [{'source': './data/xai.pdf', 'start_index': 354523}]\n",
      "* After looking at the feature interactions of each feature with all other features, we can select one\n",
      "of the features and dive deeper into all the 2-way interactions between the selected feature and the\n",
      "other features.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "151\n",
      "\n",
      "The 2-way interaction strengths (H-statistic) between number of pregnancies and each other feature. There is a strong\n",
      "interaction between the number of pregnancies and the age.\n",
      "\n",
      "Advantages\n",
      "\n",
      "The interaction H-statistic has an underlying theory through the partial dependence decomposi-\n",
      "tion.\n",
      "\n",
      "The H-statistic has a meaningful interpretation: The interaction is defined as the share of variance\n",
      "that is explained by the interaction.\n",
      "\n",
      "Since the statistic is dimensionless and always between 0 and 1, it is comparable across features\n",
      "and even across models.\n",
      "\n",
      "The statistic detects all kinds of interactions, regardless of their particular form. [{'source': './data/xai.pdf', 'start_index': 301690}]\n",
      "* Example\n",
      "\n",
      "Let us try OneR with real data. We use the cervical cancer classification task to test the OneR\n",
      "algorithm. All continuous input features were discretized into their 5 quantiles. The following rules\n",
      "are created:\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "89\n",
      "\n",
      "Age\n",
      "(12.9,27.2]\n",
      "(27.2,41.4]\n",
      "(41.4,55.6]\n",
      "(55.6,69.8]\n",
      "(69.8,84.1]\n",
      "\n",
      "prediction\n",
      "Healthy\n",
      "Healthy\n",
      "Healthy\n",
      "Healthy\n",
      "Healthy\n",
      "\n",
      "The age feature was chosen by OneR as the best predictive feature. Since cancer is rare, for each rule\n",
      "the majority class and therefore the predicted label is always Healthy, which is rather unhelpful.\n",
      "It does not make sense to use the label prediction in this unbalanced case. The cross table between\n",
      "the ‘Age’ intervals and Cancer/Healthy together with the percentage of women with cancer is more\n",
      "informative:\n",
      "\n",
      "Age=(12.9,27.2]\n",
      "Age=(27.2,41.4]\n",
      "Age=(41.4,55.6]\n",
      "Age=(55.6,69.8]\n",
      "Age=(69.8,84.1]\n",
      "\n",
      "# Cancer\n",
      "26\n",
      "25\n",
      "4\n",
      "0\n",
      "0\n",
      "\n",
      "# Healthy\n",
      "477\n",
      "290\n",
      "31\n",
      "1\n",
      "4\n",
      "\n",
      "P(Cancer)\n",
      "0.05\n",
      "0.08\n",
      "0.11\n",
      "0.00\n",
      "0.00 [{'source': './data/xai.pdf', 'start_index': 192009}]\n",
      "* Finding criticisms is independent of the selection process of the prototypes. But it makes\n",
      "sense to select prototypes according to MMD-critic, because then both prototypes and criticisms\n",
      "are created using the same method of comparing prototypes and data densities.\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "You have to choose the number of prototypes and criticisms. As much as this can be nice-to-\n",
      "have, it is also a disadvantage. How many prototypes and criticisms do we actually need? The more\n",
      "the better? The less the better? One solution is to select the number of prototypes and criticisms\n",
      "by measuring how much time humans have for the task of looking at the images, which depends\n",
      "on the particular application. Only when using MMD-critic to build a classifier do we have a way\n",
      "to optimize it directly. One solution could be a screeplot showing the number of prototypes on the\n",
      "x-axis and the MMD2 measure on the y-axis. We would choose the number of prototypes where the\n",
      "MMD2 curve flattens. [{'source': './data/xai.pdf', 'start_index': 434181}]\n",
      "* In this chapter I use the expression “data point” to refer to a single instance, to emphasize the\n",
      "interpretation that an instance is also a point in a coordinate system where each feature is a\n",
      "dimension. The following figure shows a simulated data distribution, with some of the instances\n",
      "chosen as prototypes and some as criticisms. The small points are the data, the large points the\n",
      "prototypes and the large squares the criticisms. The prototypes are selected (manually) to cover\n",
      "the centers of the data distribution and the criticisms are points in a cluster without a prototype.\n",
      "Prototypes and criticisms are always actual instances from the data.\n",
      "\n",
      "Prototypes and criticisms for a data distribution with two features x1 and x2.\n",
      "\n",
      "I selected the prototypes manually, which does not scale well and probably leads to poor results.\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "209 [{'source': './data/xai.pdf', 'start_index': 419368}]\n",
      "* E(coffeesjstress; sleep; work) = exp((cid:12)0 + (cid:12)stressxstress + (cid:12)sleepxsleep + (cid:12)workxwork)\n",
      "\n",
      "Since all the weights are in the exponential function, the effect interpretation is not additive, but\n",
      "multiplicative, because exp(a + b) is exp(a) times exp(b). The last ingredient for the interpretation\n",
      "\n",
      "\fInterpretable Models\n",
      "\n",
      "68\n",
      "\n",
      "is the actual weights of the toy example. The following table lists the estimated weights and\n",
      "exp(weights) together with the 95% confidence interval:\n",
      "\n",
      "(Intercept)\n",
      "stress\n",
      "sleep\n",
      "workYES\n",
      "\n",
      "weight\n",
      "-0.12\n",
      "0.11\n",
      "-0.16\n",
      "0.88\n",
      "\n",
      "exp(weight) [2.5%, 97.5%]\n",
      "0.89 [0.56, 1.38]\n",
      "1.11 [1.06, 1.17]\n",
      "0.85 [0.81, 0.89]\n",
      "2.42 [1.87, 3.16] [{'source': './data/xai.pdf', 'start_index': 148746}]\n",
      "* 1. Find prototypes and criticisms with MMD-critic.\n",
      "2. Train a machine learning model as usual.\n",
      "3. Predict outcomes for the prototypes and criticisms with the machine learning model.\n",
      "4. Analyse the predictions: In which cases was the algorithm wrong? Now you have a number\n",
      "of examples that represent the data well and help you to find the weaknesses of the machine\n",
      "learning model. [{'source': './data/xai.pdf', 'start_index': 430233}]\n",
      "* Let us look at the correlation between temperature, humidity and wind speed and all other features.\n",
      "Since the data also contains categorical features, we cannot only use the Pearson correlation\n",
      "coefficient, which only works if both features are numerical. Instead, I train a linear model to predict,\n",
      "for example, temperature based on one of the other features as input. Then I measure how much\n",
      "variance the other feature in the linear model explains and take the square root. If the other feature\n",
      "was numerical, then the result is equal to the absolute value of the standard Pearson correlation\n",
      "coefficient. But this model-based approach of “variance-explained” (also called ANOVA, which\n",
      "stands for ANalysis Of VAriance) works even if the other feature is categorical. The “variance-\n",
      "explained” measure lies always between 0 (no association) and 1 (temperature can be perfectly\n",
      "predicted from the other feature). We calculate the explained variance of temperature, humidity [{'source': './data/xai.pdf', 'start_index': 281785}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "Example\n",
      "\n",
      "122\n",
      "\n",
      "For example, take the cervical cancer ICE plot for age and center the lines on the youngest observed\n",
      "age:\n",
      "\n",
      "Centered ICE plot for predicted cancer probability by age. Lines are fixed to 0 at age 13. Compared to age 13, the\n",
      "predictions for most women remain unchanged until the age of 45 where the predicted probability increases.\n",
      "\n",
      "The centered ICE plots make it easier to compare the curves of individual instances. This can be\n",
      "useful if we do not want to see the absolute change of a predicted value, but the difference in the\n",
      "prediction compared to a fixed point of the feature range.\n",
      "\n",
      "Let’s have a look at centered ICE plots for the bicycle rental prediction:\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "123\n",
      "\n",
      "Centered ICE plots of predicted number of bikes by weather condition. The lines show the difference in prediction\n",
      "compared to the prediction with the respective feature value at its observed minimum.\n",
      "\n",
      "Derivative ICE Plot [{'source': './data/xai.pdf', 'start_index': 257966}]\n",
      "* Estimation of house prices, product recommendations, street sign detection, credit default prediction\n",
      "and fraud detection: All these examples have in common that they can be solved by machine\n",
      "learning. The tasks are different, but the approach is the same:\n",
      "Step 1: Data collection. The more, the better. The data must contain the outcome you want to predict\n",
      "and additional information from which to make the prediction. For a street sign detector (“Is there a\n",
      "street sign in the image?”), you would collect street images and label whether a street sign is visible\n",
      "or not. For a credit default predictor, you need past data on actual loans, information on whether\n",
      "the customers were in default with their loans, and data that will help you make predictions, such as\n",
      "income, past credit defaults, and so on. For an automatic house value estimator program, you could\n",
      "collect data from past house sales and information about the real estate such as size, location, and\n",
      "so on. [{'source': './data/xai.pdf', 'start_index': 25363}]\n"
     ]
    }
   ],
   "source": [
    "question = \"How to add explainability to convolution and attention layers?\"\n",
    "results = vector_store.similarity_search(\n",
    "    question,\n",
    "    k=400\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "results2 = vector_store2.similarity_search(\n",
    "    question,\n",
    "    k=400\n",
    ")\n",
    "for res in results2:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\\n\".join(map(lambda i: i.page_content, results))\n",
    "results2 = \"\\n\".join(map(lambda i: i.page_content, results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model_id, max_new_tokens=1000000)\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = f\"\"\"SYSTEM: \n",
    "{results}\n",
    "RAG 2 Answers 2:\n",
    "{results2}\n",
    "\n",
    "USER: {question}\n",
    "\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "\n",
    "result = hf.invoke(template)\n",
    "\n",
    "print(result['result'])\n",
    "print(\"Source Documentsresult:\", result['source_documents'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT‌4o Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index1\", embeddings, allow_dangerous_deserialization=True)\n",
    "vector_store2 = FAISS.load_local(\"faiss_index2\", embeddings2, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to add explainability to convolution and attention layers?\"\n",
    "results = vector_store.similarity_search(\n",
    "    question,\n",
    "    k=400\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "results2 = vector_store2.similarity_search(\n",
    "    question,\n",
    "    k=400\n",
    ")\n",
    "for res in results2:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\\n\".join(map(lambda i: i.page_content, results2))\n",
    "results2 = \"\\n\".join(map(lambda i: i.page_content, results2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 150850 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m      5\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mAnswer the request using only the context below.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mContext:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mRequest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     store\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     20\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombine only Context section provided and answer based on it only, summerizer it and dont use of your knowlegde in answer.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     21\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: template}\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlpllm/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlpllm/lib/python3.11/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    817\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    818\u001b[0m             {\n\u001b[1;32m    819\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    820\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    821\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    822\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    823\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    824\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    825\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    826\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    827\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    828\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    829\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    830\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    831\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    832\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    833\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    834\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    835\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    836\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    837\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    838\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    839\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    842\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    843\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    846\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    847\u001b[0m             },\n\u001b[1;32m    848\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    849\u001b[0m         ),\n\u001b[1;32m    850\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    851\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    852\u001b[0m         ),\n\u001b[1;32m    853\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    854\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlpllm/lib/python3.11/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlpllm/lib/python3.11/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    955\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    956\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    957\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    958\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    959\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    960\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlpllm/lib/python3.11/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 150850 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "template = f\"\"\"\n",
    "Answer the request using only the context below.\n",
    "Context:\n",
    "{results}\n",
    "\n",
    "{results2}\n",
    "\n",
    "\n",
    "Request: {question}\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    store=True,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Combine only Context section provided and answer based on it only, summerizer it and dont use of your knowlegde in answer.\"},\n",
    "        {\"role\": \"user\", \"content\": template}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorporating explainability into convolutional and attention layers can be achieved through various techniques. For convolutional layers, attention mechanisms enhance interpretability by highlighting which parts of an input image contributed most to the network's predictions. Techniques such as visual attention can be used to track what the model focuses on when making predictions, generating heat maps that indicate areas of importance.\n",
      "\n",
      "For attention layers, models can utilize integrated gradients or saliency maps to provide insights into how input features influence the model’s output. Additionally, gradient-based methods can be employed to visualize the contribution of different input components to the final decision made by the model.\n",
      "\n",
      "Incorporating these techniques effectively aids in understanding the decision-making process of the neural network, making it easier to identify why specific inputs lead to certain outputs. This is particularly useful in debugging models and ensuring compliance in applications that require transparency, such as loan approvals or medical diagnoses.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation phase of new architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./preface.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index1\", embeddings, allow_dangerous_deserialization=True)\n",
    "vector_store2 = FAISS.load_local(\"faiss_index2\", embeddings2, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* In  some  applications,  explainability  is  not  just  a  tool  to  debug  a  model;  it  can  be  a\n",
      "legal  requirement  (think  of  a  system  deciding  whether  or  not  it  should  grant  you  a\n",
      "loan).\n",
      "\n",
      "18 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.\n",
      "\n",
      "19 Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” Proceed‐\n",
      "ings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\n",
      "1135–1144.\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "553\n",
      "\n",
      "\fAttention  mechanisms  are  so  powerful  that  you  can  actually  build  state-of-the-art\n",
      "models using only attention mechanisms. [{'source': './data/ml.pdf', 'start_index': 1201072}]\n",
      "* Explainability\n",
      "One extra benefit of attention mechanisms is that they make it easier to understand\n",
      "what led the model to produce its output. This is called explainability. It can be espe‐\n",
      "cially useful when the model makes a mistake: for example, if an image of a dog walk‐\n",
      "ing in the snow is labeled as “a wolf walking in the snow,” then you can go back and\n",
      "check what the model focused on when it output the word “wolf.” You may find that it\n",
      "was paying attention not only to the dog, but also to the snow, hinting at a possible\n",
      "explanation: perhaps the way the model learned to distinguish dogs from wolves is by\n",
      "checking whether or not there’s a lot of snow around. You can then fix this by training\n",
      "the model with more images of wolves without snow, and dogs with snow. This exam‐\n",
      "ple comes from a great 2016 paper19 by Marco Tulio Ribeiro et al. that uses a different\n",
      "approach  to  explainability:  learning  an  interpretable  model  locally  around  a  classi‐\n",
      "fier’s prediction. [{'source': './data/ml.pdf', 'start_index': 1200084}]\n",
      "* As you can see, the field of Deep Computer Vision is vast and moving fast, with all\n",
      "sorts of architectures popping out every year, all based on convolutional neural net‐\n",
      "works. The progress made in just a few years has been astounding, and researchers\n",
      "are now focusing on harder and harder problems, such as adversarial learning (which\n",
      "attempts to make the network more resistant to images designed to fool it), explaina‐\n",
      "bility (understanding why the network makes a specific classification), realistic image\n",
      "generation (which we will come back to in Chapter 17), and single-shot learning (a sys‐\n",
      "tem  that  can  recognize  an  object  after  it  has  seen  it  just  once).  Some  even  explore\n",
      "completely  novel  architectures,  such  as  Geoffrey  Hinton’s  capsule  networks35  (I  pre‐\n",
      "sented them in a couple of videos, with the corresponding code in a notebook). Now\n",
      "on to the next chapter, where we will look at how to process sequential data such as [{'source': './data/ml.pdf', 'start_index': 1068947}]\n",
      "* Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "553\n",
      "\n",
      "\fAttention  mechanisms  are  so  powerful  that  you  can  actually  build  state-of-the-art\n",
      "models using only attention mechanisms.\n",
      "\n",
      "Attention Is All You Need: The Transformer Architecture\n",
      "In  a  groundbreaking  2017  paper,20  a  team  of  Google  researchers  suggested  that\n",
      "“Attention Is All You Need.” They managed to create an architecture called the Trans‐\n",
      "former, which significantly improved the state of the art in NMT without using any\n",
      "recurrent or convolutional layers,21 just attention mechanisms (plus embedding lay‐\n",
      "ers, dense layers, normalization layers, and a few other bits and pieces). As an extra\n",
      "bonus, this architecture was also much faster to train and easier to parallelize, so they\n",
      "managed to train it at a fraction of the time and cost of the previous state-of-the-art\n",
      "models.\n",
      "\n",
      "The Transformer architecture is represented in Figure 16-8. [{'source': './data/ml.pdf', 'start_index': 1201614}]\n",
      "* We simply wrap the decoder cell in an AttentionWrapper, and we provide the desired\n",
      "attention mechanism (Luong attention in this example).\n",
      "\n",
      "Visual Attention\n",
      "Attention mechanisms are now used for a variety of purposes. One of their first appli‐\n",
      "cations  beyond  NMT  was  in  generating  image  captions  using  visual  attention:17  a\n",
      "convolutional  neural  network  first  processes  the  image  and  outputs  some  feature\n",
      "maps, then a decoder RNN equipped with an attention mechanism generates the cap‐\n",
      "tion, one word at a time. At each decoder time step (each word), the decoder uses the\n",
      "attention  model  to  focus  on  just  the  right  part  of  the  image.  For  example,  in\n",
      "Figure  16-7,  the  model  generated  the  caption  “A  woman  is  throwing  a  frisbee  in  a\n",
      "park,” and you can see what part of the input image the decoder focused its attention\n",
      "on when it was about to output the word “frisbee”: clearly, most of its attention was\n",
      "focused on the frisbee. [{'source': './data/ml.pdf', 'start_index': 1198718}]\n",
      "* 16 Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation,” Proceed‐\n",
      "\n",
      "ings of the 2015 Conference on Empirical Methods in Natural Language Processing (2015): 1412–1421.\n",
      "\n",
      "Attention Mechanisms \n",
      "\n",
      "| \n",
      "\n",
      "551\n",
      "\n",
      "\fEquation 16-1. Attention mechanisms\n",
      "\n",
      " t = ∑\n",
      "i\n",
      "\n",
      "α t, i\n",
      "\n",
      " i\n",
      "\n",
      "with α t, i =\n",
      "\n",
      "and e t, i =\n",
      "\n",
      "exp e t, i\n",
      "∑i′ exp e t, i′\n",
      "⊺  i\n",
      "⊺   i\n",
      "tanh   t ;  i\n",
      "\n",
      " t\n",
      " t\n",
      "⊺\n",
      "\n",
      "dot\n",
      "\n",
      "general\n",
      "\n",
      "concat\n",
      "\n",
      "Here is how you can add Luong attention to an Encoder–Decoder model using Ten‐\n",
      "sorFlow Addons:\n",
      "\n",
      "attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
      "    units, encoder_state, memory_sequence_length=encoder_sequence_length)\n",
      "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
      "    decoder_cell, attention_mechanism, attention_layer_size=n_units)\n",
      "\n",
      "We simply wrap the decoder cell in an AttentionWrapper, and we provide the desired\n",
      "attention mechanism (Luong attention in this example). [{'source': './data/ml.pdf', 'start_index': 1197924}]\n",
      "* Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "397\n",
      "\n",
      "\fLet’s go through this code:\n",
      "\n",
      "• The  constructor  creates  the  DNN  with  five  dense  hidden  layers  and  one  dense\n",
      "\n",
      "output layer.\n",
      "\n",
      "• The  build()  method  creates  an  extra  dense  layer  which  will  be  used  to  recon‐\n",
      "struct the inputs of the model. It must be created here because its number of units\n",
      "must be equal to the number of inputs, and this number is unknown before the\n",
      "build() method is called.\n",
      "\n",
      "• The  call()  method  processes  the  inputs  through  all  five  hidden  layers,  then\n",
      "passes  the  result  through  the  reconstruction  layer,  which  produces  the  recon‐\n",
      "struction. [{'source': './data/ml.pdf', 'start_index': 857618}]\n",
      "* Convolutional neural networks (CNNs) emerged from the study of the brain’s visual\n",
      "cortex, and they have been used in image recognition since the 1980s. In the last few\n",
      "years, thanks to the increase in computational power, the amount of available training\n",
      "data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐\n",
      "aged to achieve superhuman performance on some complex visual tasks. They power\n",
      "image  search  services,  self-driving  cars,  automatic  video  classification  systems,  and\n",
      "more. Moreover, CNNs are not restricted to visual perception: they are also successful\n",
      "at many other tasks, such as voice recognition and natural language processing. How‐\n",
      "ever, we will focus on visual applications for now. [{'source': './data/ml.pdf', 'start_index': 965580}]\n",
      "* In the second part of this chapter, we will look at attention mechanisms. As their name\n",
      "suggests,  these  are  neural  network  components  that  learn  to  select  the  part  of  the\n",
      "inputs that the rest of the model should focus on at each time step. First we will see\n",
      "how to boost the performance of an RNN-based Encoder–Decoder architecture using\n",
      "attention, then we will drop RNNs altogether and look at a very successful attention-\n",
      "only  architecture  called  the  Transformer.  Finally,  we  will  take  a  look  at  some  of  the\n",
      "most  important  advances  in  NLP  in  2018  and  2019,  including  incredibly  powerful\n",
      "language models such as GPT-2 and BERT, both based on Transformers.\n",
      "\n",
      "Let’s start with a simple and fun model that can write like Shakespeare (well, sort of). [{'source': './data/ml.pdf', 'start_index': 1134784}]\n",
      "* Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "449\n",
      "\n",
      "\fFigure 14-4. Reducing dimensionality using a stride of 2\n",
      "\n",
      "Filters\n",
      "A neuron’s weights can be represented as a small image the size of the receptive field.\n",
      "For example, Figure 14-5 shows two possible sets of weights, called filters (or convolu‐\n",
      "tion kernels). The first one is represented as a black square with a vertical white line in\n",
      "the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\n",
      "1s); neurons using these weights will ignore everything in their receptive field except\n",
      "for  the  central  vertical  line  (since  all  inputs  will  get  multiplied  by  0,  except  for  the\n",
      "ones  located  in  the  central  vertical  line).  The  second  filter  is  a  black  square  with  a\n",
      "horizontal  white  line  in  the  middle.  Once  again,  neurons  using  these  weights  will\n",
      "ignore everything in their receptive field except for the central horizontal line. [{'source': './data/ml.pdf', 'start_index': 973811}]\n",
      "* an inception network or every residual unit in a ResNet to recalibrate the relative\n",
      "importance  of  feature  maps.  Finally,  Xception’s  main  innovation  was  the  use  of\n",
      "depthwise  separable  convolutional  layers,  which  look  at  spatial  patterns  and\n",
      "depthwise patterns separately. [{'source': './data/ml.pdf', 'start_index': 1620115}]\n",
      "* CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "475\n",
      "\n",
      "\fSeparable convolutional layers use fewer parameters, less memory,\n",
      "and  fewer  computations  than  regular  convolutional  layers,  and  in\n",
      "general  they  even  perform  better,  so  you  should  consider  using\n",
      "them by default (except after layers with few channels).\n",
      "\n",
      "The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\n",
      "versity of Hong Kong. They used an ensemble of many different techniques, includ‐\n",
      "ing  a  sophisticated  object-detection  system  called  GBD-Net,21  to  achieve  a  top-five\n",
      "error rate below 3%. Although this result is unquestionably impressive, the complex‐\n",
      "ity of the solution contrasted with the simplicity of ResNets. Moreover, one year later\n",
      "another fairly simple architecture performed even better, as we will see now. [{'source': './data/ml.pdf', 'start_index': 1022942}]\n",
      "* 280-295\n",
      "\n",
      "Hopfield networks, 773\n",
      "implementing MLPs with Keras, 295-320\n",
      "overview of, 279\n",
      "restricted Boltzmann machines (RBMs), 776\n",
      "self-organizing maps (SOMs), 780\n",
      "\n",
      "artificial neurons, 283\n",
      "association rule learning, 12\n",
      "associative memory networks, 773\n",
      "Asynchronous Advantage Actor-Critic (A3C),\n",
      "\n",
      "662\n",
      "\n",
      "asynchronous updates, 707\n",
      "Atari preprocessing, 645\n",
      "attention mechanisms\n",
      "\n",
      "defined, 526\n",
      "explainability and, 553\n",
      "overview of, 549\n",
      "Transformer architecture, 554\n",
      "visual attention, 552\n",
      "\n",
      "802 \n",
      "\n",
      "| \n",
      "\n",
      "Index\n",
      "\n",
      "attributes, 8\n",
      "autoencoders\n",
      "\n",
      "convolutional, 579\n",
      "denoising, 581\n",
      "efficient data representations, 569\n",
      "generative, 586\n",
      "versus Generative Adversarial Networks\n",
      "\n",
      "(GANs), 568\n",
      "overview of, 567\n",
      "parts of, 569\n",
      "PCA with undercomplete linear autoencod‐\n",
      "\n",
      "ers, 570\n",
      "\n",
      "probabilistic, 586\n",
      "recurrent, 580\n",
      "sparse, 582\n",
      "stacked, 572-575\n",
      "undercomplete, 570\n",
      "unsupervised pretraining using stacked,\n",
      "\n",
      "576-579\n",
      "\n",
      "variational, 586-591\n",
      "\n",
      "AutoGraphs, 407\n",
      "automatic differentiation (autodiff), 290, 399, [{'source': './data/ml.pdf', 'start_index': 1729952}]\n",
      "* VGGNet, 470\n",
      "\n",
      "\fvirtual GPU devices, 695\n",
      "visible units, 775\n",
      "visual attention, 552\n",
      "visualization algorithms, 11\n",
      "vocabulary, 432\n",
      "voice recognition, 445\n",
      "\n",
      "W\n",
      "wall time, 341\n",
      "warmup phase, 708\n",
      "WaveNet, 498, 521\n",
      "weak learners, 190\n",
      "weighted moving average model, 506\n",
      "white box models, 178\n",
      "Wide & Deep neural networks, 308\n",
      "wisdom of the crowd, 189\n",
      "word embeddings, 434\n",
      "\n",
      "word tokenization, 536\n",
      "WordTrees, 490\n",
      "workspace creation, 42\n",
      "\n",
      "X\n",
      "Xavier initialization, 333\n",
      "Xception (Extreme Inception), 474\n",
      "XGBoost, 208\n",
      "\n",
      "Y\n",
      "You Only Look Once (YOLO), 489\n",
      "\n",
      "Z\n",
      "zero padding, 449\n",
      "zero-shot learning (ZSL), 564\n",
      "ZF Net, 466\n",
      "\n",
      "Index \n",
      "\n",
      "| \n",
      "\n",
      "819\n",
      "\n",
      "\fAbout the Author [{'source': './data/ml.pdf', 'start_index': 1770749}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "743\n",
      "\n",
      "\fsuffers less from unstable gradients. One or more 1D convolutional layers can be\n",
      "useful in an RNN to efficiently preprocess the inputs, for example to reduce their\n",
      "temporal  resolution  (downsampling)  and  thereby  help  the  RNN  layers  detect\n",
      "long-term  patterns.  In  fact,  it  is  possible  to  use  only  convolutional  layers,  for\n",
      "example by building a WaveNet architecture. [{'source': './data/ml.pdf', 'start_index': 1627271}]\n",
      "* 5 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,\n",
      "\n",
      "no. 11 (1998): 2278–2324.\n",
      "\n",
      "The Architecture of the Visual Cortex \n",
      "\n",
      "| \n",
      "\n",
      "447\n",
      "\n",
      "\fConvolutional Layers\n",
      "The most important building block of a CNN is the convolutional layer:6 neurons in\n",
      "the first convolutional layer are not connected to every single pixel in the input image\n",
      "(like they were in the layers discussed in previous chapters), but only to pixels in their\n",
      "receptive  fields  (see  Figure  14-2).  In  turn,  each  neuron  in  the  second  convolutional\n",
      "layer is connected only to neurons located within a small rectangle in the first layer.\n",
      "This architecture allows the network to concentrate on small low-level features in the\n",
      "first  hidden  layer,  then  assemble  them  into  larger  higher-level  features  in  the  next\n",
      "hidden layer, and so on. This hierarchical structure is common in real-world images,\n",
      "which is one of the reasons why CNNs work so well for image recognition. [{'source': './data/ml.pdf', 'start_index': 970630}]\n",
      "* Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "vii\n",
      "\n",
      "\fSaving and Restoring a Model                                                                                314\n",
      "Using Callbacks                                                                                                         315\n",
      "Using TensorBoard for Visualization                                                                    317\n",
      "Fine-Tuning Neural Network Hyperparameters                                                     320\n",
      "Number of Hidden Layers                                                                                       323\n",
      "Number of Neurons per Hidden Layer                                                                 324\n",
      "Learning Rate, Batch Size, and Other Hyperparameters                                    325\n",
      "Exercises                                                                                                                        327 [{'source': './data/ml.pdf', 'start_index': 24201}]\n",
      "* 17 Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,” Proceedings\n",
      "\n",
      "of the 32nd International Conference on Machine Learning (2015): 2048–2057.\n",
      "\n",
      "552 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fFigure 16-7. Visual attention: an input image (left) and the model’s focus before produc‐\n",
      "ing the word “frisbee” (right)18 [{'source': './data/ml.pdf', 'start_index': 1199693}]\n",
      "* Figure 14-3. Connections between layers and zero padding\n",
      "\n",
      "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
      "the  receptive  fields,  as  shown  in  Figure  14-4.  This  dramatically  reduces  the  model’s\n",
      "computational complexity. The shift from one receptive field to the next is called the\n",
      "stride. In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4\n",
      "layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride is the\n",
      "same  in  both  directions,  but  it  does  not  have  to  be  so).  A  neuron  located  in  row  i,\n",
      "column j in the upper layer is connected to the outputs of the neurons in the previous\n",
      "layer located in rows i × sh to i × sh + fh – 1, columns j × sw to j × sw + fw – 1, where sh\n",
      "and sw are the vertical and horizontal strides.\n",
      "\n",
      "Convolutional Layers \n",
      "\n",
      "| \n",
      "\n",
      "449\n",
      "\n",
      "\fFigure 14-4. Reducing dimensionality using a stride of 2 [{'source': './data/ml.pdf', 'start_index': 972972}]\n",
      "* Generative Adversarial Networks \n",
      "\n",
      "| \n",
      "\n",
      "603\n",
      "\n",
      "\favoid division by zero). This technique avoids explosions in the activations due\n",
      "to excessive competition between the generator and the discriminator.\n",
      "\n",
      "The  combination  of  all  these  techniques  allowed  the  authors  to  generate  extremely\n",
      "convincing high-definition images of faces. But what exactly do we call “convincing”?\n",
      "Evaluation is one of the big challenges when working with GANs: although it is possi‐\n",
      "ble to automatically evaluate the diversity of the generated images, judging their qual‐\n",
      "ity is a much trickier and subjective task. One technique is to use human raters, but\n",
      "this is costly and time-consuming. So the authors proposed to measure the similarity\n",
      "between  the  local  image  structure  of  the  generated  images  and  the  training  images,\n",
      "considering  every  scale.  This  idea  led  them  to  another  groundbreaking  innovation:\n",
      "StyleGANs. [{'source': './data/ml.pdf', 'start_index': 1306178}]\n",
      "* Exercise Solutions \n",
      "\n",
      "| \n",
      "\n",
      "741\n",
      "\n",
      "\fsible to convert a trained CNN this way by appropriately reshaping the dense lay‐\n",
      "ers’ weight matrices.\n",
      "\n",
      "8. The main technical difficulty of semantic segmentation is the fact that a lot of the\n",
      "spatial  information  gets  lost  in  a  CNN  as  the  signal  flows  through  each  layer,\n",
      "especially  in  pooling  layers  and  layers  with  a  stride  greater  than  1.  This  spatial\n",
      "information needs to be restored somehow to accurately predict the class of each\n",
      "pixel.\n",
      "\n",
      "For  the  solutions  to  exercises  9  to  12,  please  see  the  Jupyter  notebooks  available  at\n",
      "https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Chapter 15: Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "1. Here are a few RNN applications: [{'source': './data/ml.pdf', 'start_index': 1621431}]\n",
      "* 460 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\ffully  connected  layers  (+ReLUs),  and  the  final  layer  outputs  the  prediction  (e.g.,  a\n",
      "softmax layer that outputs estimated class probabilities).\n",
      "\n",
      "Figure 14-11. Typical CNN architecture\n",
      "\n",
      "A common mistake is to use convolution kernels that are too large.\n",
      "For  example,  instead  of  using  a  convolutional  layer  with  a  5  ×  5\n",
      "kernel, stack two layers with 3 × 3 kernels: it will use fewer parame‐\n",
      "ters  and  require  fewer  computations,  and  it  will  usually  perform\n",
      "better. One exception is for the first convolutional layer: it can typi‐\n",
      "cally have a large kernel (e.g., 5 × 5), usually with a stride of 2 or\n",
      "more: this will reduce the spatial dimension of the image without\n",
      "losing  too  much  information,  and  since  the  input  image  only  has\n",
      "three channels in general, it will not be too costly. [{'source': './data/ml.pdf', 'start_index': 995338}]\n",
      "* Now let’s look at the second common building block of CNNs: the pooling layer.\n",
      "\n",
      "Pooling Layers\n",
      "Once  you  understand  how  convolutional  layers  work,  the  pooling  layers  are  quite\n",
      "easy  to  grasp.  Their  goal  is  to  subsample  (i.e.,  shrink)  the  input  image  in  order  to\n",
      "\n",
      "7 A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\n",
      "\n",
      "× 1002 × 3 = 675 million parameters!\n",
      "\n",
      "8 In the international system of units (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\n",
      "\n",
      "456 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\freduce  the  computational  load,  the  memory  usage,  and  the  number  of  parameters\n",
      "(thereby limiting the risk of overfitting). [{'source': './data/ml.pdf', 'start_index': 986538}]\n",
      "* 472 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-17. ResNet architecture\n",
      "\n",
      "Note that the number of feature maps is doubled every few residual units, at the same\n",
      "time as their height and width are halved (using a convolutional layer with stride 2).\n",
      "When this happens, the inputs cannot be added directly to the outputs of the residual\n",
      "unit  because  they  don’t  have  the  same  shape  (for  example,  this  problem  affects  the\n",
      "skip connection represented by the dashed arrow in Figure 14-17). To solve this prob‐\n",
      "lem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the\n",
      "right number of output feature maps (see Figure 14-18).\n",
      "\n",
      "Figure 14-18. Skip connection when changing feature map size and depth\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "473 [{'source': './data/ml.pdf', 'start_index': 1017733}]\n",
      "* 740 \n",
      "\n",
      "|  Appendix A: Exercise Solutions\n",
      "\n",
      "\f3. If your GPU runs out of memory while training a CNN, here are five things you\n",
      "could try to solve the problem (other than purchasing a GPU with more RAM):\n",
      "\n",
      "• Reduce the mini-batch size.\n",
      "\n",
      "• Reduce dimensionality using a larger stride in one or more layers.\n",
      "\n",
      "• Remove one or more layers.\n",
      "\n",
      "• Use 16-bit floats instead of 32-bit floats.\n",
      "\n",
      "• Distribute the CNN across multiple devices.\n",
      "\n",
      "4. A max pooling layer has no parameters at all, whereas a convolutional layer has\n",
      "\n",
      "quite a few (see the previous questions).\n",
      "\n",
      "5. A local response normalization layer makes the neurons that most strongly acti‐\n",
      "vate inhibit neurons at the same location but in neighboring feature maps, which\n",
      "encourages  different  feature  maps  to  specialize  and  pushes  them  apart,  forcing\n",
      "them to explore a wider range of features. It is typically used in the lower layers to\n",
      "have a larger pool of low-level features that the upper layers can build upon. [{'source': './data/ml.pdf', 'start_index': 1618393}]\n",
      "* (2017).\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "323\n",
      "\n",
      "\famount  of  time:  you  would  have  to  draw  each  tree  individually,  branch  by  branch,\n",
      "leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch,\n",
      "then copy and paste that branch to create a tree, and finally copy and paste this tree to\n",
      "make a forest, you would be finished in no time. Real-world data is often structured\n",
      "in such a hierarchical way, and deep neural networks automatically take advantage of\n",
      "this fact: lower hidden layers model low-level structures (e.g., line segments of vari‐\n",
      "ous  shapes  and  orientations),  intermediate  hidden  layers  combine  these  low-level\n",
      "structures to model intermediate-level structures (e.g., squares, circles), and the high‐\n",
      "est  hidden  layers  and  the  output  layer  combine  these  intermediate  structures  to\n",
      "model high-level structures (e.g., faces). [{'source': './data/ml.pdf', 'start_index': 685755}]\n",
      "* For example, suppose we’d already trained a CNN for flower classification and locali‐\n",
      "zation. It was trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4\n",
      "are sent through the softmax activation function, and this gives the class probabilities\n",
      "(one per class); output 5 is sent through the logistic activation function, and this gives\n",
      "the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐\n",
      "resent the bounding box’s center coordinates, as well as its height and width. We can\n",
      "now  convert  its  dense  layers  to  convolutional  layers.  In  fact,  we  don’t  even  need  to\n",
      "retrain it; we can just copy the weights from the dense layers to the convolutional lay‐\n",
      "ers! Alternatively, we could have converted the CNN into an FCN before training. [{'source': './data/ml.pdf', 'start_index': 1051139}]\n",
      "* hence the name of these modules.\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "467\n",
      "\n",
      "\ftional  cost  and  the  number  of  parameters,  speeding  up  training  and  improving\n",
      "generalization.\n",
      "\n",
      "• Each  pair  of  convolutional  layers  ([1  ×  1,  3  ×  3]  and  [1  ×  1,  5  ×  5])  acts  like  a\n",
      "single powerful convolutional layer, capable of capturing more complex patterns.\n",
      "Indeed, instead of sweeping a simple linear classifier across the image (as a single\n",
      "convolutional  layer  does),  this  pair  of  convolutional  layers  sweeps  a  two-layer\n",
      "neural network across the image.\n",
      "\n",
      "In  short,  you  can  think  of  the  whole  inception  module  as  a  convolutional  layer  on\n",
      "steroids, able to output feature maps that capture complex patterns at various scales.\n",
      "\n",
      "The number of convolutional kernels for each convolutional layer\n",
      "is  a  hyperparameter.  Unfortunately,  this  means  that  you  have  six\n",
      "more hyperparameters to tweak for every inception layer you add. [{'source': './data/ml.pdf', 'start_index': 1009843}]\n",
      "* | \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-5. Applying two different filters to get two feature maps\n",
      "\n",
      "Stacking Multiple Feature Maps\n",
      "Up to now, for simplicity, I have represented the output of each convolutional layer as\n",
      "a  2D  layer,  but  in  reality  a  convolutional  layer  has  multiple  filters  (you  decide  how\n",
      "many) and outputs one feature map per filter, so it is more accurately represented in\n",
      "3D (see Figure 14-6). It has one neuron per pixel in each feature map, and all neurons\n",
      "within a given feature map share the same parameters (i.e., the same weights and bias\n",
      "term). Neurons in different feature maps use different parameters. A neuron’s recep‐\n",
      "tive field is the same as described earlier, but it extends across all the previous layers’\n",
      "feature maps. In short, a convolutional layer simultaneously applies multiple trainable\n",
      "filters  to  its  inputs,  making  it  capable  of  detecting  multiple  features  anywhere  in  its\n",
      "inputs. [{'source': './data/ml.pdf', 'start_index': 975665}]\n",
      "* CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "471\n",
      "\n",
      "\fFigure 14-16. Regular deep neural network (left) and deep residual network (right)\n",
      "\n",
      "Now let’s look at ResNet’s architecture (see Figure 14-17). It is surprisingly simple. It\n",
      "starts  and  ends  exactly  like  GoogLeNet  (except  without  a  dropout  layer),  and  in\n",
      "between is just a very deep stack of simple residual units. Each residual unit is com‐\n",
      "posed of two convolutional layers (and no pooling layer!), with Batch Normalization\n",
      "(BN)  and  ReLU  activation,  using  3  ×  3  kernels  and  preserving  spatial  dimensions\n",
      "(stride 1, \"same\" padding).\n",
      "\n",
      "472 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-17. ResNet architecture [{'source': './data/ml.pdf', 'start_index': 1017145}]\n",
      "* Customizing Models and Training Algorithms                                                       384\n",
      "Custom Loss Functions                                                                                           384\n",
      "Saving and Loading Models That Contain Custom Components                     385\n",
      "Custom Activation Functions, Initializers, Regularizers, and Constraints      387\n",
      "Custom Metrics                                                                                                         388\n",
      "Custom Layers                                                                                                           391\n",
      "Custom Models                                                                                                         394\n",
      "Losses and Metrics Based on Model Internals                                                      397\n",
      "Computing Gradients Using Autodiff                                                                   399 [{'source': './data/ml.pdf', 'start_index': 28898}]\n",
      "* (CUDA), 690\n",
      "\n",
      "concatenative attention, 550\n",
      "concrete functions, 791\n",
      "conditional probability, 547\n",
      "confusion matrix, 90\n",
      "connectionism, 280\n",
      "constrained optimization, 166\n",
      "Contrastive Divergence, 777\n",
      "convergence, 118\n",
      "convex function, 120\n",
      "convolution kernels, 450\n",
      "convolutional autoencoders, 579\n",
      "convolutional layer\n",
      "filters, 450\n",
      "memory requirements, 456\n",
      "overview of, 448\n",
      "stacking multiple feature maps, 451\n",
      "TensorFlow implementation, 453\n",
      "Convolutional Neural Networks (CNNs)\n",
      "architecture of visual cortex, 446\n",
      "classification and localization, 483\n",
      "CNN architectures, 460-478\n",
      "convolutional layer, 448-456\n",
      "object detection, 485-492\n",
      "overview of, 445\n",
      "pooling layer, 456\n",
      "pretrained models for transfer learning, 481\n",
      "pretrained models from Keras, 479\n",
      "ResNet-34 using Keras, 478\n",
      "semantic segmentation, 492\n",
      "\n",
      "core instances, 255\n",
      "corpus development, 24\n",
      "correlation coefficient, 58\n",
      "cost functions [{'source': './data/ml.pdf', 'start_index': 1734204}]\n",
      "* Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +\n",
      "1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and \"same\" padding. The input\n",
      "signal is first copied and fed to four different layers. All convolutional layers use the\n",
      "ReLU activation function. Note that the second set of convolutional layers uses differ‐\n",
      "ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different\n",
      "scales. Also note that every single layer uses a stride of 1 and \"same\" padding (even\n",
      "the max pooling layer), so their outputs all have the same height and width as their\n",
      "inputs. This makes it possible to concatenate all the outputs along the depth dimen‐\n",
      "sion  in  the  final  depth  concatenation  layer  (i.e.,  stack  the  feature  maps  from  all  four\n",
      "top  convolutional  layers).  This  concatenation  layer  can  be  implemented  in  Tensor‐\n",
      "Flow using the tf.concat() operation, with axis=3 (the axis is the depth).\n",
      "\n",
      "Figure 14-13. Inception module [{'source': './data/ml.pdf', 'start_index': 1008208}]\n",
      "* 15 Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recog‐\n",
      "\n",
      "nition,” arXiv preprint arXiv:1409.1556 (2014).\n",
      "\n",
      "470 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fResNet\n",
      "Kaiming  He  et  al.  won  the  ILSVRC  2015  challenge  using  a  Residual  Network  (or\n",
      "ResNet),16 that delivered an astounding top-five error rate under 3.6%. The winning\n",
      "variant used an extremely deep CNN composed of 152 layers (other variants had 34,\n",
      "50,  and  101  layers).  It  confirmed  the  general  trend:  models  are  getting  deeper  and\n",
      "deeper, with fewer and fewer parameters. The key to being able to train such a deep\n",
      "network is to use skip connections (also called shortcut connections): the signal feeding\n",
      "into a layer is also added to the output of a layer located a bit higher up the stack. Let’s\n",
      "see why this is useful. [{'source': './data/ml.pdf', 'start_index': 1014993}]\n",
      "* In  this  chapter  we  will  explore  where  CNNs  came  from,  what  their  building  blocks\n",
      "look like, and how to implement them using TensorFlow and Keras. Then we will dis‐\n",
      "cuss  some  of  the  best  CNN  architectures,  as  well  as  other  visual  tasks,  including\n",
      "\n",
      "445\n",
      "\n",
      "\fobject detection (classifying multiple objects in an image and placing bounding boxes\n",
      "around  them)  and  semantic  segmentation  (classifying  each  pixel  according  to  the\n",
      "class of the object it belongs to). [{'source': './data/ml.pdf', 'start_index': 966319}]\n",
      "* 560 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fIf  we  ignore  the  skip  connections,  the  layer  normalization  layers,  the  Feed  Forward\n",
      "blocks, and the fact that this is Scaled Dot-Product Attention, not exactly Multi-Head\n",
      "Attention, then the rest of the Transformer model can be implemented like this:\n",
      "\n",
      "Z = encoder_in\n",
      "for N in range(6):\n",
      "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
      "\n",
      "encoder_outputs = Z\n",
      "Z = decoder_in\n",
      "for N in range(6):\n",
      "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
      "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
      "\n",
      "outputs = keras.layers.TimeDistributed(\n",
      "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z) [{'source': './data/ml.pdf', 'start_index': 1216623}]\n",
      "* weights, just like in Bahdanau attention. Another simplification they proposed was to\n",
      "use the decoder’s hidden state at the current time step rather than at the previous time\n",
      "step  (i.e.,  h(t))  rather  than  h(t–1)),  then  to  use  the  output  of  the  attention  mechanism\n",
      "(noted   t )  directly  to  compute  the  decoder’s  predictions  (rather  than  using  it  to\n",
      "compute the decoder’s current hidden state). They also proposed a variant of the dot\n",
      "product mechanism where the encoder outputs first go through a linear transforma‐\n",
      "tion (i.e., a time-distributed Dense layer without a bias term) before the dot products\n",
      "are computed. This is called the “general” dot product approach. They compared both\n",
      "dot product approaches to the concatenative attention mechanism (adding a rescaling\n",
      "parameter vector v), and they observed that the dot product variants performed bet‐\n",
      "ter than concatenative attention. For this reason, concatenative attention is much less [{'source': './data/ml.pdf', 'start_index': 1196850}]\n",
      "* Fully Convolutional Networks\n",
      "The idea of FCNs was first introduced in a 2015 paper25 by Jonathan Long et al., for\n",
      "semantic  segmentation  (the  task  of  classifying  every  pixel  in  an  image  according  to\n",
      "the class of the object it belongs to). The authors pointed out that you could replace\n",
      "the dense layers at the top of a CNN by convolutional layers. To understand this, let’s\n",
      "look at an example: suppose a dense layer with 200 neurons sits on top of a convolu‐\n",
      "tional layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map\n",
      "size, not the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7\n",
      "activations  from  the  convolutional  layer  (plus  a  bias  term).  Now  let’s  see  what  hap‐\n",
      "pens if we replace the dense layer with a convolutional layer using 200 filters, each of\n",
      "size 7 × 7, and with \"valid\" padding. This layer will output 200 feature maps, each 1 [{'source': './data/ml.pdf', 'start_index': 1048414}]\n",
      "* 468 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-14. GoogLeNet architecture\n",
      "\n",
      "Let’s go through this network:\n",
      "\n",
      "• The first two layers divide the image’s height and width by 4 (so its area is divided\n",
      "by 16), to reduce the computational load. The first layer uses a large kernel size so\n",
      "that much of the information is preserved.\n",
      "\n",
      "• Then the local response normalization layer ensures that the previous layers learn\n",
      "\n",
      "a wide variety of features (as discussed earlier).\n",
      "\n",
      "• Two  convolutional  layers  follow,  where  the  first  acts  like  a  bottleneck  layer.  As\n",
      "explained  earlier,  you  can  think  of  this  pair  as  a  single  smarter  convolutional\n",
      "layer.\n",
      "\n",
      "• Again, a local response normalization layer ensures that the previous layers cap‐\n",
      "\n",
      "ture a wide variety of patterns.\n",
      "\n",
      "• Next,  a  max  pooling  layer  reduces  the  image  height  and  width  by  2,  again  to\n",
      "\n",
      "speed up computations. [{'source': './data/ml.pdf', 'start_index': 1011432}]\n",
      "* Then we can create the first layers of the Transformer:\n",
      "\n",
      "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
      "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
      "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
      "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
      "encoder_embeddings = embeddings(encoder_inputs)\n",
      "decoder_embeddings = embeddings(decoder_inputs)\n",
      "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
      "encoder_in = positional_encoding(encoder_embeddings)\n",
      "decoder_in = positional_encoding(decoder_embeddings)\n",
      "\n",
      "558 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 16: Natural Language Processing with RNNs and Attention\n",
      "\n",
      "\fNow let’s look deeper into the heart of the Transformer model: the Multi-Head Atten‐\n",
      "tion layer.\n",
      "\n",
      "Multi-Head Attention [{'source': './data/ml.pdf', 'start_index': 1210825}]\n",
      "==================================================\n",
      "* What Is a Good Explanation?\n",
      "\n",
      "This section further condenses Miller’s summary on “good” explanations and adds concrete\n",
      "implications for interpretable machine learning.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "30 [{'source': './data/xai.pdf', 'start_index': 71027}]\n",
      "* ¹⁷Lipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-266.\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "31\n",
      "\n",
      "explanations) usually perform well because averaging over those “stories” makes the predictions\n",
      "more robust and accurate. But it also means that there is more than one selective explanation why\n",
      "a certain prediction was made. What it means for interpretable machine learning:\n",
      "Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex. The\n",
      "LIME method does a good job with this. [{'source': './data/xai.pdf', 'start_index': 74997}]\n",
      "* ¹⁰Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).\n",
      "¹¹Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.”\n",
      "\n",
      "Advances in Neural Information Processing Systems (2016).\n",
      "\n",
      "¹²Doshi-Velez, Finale, and Been Kim.\n",
      "\n",
      "“Towards a rigorous\n",
      "\n",
      "science of\n",
      "\n",
      "interpretable machine\n",
      "\n",
      "learning,” no. Ml: 1–13.\n",
      "\n",
      "http://arxiv.org/abs/1702.08608 ( 2017).\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "16 [{'source': './data/xai.pdf', 'start_index': 34924}]\n",
      "* Interpretability\n",
      "\n",
      "29\n",
      "\n",
      "Human-friendly Explanations\n",
      "\n",
      "Let us dig deeper and discover what we humans see as “good” explanations and what the\n",
      "implications are for interpretable machine learning. Humanities research can help us find out. Miller\n",
      "(2017) has conducted a huge survey of publications on explanations, and this chapter builds on his\n",
      "summary.\n",
      "\n",
      "In this chapter, I want to convince you of the following: As an explanation for an event, humans\n",
      "prefer short explanations (only 1 or 2 causes) that contrast the current situation with a situation in\n",
      "which the event would not have occurred. Especially abnormal causes provide good explanations.\n",
      "Explanations are social interactions between the explainer and the explainee (recipient of the\n",
      "explanation) and therefore the social context has a great influence on the actual content of the\n",
      "explanation. [{'source': './data/xai.pdf', 'start_index': 68811}]\n",
      "* An explanation is the answer to a why-question (Miller 2017).\n",
      "\n",
      "• Why did not the treatment work on the patient?\n",
      "• Why was my loan rejected?\n",
      "• Why have we not been contacted by alien life yet?\n",
      "\n",
      "The first two questions can be answered with an “everyday”-explanation, while the third one comes\n",
      "from the category “More general scientific phenomena and philosophical questions”. We focus on\n",
      "the “everyday”-type explanations, because those are relevant to interpretable machine learning.\n",
      "Questions that start with “how” can usually be rephrased as “why” questions: “How was my loan\n",
      "rejected?” can be turned into “Why was my loan rejected?”.\n",
      "\n",
      "In the following, the term “explanation” refers to the social and cognitive process of explaining, but\n",
      "also to the product of these processes. The explainer can be a human being or a machine.\n",
      "\n",
      "What Is a Good Explanation?\n",
      "\n",
      "This section further condenses Miller’s summary on “good” explanations and adds concrete\n",
      "implications for interpretable machine learning. [{'source': './data/xai.pdf', 'start_index': 70198}]\n",
      "* Explanations are contrastive (Lipton 1990¹⁷): Humans usually do not ask why a certain prediction\n",
      "was made, but why this prediction was made instead of another prediction. We tend to think in\n",
      "counterfactual cases, i.e. “How would the prediction have been if input X had been different?”.\n",
      "For a house price prediction, the house owner might be interested in why the predicted price was\n",
      "high compared to the lower price they had expected. If my loan application is rejected, I do not\n",
      "care to hear all the factors that generally speak for or against a rejection. I am interested in the\n",
      "factors in my application that would need to change to get the loan. I want to know the contrast\n",
      "between my application and the would-be-accepted version of my application. The recognition that\n",
      "contrasting explanations matter is an important finding for explainable machine learning. From\n",
      "most interpretable models, you can extract an explanation that implicitly contrasts a prediction of [{'source': './data/xai.pdf', 'start_index': 71218}]\n",
      "* Explanations are social: They are part of a conversation or interaction between the explainer\n",
      "and the receiver of the explanation. The social context determines the content and nature of the\n",
      "explanations. If I wanted to explain to a technical person why digital cryptocurrencies are worth\n",
      "so much, I would say things like: “The decentralized, distributed, blockchain-based ledger, which\n",
      "cannot be controlled by a central entity, resonates with people who want to secure their wealth,\n",
      "which explains the high demand and price.” But to my grandmother I would say: “Look, Grandma:\n",
      "Cryptocurrencies are a bit like computer gold. People like and pay a lot for gold, and young people\n",
      "like and pay a lot for computer gold.” What it means for interpretable machine learning:\n",
      "Pay attention to the social environment of your machine learning application and the target\n",
      "audience. Getting the social part of the machine learning model right depends entirely on your [{'source': './data/xai.pdf', 'start_index': 75540}]\n",
      "* contrasting explanations matter is an important finding for explainable machine learning. From\n",
      "most interpretable models, you can extract an explanation that implicitly contrasts a prediction of\n",
      "an instance with the prediction of an artificial data instance or an average of instances. Physicians\n",
      "might ask: “Why did the drug not work for my patient?”. And they might want an explanation that\n",
      "contrasts their patient with a patient for whom the drug worked and who is similar to the non-\n",
      "responding patient. Contrastive explanations are easier to understand than complete explanations.\n",
      "A complete explanation of the physician’s question why the drug does not work might include: The\n",
      "patient has had the disease for 10 years, 11 genes are over-expressed, the patients body is very quick\n",
      "in breaking the drug down into ineffective chemicals, … A contrastive explanation might be much\n",
      "simpler: In contrast to the responding patient, the non-responding patient has a certain combination [{'source': './data/xai.pdf', 'start_index': 71994}]\n",
      "* ⁶¹Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of machine learning.” ICML Workshop on\n",
      "\n",
      "Human Interpretability in Machine Learning. (2016).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "111\n",
      "\n",
      "The big picture of explainable machine learning. The real world goes through many layers before it reaches the human\n",
      "in the form of explanations.\n",
      "\n",
      "The lowest layer is the World. This could literally be nature itself, like the biology of the human\n",
      "body and how it reacts to medication, but also more abstract things like the real estate market. The\n",
      "World layer contains everything that can be observed and is of interest. Ultimately, we want to learn\n",
      "something about the World and interact with it.\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "112\n",
      "\n",
      "The second layer is the Data layer. We have to digitize the World in order to make it processable\n",
      "for computers and also to store information. The Data layer contains anything from images, texts,\n",
      "tabular data and so on. [{'source': './data/xai.pdf', 'start_index': 240882}]\n",
      "* • Comprehensibility: How well do humans understand the explanations? This looks just like\n",
      "one more property among many, but it is the elephant in the room. Difficult to define and\n",
      "measure, but extremely important to get right. Many people agree that comprehensibility\n",
      "depends on the audience. Ideas for measuring comprehensibility include measuring the size\n",
      "of the explanation (number of features with a non-zero weight in a linear model, number of\n",
      "decision rules, …) or testing how well people can predict the behavior of the machine learning\n",
      "model from the explanations. The comprehensibility of the features used in the explanation\n",
      "should also be considered. A complex transformation of features might be less comprehensible\n",
      "than the original features. [{'source': './data/xai.pdf', 'start_index': 66598}]\n",
      "* When you need explanations with ALL factors for a particular prediction or behavior, you do not\n",
      "want a human-friendly explanation, but a complete causal attribution. You probably want a causal\n",
      "attribution if you are legally required to specify all influencing features or if you debug the machine\n",
      "learning model. In this case, ignore the following points. In all other cases, where lay people or people\n",
      "with little time are the recipients of the explanation, the following sections should be interesting to\n",
      "you.\n",
      "\n",
      "What Is an Explanation?\n",
      "\n",
      "An explanation is the answer to a why-question (Miller 2017).\n",
      "\n",
      "• Why did not the treatment work on the patient?\n",
      "• Why was my loan rejected?\n",
      "• Why have we not been contacted by alien life yet? [{'source': './data/xai.pdf', 'start_index': 69660}]\n",
      "* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of\n",
      "machine learning.” ICML Workshop on Human Interpretability in Machine Learning. (2016).\n",
      "\n",
      "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should I trust you?: Explaining the\n",
      "predictions of any classifier.” Proceedings of the 22nd ACM SIGKDD international conference on\n",
      "knowledge discovery and data mining. ACM (2016).\n",
      "\n",
      "Shapley, Lloyd S. “A value for n-person games.” Contributions to the Theory of Games 2.28 (1953):\n",
      "307-317.\n",
      "\n",
      "Staniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and\n",
      "breakDown packages.” arXiv preprint arXiv:1804.01955 (2018).\n",
      "\n",
      "Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep\n",
      "neural networks.” IEEE Transactions on Evolutionary Computation (2019).\n",
      "\n",
      "Szegedy, Christian, et al. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199\n",
      "(2013). [{'source': './data/xai.pdf', 'start_index': 499406}]\n",
      "* ⁹³Staniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and breakDown packages.” arXiv preprint\n",
      "\n",
      "arXiv:1804.01955 (2018).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "Example-based explanation methods select particular instances of the dataset to explain the behavior\n",
      "of machine learning models or to explain the underlying data distribution. [{'source': './data/xai.pdf', 'start_index': 370991}]\n",
      "* Interpretability\n",
      "\n",
      "26\n",
      "\n",
      "Properties of Explanations\n",
      "\n",
      "We want to explain the predictions of a machine learning model. To achieve this, we rely on some\n",
      "explanation method, which is an algorithm that generates explanations. An explanation usually\n",
      "relates the feature values of an instance to its model prediction in a humanly understandable\n",
      "way. Other types of explanations consist of a set of data instances (e.g in the case of the k-nearest\n",
      "neighbor model). For example, we could predict cancer risk using a support vector machine and\n",
      "explain predictions using the local surrogate method, which generates decision trees as explanations.\n",
      "Or we could use a linear regression model instead of a support vector machine. The linear regression\n",
      "model is already equipped with an explanation method (interpretation of the weights). [{'source': './data/xai.pdf', 'start_index': 61052}]\n",
      "* The next chapter focuses on the evaluation of explanations for individual predictions on the function\n",
      "level. What are the relevant properties of explanations that we would consider for their evaluation?\n",
      "\n",
      "\fInterpretability\n",
      "\n",
      "26\n",
      "\n",
      "Properties of Explanations [{'source': './data/xai.pdf', 'start_index': 60847}]\n",
      "* Doshi-Velez and Kim (2017) propose three main levels for the evaluation of interpretability:\n",
      "\n",
      "Application level evaluation (real task): Put the explanation into the product and have it tested\n",
      "by the end user. Imagine fracture detection software with a machine learning component that\n",
      "locates and marks fractures in X-rays. At the application level, radiologists would test the fracture\n",
      "detection software directly to evaluate the model. This requires a good experimental setup and an\n",
      "understanding of how to assess quality. A good baseline for this is always how good a human would\n",
      "be at explaining the same decision. [{'source': './data/xai.pdf', 'start_index': 59270}]\n",
      "* In interpretable machine learning, counterfactual explanations can be used to explain predictions\n",
      "of individual instances. The “event” is the predicted outcome of an instance, the “causes” are the\n",
      "particular feature values of this instance that were input to the model and “caused” a certain\n",
      "prediction. Displayed as a graph, the relationship between the inputs and the prediction is very\n",
      "simple: The feature values cause the prediction.\n",
      "\n",
      "The causal relationships between inputs of a machine learning model and the predictions, when the model is merely\n",
      "seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data).\n",
      "\n",
      "Even if in reality the relationship between the inputs and the outcome to be predicted might not be\n",
      "causal, we can see the inputs of a model as the cause of the prediction. [{'source': './data/xai.pdf', 'start_index': 377061}]\n",
      "* Explanations are truthful. Good explanations prove to be true in reality (i.e. in other situations).\n",
      "But disturbingly, this is not the most important factor for a “good” explanation. For example,\n",
      "selectiveness seems to be more important than truthfulness. An explanation that selects only one\n",
      "or two possible causes rarely covers the entire list of relevant causes. Selectivity omits part of the\n",
      "truth. It is not true that only one or two factors, for example, have caused a stock market crash, but\n",
      "the truth is that there are millions of causes that influence millions of people to act in such a way\n",
      "that in the end a crash was caused. What it means for interpretable machine learning:\n",
      "The explanation should predict the event as truthfully as possible, which in machine learning is\n",
      "sometimes called fidelity. So if we say that a second balcony increases the price of a house, then\n",
      "that also should apply to other houses (or at least to similar houses). For humans, fidelity of an [{'source': './data/xai.pdf', 'start_index': 79999}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "168\n",
      "\n",
      "Local Surrogate (LIME)\n",
      "\n",
      "Local surrogate models are interpretable models that are used to explain individual predictions of\n",
      "black box machine learning models. Local interpretable model-agnostic explanations (LIME)⁷⁹ is a\n",
      "paper in which the authors propose a concrete implementation of local surrogate models. Surrogate\n",
      "models are trained to approximate the predictions of the underlying black box model. Instead of\n",
      "training a global surrogate model, LIME focuses on training local surrogate models to explain\n",
      "individual predictions. [{'source': './data/xai.pdf', 'start_index': 332397}]\n",
      "* Importance of Interpretability\n",
      "\n",
      "If a machine learning model performs well, why do not we just trust the model and ignore why\n",
      "it made a certain decision? “The problem is that a single metric, such as classification accuracy, is\n",
      "an incomplete description of most real-world tasks.” (Doshi-Velez and Kim 2017 ¹²) [{'source': './data/xai.pdf', 'start_index': 33166}]\n",
      "* 214\n",
      "\n",
      "For example, you can create an interpretable prediction model: a so-called “nearest prototype model”.\n",
      "The prediction function is defined as:\n",
      "\n",
      "^f (x) = argmaxi2Sk(x; xi)\n",
      "\n",
      "which means that we select the prototype i from the set of prototypes S that is closest to the new\n",
      "data point, in the sense that it yields the highest value of the kernel function. The prototype itself\n",
      "is returned as an explanation for the prediction. This procedure has three tuning parameters: The\n",
      "type of kernel, the kernel scaling parameter and the number of prototypes. All parameters can be\n",
      "optimized within a cross validation loop. The criticisms are not used in this approach.\n",
      "\n",
      "As a third option, we can use MMD-critic to make any machine learning model globally explainable\n",
      "by examining prototypes and criticisms along with their model predictions. The procedure is as\n",
      "follows: [{'source': './data/xai.pdf', 'start_index': 429370}]\n",
      "* The complexity of the explanation model has to be defined in advance. This is just a small complaint,\n",
      "because in the end the user always has to define the compromise between fidelity and sparsity.\n",
      "\n",
      "Another really big problem is the instability of the explanations. In an article ⁸⁸ the authors showed\n",
      "that the explanations of two very close points varied greatly in a simulated setting. Also, in my\n",
      "experience, if you repeat the sampling process, then the explantions that come out can be different.\n",
      "Instability means that it is difficult to trust the explanations, and you should be very critical.\n",
      "\n",
      "Conclusion: Local surrogate models, with LIME as a concrete implementation, are very promising.\n",
      "But the method is still in development phase and many problems need to be solved before it can be\n",
      "safely applied.\n",
      "\n",
      "⁸⁸Alvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv preprint arXiv:1806.08049 (2018).\n",
      "\n",
      "\fModel-Agnostic Methods\n",
      "\n",
      "Shapley Values\n",
      "\n",
      "177 [{'source': './data/xai.pdf', 'start_index': 348817}]\n",
      "* Let us dive deeper into the reasons why interpretability is so important. When it comes to predictive\n",
      "modeling, you have to make a trade-off: Do you just want to know what is predicted? For example,\n",
      "the probability that a customer will churn or how effective some drug will be for a patient. Or do you\n",
      "want to know why the prediction was made and possibly pay for the interpretability with a drop\n",
      "in predictive performance? In some cases, you do not care why a decision was made, it is enough\n",
      "to know that the predictive performance on a test dataset was good. But in other cases, knowing\n",
      "the ‘why’ can help you learn more about the problem, the data and the reason why a model might\n",
      "fail. Some models may not require explanations because they are used in a low-risk environment,\n",
      "meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the\n",
      "method has already been extensively studied and evaluated (e.g. optical character recognition). The [{'source': './data/xai.pdf', 'start_index': 33477}]\n",
      "* Martens, David, and Foster Provost. “Explaining data-driven document classifications.” (2014).\n",
      "\n",
      "Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint\n",
      "arXiv:1706.07269. (2017).\n",
      "\n",
      "Nickerson, Raymond S. “Confirmation Bias: A ubiquitous phenomenon in many guises.” Review of\n",
      "General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).\n",
      "\n",
      "Papernot, Nicolas, et al. “Practical black-box attacks against machine learning.” Proceedings of the\n",
      "2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).\n",
      "\n",
      "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-precision model-agnostic\n",
      "explanations.” AAAI Conference on Artificial Intelligence (2018).\n",
      "\n",
      "Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of\n",
      "machine learning.” ICML Workshop on Human Interpretability in Machine Learning. (2016). [{'source': './data/xai.pdf', 'start_index': 498663}]\n",
      "* Interpretable Machine Learning\n",
      "\n",
      "A Guide for Making Black Box Models Explainable\n",
      "\n",
      "Christoph Molnar\n",
      "\n",
      "This book is for sale at http://leanpub.com/interpretable-machine-learning\n",
      "\n",
      "This version was published on 2019-02-21\n",
      "\n",
      "This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing\n",
      "process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and\n",
      "many iterations to get reader feedback, pivot until you have the right book and build traction once\n",
      "you do.\n",
      "\n",
      "This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0\n",
      "International License\n",
      "\n",
      "\fContents\n",
      "\n",
      "Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "1 [{'source': './data/xai.pdf', 'start_index': 1}]\n",
      "* Model-Agnostic Methods\n",
      "\n",
      "Separating the explanations from the machine learning model (= model-agnostic interpretation\n",
      "methods) has some advantages (Ribeiro, Singh, and Guestrin 2016⁶¹). The great advantage of model-\n",
      "agnostic interpretation methods over model-specific ones is their flexibility. Machine learning\n",
      "developers are free to use any machine learning model they like when the interpretation methods\n",
      "can be applied to any model. Anything that builds on an interpretation of a machine learning model,\n",
      "such as a graphic or user interface, also becomes independent of the underlying machine learning\n",
      "model. Typically, not just one, but many types of machine learning models are evaluated to solve\n",
      "a task, and when comparing models in terms of interpretability, it is easier to work with model-\n",
      "agnostic explanations, because the same method can be used for any type of model. [{'source': './data/xai.pdf', 'start_index': 238526}]\n",
      "* Interpretability\n",
      "\n",
      "There is no mathematical definition of interpretability. A (non-mathematical) definition I like by\n",
      "Miller (2017)¹⁰ is: Interpretability is the degree to which a human can understand the cause of\n",
      "a decision. Another one is: Interpretability is the degree to which a human can consistently\n",
      "predict the model’s result ¹¹. The higher the interpretability of a machine learning model, the\n",
      "easier it is for someone to comprehend why certain decisions or predictions have been made.\n",
      "A model is better interpretable than another model if its decisions are easier for a human to\n",
      "comprehend than decisions from the other model. I will use both the terms interpretable and\n",
      "explainable interchangeably. Like Miller (2017), I think it makes sense to distinguish between the\n",
      "terms interpretability/explainability and explanation. I will use “explanation” for explanations of\n",
      "individual predictions. See the section about explanations to learn what we humans see as a good\n",
      "explanation. [{'source': './data/xai.pdf', 'start_index': 32176}]\n",
      "* We take a closer look at the properties of explanation methods and explanations (Robnik-Sikonja\n",
      "and Bohanec, 2018¹⁵). These properties can be used to judge how good an explanation method or\n",
      "explanation is. It is not clear for all these properties how to measure them correctly, so one of the\n",
      "challenges is to formalize how they could be calculated.\n",
      "\n",
      "Properties of Explanation Methods\n",
      "\n",
      "• Expressive Power is the “language” or structure of the explanations the method is able to\n",
      "generate. An explanation method could generate IF-THEN rules, decision trees, a weighted\n",
      "sum, natural language or something else. [{'source': './data/xai.pdf', 'start_index': 61873}]\n",
      "* The Future of Interpretability\n",
      "\n",
      "Let us take a look at the possible future of machine learning interpretability.\n",
      "\n",
      "The focus will be on model-agnostic interpretability tools.\n",
      "\n",
      "It is much easier to automate interpretability when it is decoupled from the underlying machine\n",
      "learning model. The advantage of model-agnostic interpretability lies in its modularity. We can eas-\n",
      "ily replace the underlying machine learning model. We can just as easily replace the interpretation\n",
      "method. For these reasons, model-agnostic methods will scale much better. That is why I believe that\n",
      "model-agnostic methods will become more dominant in the long term. But intrinsically interpretable\n",
      "methods will also have a place.\n",
      "\n",
      "Machine learning will be automated and, with it, interpretability. [{'source': './data/xai.pdf', 'start_index': 482487}]\n",
      "* meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the\n",
      "method has already been extensively studied and evaluated (e.g. optical character recognition). The\n",
      "need for interpretability arises from an incompleteness in problem formalization (Doshi-Velez and\n",
      "Kim 2017), which means that for certain problems or tasks it is not enough to get the prediction\n",
      "(the what). The model must also explain how it came to the prediction (the why), because a correct\n",
      "prediction only partially solves your original problem. The following reasons drive the demand for\n",
      "interpretability and explanations (Doshi-Velez and Kim 2017 and Miller 2017). [{'source': './data/xai.pdf', 'start_index': 34257}]\n",
      "* Example-Based Explanations\n",
      "\n",
      "space.\n",
      "\n",
      "230\n",
      "\n",
      "Dog or fish? For the SVM prediction (middle row) images that had similar colors as the test image were the most\n",
      "influential. For the neural network prediction (bottom row) fish in different setting were most influential, but also a\n",
      "dog image (top right). Work by Koh and Liang (2017).\n",
      "\n",
      "Handling domain mismatches / Debugging model errors [{'source': './data/xai.pdf', 'start_index': 464066}]\n",
      "* Interpretability\n",
      "\n",
      "20\n",
      "\n",
      "• Trust: It is easier for humans to trust a system that explains its decisions compared to a black\n",
      "\n",
      "box.\n",
      "\n",
      "When we do not need interpretability.\n",
      "\n",
      "The following scenarios illustrate when we do not need or even do not want interpretability of\n",
      "machine learning models. [{'source': './data/xai.pdf', 'start_index': 44911}]\n",
      "* model. The Model-Agnostic Methods chapter deals with methods such as partial dependence plots\n",
      "and permutation feature importance. Model-agnostic methods work by changing the input of the\n",
      "machine learning model and measuring changes in the prediction output. Model-agnostic methods\n",
      "that return data instances as explanations are discussed in the chapter Example Based Explanations.\n",
      "All model-agnostic methods can be further differentiated based on whether they explain global\n",
      "model behavior across all data instances or individual predictions. The following methods explain\n",
      "the overall behavior of the model: Partial Dependence Plots, Accumulated Local Effects, Feature\n",
      "Interaction, Feature Importance, Global Surrogate Models and Prototypes and Criticisms. To explain\n",
      "individual predictions we have Local Surrogate Models, Shapley Value Explanations, Counterfactual\n",
      "Explanations (and closely related: Adversarial Examples). Some methods can be used to explain both [{'source': './data/xai.pdf', 'start_index': 9707}]\n",
      "* Human curiosity and learning: Humans have a mental model of their environment that is updated\n",
      "when something unexpected happens. This update is performed by finding an explanation for the\n",
      "unexpected event. For example, a human feels unexpectedly sick and asks, “Why do I feel so sick?”.\n",
      "He learns that he gets sick every time he eats those red berries. He updates his mental model and\n",
      "decides that the berries caused the sickness and should therefore be avoided. When opaque machine\n",
      "learning models are used in research, scientific findings remain completely hidden if the model only\n",
      "gives predictions without explanations. To facilitate learning and satisfy curiosity as to why certain\n",
      "predictions or behaviors are created by machines, interpretability and explanations are crucial. Of\n",
      "course, humans do not need explanations for everything that happens. For most people it is okay that\n",
      "they do not understand how a computer works. Unexpected events makes us curious. For example: [{'source': './data/xai.pdf', 'start_index': 35434}]\n",
      "* Example-Based Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n",
      "Counterfactual Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n",
      "Adversarial Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n",
      "Prototypes and Criticisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n",
      "Influential Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n",
      "\n",
      "A Look into the Crystal Ball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n",
      "The Future of Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n",
      "The Future of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n",
      "\n",
      "Contribute to the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 [{'source': './data/xai.pdf', 'start_index': 4276}]\n",
      "* Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.”\n",
      "arXiv preprint arXiv:1703.04730 (2017).\n",
      "\n",
      "Laugel, Thibault, et al. “Inverse classification for comparison-based interpretability in machine\n",
      "learning.” arXiv preprint arXiv:1712.08443 (2017).\n",
      "\n",
      "Letham, Benjamin, et al. “Interpretable classifiers using rules and Bayesian analysis: Building a better\n",
      "stroke prediction model.” The Annals of Applied Statistics 9.3 (2015): 1350-1371.\n",
      "\n",
      "Lipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-\n",
      "266.\n",
      "\n",
      "Lipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).\n",
      "\n",
      "\fReferences\n",
      "\n",
      "245\n",
      "\n",
      "Lundberg, Scott, and Su-In Lee. “An unexpected unity among methods for interpreting model\n",
      "predictions.” arXiv preprint arXiv:1611.07478 (2016).\n",
      "\n",
      "Martens, David, and Foster Provost. “Explaining data-driven document classifications.” (2014). [{'source': './data/xai.pdf', 'start_index': 497819}]\n",
      "* Global, Holistic Model Interpretability\n",
      "\n",
      "How does the trained model make predictions? [{'source': './data/xai.pdf', 'start_index': 54053}]\n",
      "* In this section, I will present the approach suggested by Wachter et. al (2017)⁹⁶. They suggest\n",
      "minimizing the following loss.\n",
      "\n",
      "L(x; x′; y′; (cid:21)) = (cid:21) (cid:1) ( ^f (x′) (cid:0) y′)2 + d(x; x′)\n",
      "\n",
      "The first term is the quadratic distance between the model prediction for the counterfactual x’ and\n",
      "the desired outcome y’, which the user must define in advance. The second term is the distance d\n",
      "between the instance x to be explained and the counterfactual x’, but more about this later. The\n",
      "parameter (cid:21) balances the distance in prediction (first term) against the distance in feature values\n",
      "(second term). The loss is solved for a given (cid:21) and returns a counterfactual x’. A higher value of (cid:21)\n",
      "\n",
      "⁹⁶Wachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without opening the black box: Automated decisions\n",
      "\n",
      "and the GDPR.” (2017).\n",
      "\n",
      "\fExample-Based Explanations\n",
      "\n",
      "194 [{'source': './data/xai.pdf', 'start_index': 385234}]\n",
      "* in breaking the drug down into ineffective chemicals, … A contrastive explanation might be much\n",
      "simpler: In contrast to the responding patient, the non-responding patient has a certain combination\n",
      "of genes that make the drug less effective. The best explanation is the one that highlights the greatest\n",
      "difference between the object of interest and the reference object. What it means for interpretable\n",
      "machine learning:\n",
      "Humans do not want a complete explanation for a prediction, but want to compare what the\n",
      "differences were to another instance’s prediction (can be an artificial one). Creating contrastive\n",
      "explanations is application-dependent because it requires a point of reference for comparison. And\n",
      "this may depend on the data point to be explained, but also on the user receiving the explanation.\n",
      "A user of a house price prediction website might want to have an explanation of a house price\n",
      "prediction contrastive to their own house or maybe to another house on the website or maybe [{'source': './data/xai.pdf', 'start_index': 72780}]\n",
      "* • Portability describes the range of machine learning models with which the explanation\n",
      "method can be used. Methods with a low translucency have a higher portability because they\n",
      "treat the machine learning model as a black box. Surrogate models might be the explanation\n",
      "method with the highest portability. Methods that only work for e.g. recurrent neural networks\n",
      "have low portability.\n",
      "\n",
      "• Algorithmic Complexity describes the computational complexity of the method that generates\n",
      "the explanation. This property is important to consider when computation time is a bottleneck\n",
      "in generating explanations.\n",
      "\n",
      "Properties of Individual Explanations\n",
      "\n",
      "• Accuracy: How well does an explanation predict unseen data? High accuracy is especially\n",
      "important if the explanation is used for predictions in place of the machine learning model.\n",
      "\n",
      "¹⁵Robnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning.\n",
      "\n",
      "Springer, Cham. 159-175. (2018). [{'source': './data/xai.pdf', 'start_index': 63134}]\n"
     ]
    }
   ],
   "source": [
    "question = \"How to add explainability to convolution and attention layers?\"\n",
    "results = vector_store.similarity_search(\n",
    "    question,\n",
    "    k=40\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "results2 = vector_store2.similarity_search(\n",
    "    question,\n",
    "    k=40\n",
    ")\n",
    "for res in results2:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def query(question, results, client):\n",
    "    template = f\"\"\"\n",
    "    Answer the request using only the context below.\n",
    "    Context:\n",
    "    \n",
    "    {\"\\n\".join(results)}\n",
    "\n",
    "\n",
    "    Request: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Combine only Context section provided and answer based on it only, summerizer it and dont use of your knowlegde in answer.\"},\n",
    "            {\"role\": \"user\", \"content\": template}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"To add explainability to convolution and attention layers, the following strategies can be employed:\\n\\n1. **Attention Mechanisms**: Incorporate attention mechanisms in the model, as they enhance understanding by showing which parts of the input the model focuses on when making predictions. For example, in visual tasks, attention helps identify which areas of an image are influencing the model's output, allowing for insights into model behavior during errors.\\n\\n2. **Contrastive Explanations**: Utilize contrastive explanations that highlight differences between an instance and a hypothetical scenario where a different prediction might occur. This helps clarify why specific outputs were generated based on particular input features.\\n\\n3. **Model-Agnostic Methods**: Apply model-agnostic interpretability techniques that can provide insights into any machine learning model. These methods can help in understanding the relationships between input data and model predictions across various layers, including convolution and attention layers.\\n\\n4. **Visualizing Feature Maps**: In convolutional layers, visualize feature maps to illustrate how different filters respond to specific features in the input data. This can demonstrate which features are important for classification.\\n\\nBy implementing these methods, you can significantly improve the explainability of models that use convolution and attention layers.\", \"The context discusses the importance of explainability in machine learning models, emphasizing the need for understandable explanations for model predictions. For convolutional layers, while it does not provide a specific method to add explainability, the general approach involves interpreting the feature maps created by these layers. Techniques like visualizations of activation maps or saliency maps can be used to show which parts of an input image contributed most to a model's decisions.\\n\\nFor attention layers, visual attention mechanisms can be leveraged to explain model predictions by showing where the model focused while making a prediction. This can help in understanding the importance of different input features in the decision-making process.\\n\\nOverall, integrating local surrogate models like LIME can also help in providing explanations for predictions made by convolutional and attention layers, allowing users to gain insights into the factors influencing specific outcomes.\", 'The provided context does not offer specific methods or frameworks on how to add explainability to convolution and attention layers. It addresses concepts related to CNN architectures, the importance of interpretability in machine learning, and technical details about layers, but lacks direct information on integrating explainability into convolutional and attention mechanisms.', 'To add explainability to convolution and attention layers, one can leverage model-agnostic methods that analyze how input features influence predictions. Example-based explanations can be used to show influential instances for specific predictions, while techniques such as partial dependence plots, Shapley value explanations, and local surrogate models can provide insights into model behavior at both the individual and global levels. Furthermore, for attention mechanisms, incorporating visualizations of attention weights can illustrate how different parts of the input are prioritized, enhancing understanding of model decisions.']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def group_elements(input_list, group_size):\n",
    "    return [\"\\n\".join(map(lambda l: l.page_content, input_list[i:i + group_size])) for i in range(0, len(input_list), group_size)]\n",
    "\n",
    "\n",
    "input1 = group_elements(results, 10)\n",
    "input2 = group_elements(results2, 10)\n",
    "output = []\n",
    "for res in zip(input1, input2):\n",
    "    output.append(query(question, res, client))\n",
    "    time.sleep(30)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Explainability in models utilizing convolutional and attention layers can be enhanced through attention mechanisms. These mechanisms help in understanding model outputs by identifying which parts of the input (images or textual data) the model focused on when making predictions. For example, in a scenario where an image of a dog is incorrectly labeled as a wolf, attention mechanisms allow the user to analyze the model's focus, revealing its reasoning behind the classification.\\n\\nIn practice, when applying attention mechanisms, you can implement methods such as visual attention, where a convolutional neural network processes an image and provides feature maps to a decoder. The decoder, equipped with an attention mechanism, generates outputs (like captions) one word at a time while focusing on relevant parts of the input image.\\n\\nAdditionally, models like the Transformer architecture, which relies solely on attention mechanisms without using recurrent or convolutional layers, showcase how effective attention can be in understanding and explaining model predictions. By incorporating attention layers, you can make models more interpretable and capable of providing insights into their decision-making process.\", 'The context provided does not directly address methods for adding explainability to convolution and attention layers in neural networks. It focuses primarily on the architecture and functioning of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), including technical details about layers, hyperparameters, and challenges in tasks like semantic segmentation. However, it does mention the use of attention mechanisms in the context of the Transformer model, but lacks specific strategies or techniques for enhancing explainability.', 'The context provided does not contain information on how to add explainability specifically to convolution and attention layers in machine learning models. It predominantly discusses the nature of good explanations, interpretability, and the social aspects of explanations within machine learning, rather than technical methodologies for specific model components.', 'The context does not provide specific methods or steps on how to add explainability to convolution and attention layers directly. However, it mentions model-agnostic interpretability methods which can be broadly applied to various machine learning models, including those involving convolution and attention. Approaches such as local surrogate models, Shapley values, and example-based explanations can be utilized to explain individual predictions from such layers. The use of prototypes and criticisms, as well as counterfactual explanations, are also noted as relevant interpretability techniques that could potentially be adapted to these types of layers. Additionally, ensuring stability and trust in the generated explanations is emphasized, as the explanations can vary significantly for very similar input data points.']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def group_elements(input_list, group_size):\n",
    "    return [\"\\n\".join(map(lambda l: l.page_content, input_list[i:i + group_size])) for i in range(0, len(input_list), group_size)]\n",
    "\n",
    "\n",
    "input1 = group_elements(results, 10)\n",
    "input2 = group_elements(results2, 10)\n",
    "input1.extend(input2)\n",
    "output = []\n",
    "for i in range(0, len(input1), 2):\n",
    "    output.append(query(question, input1[i: i + 2], client))\n",
    "    time.sleep(10)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To enhance explainability in convolutional and attention layers, attention mechanisms can be applied. These mechanisms allow for an understanding of model outputs by identifying which parts of the input (such as images or text) the model concentrated on during predictions. For instance, if an image is misclassified, attention mechanisms can help analyze the model's focus, clarifying its reasoning.\\n\\nIn practice, visual attention can be incorporated, where a convolutional neural network processes an image and delivers feature maps to a decoder. The decoder, using an attention mechanism, generates outputs (like captions) sequentially while emphasizing relevant aspects of the input image.\\n\\nAdditionally, the Transformer architecture exemplifies the effectiveness of solely using attention mechanisms for understanding and explaining model predictions. By including attention layers, models become more interpretable, offering insights into their decision-making processes.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(question, output, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To enhance explainability in convolutional and attention layers, attention '\n",
      " 'mechanisms can be applied. These mechanisms allow for an understanding of '\n",
      " 'model outputs by identifying which parts of the input (such as images or '\n",
      " 'text) the model concentrated on during predictions. For instance, if an '\n",
      " \"image is misclassified, attention mechanisms can help analyze the model's \"\n",
      " 'focus, clarifying its reasoning.\\n'\n",
      " '\\n'\n",
      " 'In practice, visual attention can be incorporated, where a convolutional '\n",
      " 'neural network processes an image and delivers feature maps to a decoder. '\n",
      " 'The decoder, using an attention mechanism, generates outputs (like captions) '\n",
      " 'sequentially while emphasizing relevant aspects of the input image.\\n'\n",
      " '\\n'\n",
      " 'Additionally, the Transformer architecture exemplifies the effectiveness of '\n",
      " 'solely using attention mechanisms for understanding and explaining model '\n",
      " 'predictions. By including attention layers, models become more '\n",
      " 'interpretable, offering insights into their decision-making processes.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(\"To enhance explainability in convolutional and attention layers, attention mechanisms can be applied. These mechanisms allow for an understanding of model outputs by identifying which parts of the input (such as images or text) the model concentrated on during predictions. For instance, if an image is misclassified, attention mechanisms can help analyze the model's focus, clarifying its reasoning.\\n\\nIn practice, visual attention can be incorporated, where a convolutional neural network processes an image and delivers feature maps to a decoder. The decoder, using an attention mechanism, generates outputs (like captions) sequentially while emphasizing relevant aspects of the input image.\\n\\nAdditionally, the Transformer architecture exemplifies the effectiveness of solely using attention mechanisms for understanding and explaining model predictions. By including attention layers, models become more interpretable, offering insights into their decision-making processes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
